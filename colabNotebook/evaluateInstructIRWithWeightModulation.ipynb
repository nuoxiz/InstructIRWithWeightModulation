{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "qseY2cW3uBNg"
      ],
      "gpuType": "A100",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CrRA8hse3q46",
        "outputId": "7be74fc2-befe-431e-aa49-773787d29450"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#NoiseDegradation.py\n"
      ],
      "metadata": {
        "id": "d3T7Zggbux3T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision.transforms import ToPILImage, Compose, RandomCrop, ToTensor\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "class NoiseDegradation(object):\n",
        "    def __init__(self, args):\n",
        "        super(NoiseDegradation, self).__init__()\n",
        "        self.args = args\n",
        "        self.toTensor = ToTensor()\n",
        "        self.crop_transform = Compose([\n",
        "            ToPILImage(),\n",
        "            RandomCrop(args.patch_size),\n",
        "        ])\n",
        "    def _add_gaussian_noise(self, clean_patch, sigma):\n",
        "        noise = np.random.randn(*clean_patch.shape)\n",
        "        noisy_patch = np.clip(clean_patch + noise * sigma, 0, 255).astype(np.uint8)\n",
        "\n",
        "        return noisy_patch, clean_patch\n",
        "\n",
        "    def _add_noise_degradation_by_level(self, clean_patch, degrade_type):\n",
        "        degraded_patch = None\n",
        "        if degrade_type == 0:\n",
        "            # noise level (sigma) =15\n",
        "            degraded_patch, clean_patch = self._add_gaussian_noise(clean_patch, sigma=15)\n",
        "        elif degrade_type == 7:\n",
        "            # noise level (sigma) =25\n",
        "            degraded_patch, clean_patch = self._add_gaussian_noise(clean_patch, sigma=25)\n",
        "        elif degrade_type == 8:\n",
        "            # noise level (sigma) =50\n",
        "            degraded_patch, clean_patch = self._add_gaussian_noise(clean_patch, sigma=50)\n",
        "\n",
        "         # If degraded_patch is still None (meaning degrade_type wasn't 0, 7, or 8)\n",
        "        # handle the case (raise an error, return the original, etc.)\n",
        "        if degraded_patch is None:\n",
        "            degraded_patch = clean_patch # or raise ValueError(f\"Invalid degrade_type: {degrade_type}\")\n",
        "\n",
        "        return degraded_patch, clean_patch\n",
        "\n",
        "    def add_noise_degradation(self,clean_patch,degrade_type = None):\n",
        "        if degrade_type == 0 or degrade_type == 7 or degrade_type == 8:\n",
        "            degrade_type= degrade_type\n",
        "        else:\n",
        "            degrade_type = random.choices([0,7,8])\n",
        "\n",
        "\n",
        "        degraded_patch, _ = self._add_noise_degradation_by_level(clean_patch, degrade_type)\n",
        "        return degraded_patch\n"
      ],
      "metadata": {
        "id": "tWd00-oru1IJ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#utils.py\n"
      ],
      "metadata": {
        "id": "qseY2cW3uBNg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import imageio\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import argparse\n",
        "import random\n",
        "import torch\n",
        "\n",
        "\n",
        "def seed_everything(SEED=42):\n",
        "    random.seed(SEED)\n",
        "    np.random.seed(SEED)\n",
        "    torch.manual_seed(SEED)\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "def saveImage(filename, image):\n",
        "    imageTMP = np.clip(image * 255.0, 0, 255).astype('uint8')\n",
        "    imageio.imwrite(filename, imageTMP)\n",
        "\n",
        "def save_rgb (img, filename):\n",
        "\n",
        "    img = np.clip(img, 0., 1.)\n",
        "    if np.max(img) <= 1:\n",
        "        img = img * 255\n",
        "\n",
        "    img = img.astype(np.float32)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
        "    cv2.imwrite(filename, img)\n",
        "\n",
        "\n",
        "def load_img (filename, norm=True,):\n",
        "\n",
        "    # original code: img = np.array(Image.open(filename).convert(\"RGB\"))\n",
        "    # changed to:\n",
        "    img = np.array(Image.open(filename).convert(\"RGB\").resize((320,480)))\n",
        "\n",
        "    if norm:\n",
        "        img = img / 255.\n",
        "        img = img.astype(np.float32)\n",
        "    return img\n",
        "\n",
        "def plot_all (images, figsize=(20,10), axis='off', names=None):\n",
        "    nplots = len(images)\n",
        "    fig, axs = plt.subplots(1,nplots, figsize=figsize, dpi=80,constrained_layout=True)\n",
        "    for i in range(nplots):\n",
        "        axs[i].imshow(images[i])\n",
        "        if names: axs[i].set_title(names[i])\n",
        "        axs[i].axis(axis)\n",
        "    plt.show()\n",
        "\n",
        "def modcrop(img_in, scale=2):\n",
        "    # img_in: Numpy, HWC or HW\n",
        "    img = np.copy(img_in)\n",
        "    if img.ndim == 2:\n",
        "        H, W = img.shape\n",
        "        H_r, W_r = H % scale, W % scale\n",
        "        img = img[:H - H_r, :W - W_r]\n",
        "    elif img.ndim == 3:\n",
        "        H, W, C = img.shape\n",
        "        H_r, W_r = H % scale, W % scale\n",
        "        img = img[:H - H_r, :W - W_r, :]\n",
        "    else:\n",
        "        raise ValueError('Wrong img ndim: [{:d}].'.format(img.ndim))\n",
        "    return img\n",
        "\n",
        "def dict2namespace(config):\n",
        "    namespace = argparse.Namespace()\n",
        "    for key, value in config.items():\n",
        "        if isinstance(value, dict):\n",
        "            new_value = dict2namespace(value)\n",
        "        else:\n",
        "            new_value = value\n",
        "        setattr(namespace, key, new_value)\n",
        "    return namespace\n",
        "\n",
        "\n",
        "########## MODEL\n",
        "\n",
        "def count_params(model):\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    return trainable_params"
      ],
      "metadata": {
        "id": "XyDUDVtvubBC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#TrainDataset.py"
      ],
      "metadata": {
        "id": "QEXc1LhItuSf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "import torchvision.transforms.functional as TF\n",
        "from torchvision.transforms import ToPILImage, Compose, RandomCrop, ToTensor\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "\n",
        "# from utils import load_img, modcrop\n",
        "# from torchvision import transforms\n",
        "# from training_utils.NoiseDegradation import NoiseDegradation\n",
        "\n",
        "\n",
        "\n",
        "DEG_MAP = {\n",
        "    \"noise_15\" : 0,\n",
        "    \"blur\"     : 1,\n",
        "    \"rain\"     : 2,\n",
        "    \"haze\"     : 3,\n",
        "    \"lol\"      : 4,\n",
        "    \"sr\"       : 5,\n",
        "    \"en\"       : 6,\n",
        "    \"noise_25\" : 7,\n",
        "    \"noise_50\" : 8\n",
        "}\n",
        "\n",
        "DEG2TASK = {\n",
        "    \"noise\": \"denoising\",\n",
        "    \"blur\" : \"deblurring\",\n",
        "    \"rain\" : \"deraining\",\n",
        "    \"haze\" : \"dehazing\",\n",
        "    \"lol\"  : \"lol\",\n",
        "    \"sr\"   : \"sr\",\n",
        "    \"en\"   : \"enhancement\"\n",
        "}\n",
        "\n",
        "def crop_img(image, base=16):\n",
        "    \"\"\"\n",
        "    Mod crop the image to ensure the dimension is divisible by base. Also done by SwinIR, Restormer and others.\n",
        "    \"\"\"\n",
        "    h = image.shape[0]\n",
        "    w = image.shape[1]\n",
        "    crop_h = h % base\n",
        "    crop_w = w % base\n",
        "    return image[crop_h // 2:h - crop_h + crop_h // 2, crop_w // 2:w - crop_w + crop_w // 2, :]\n",
        "\n",
        "def data_augmentation(image, mode):\n",
        "    if mode == 0:\n",
        "        # original\n",
        "        out = image.numpy()\n",
        "    elif mode == 1:\n",
        "        # flip up and down\n",
        "        out = np.flipud(image)\n",
        "    elif mode == 2:\n",
        "        # rotate counterwise 90 degree\n",
        "        out = np.rot90(image)\n",
        "    elif mode == 3:\n",
        "        # rotate 90 degree and flip up and down\n",
        "        out = np.rot90(image)\n",
        "        out = np.flipud(out)\n",
        "    elif mode == 4:\n",
        "        # rotate 180 degree\n",
        "        out = np.rot90(image, k=2)\n",
        "    elif mode == 5:\n",
        "        # rotate 180 degree and flip\n",
        "        out = np.rot90(image, k=2)\n",
        "        out = np.flipud(out)\n",
        "    elif mode == 6:\n",
        "        # rotate 270 degree\n",
        "        out = np.rot90(image, k=3)\n",
        "    elif mode == 7:\n",
        "        # rotate 270 degree and flip\n",
        "        out = np.rot90(image, k=3)\n",
        "        out = np.flipud(out)\n",
        "    else:\n",
        "        raise Exception('Invalid choice of image transformation')\n",
        "    return out\n",
        "\n",
        "def random_augmentation(*args):\n",
        "    out = []\n",
        "    flag_aug = random.randint(1, 7)\n",
        "    for data in args:\n",
        "        out.append(data_augmentation(data, flag_aug).copy())\n",
        "    return out\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "################# DATASETS\n",
        "\n",
        "\n",
        "class InstructIRTrainDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset for Image Restoration having low-quality image and the reference image.\n",
        "    Tasks: synthetic denoising, deblurring, super-res, etc.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, args):\n",
        "\n",
        "        # assert len(hq_img_paths) == len(lq_img_paths)\n",
        "\n",
        "        super(InstructIRTrainDataset, self).__init__()\n",
        "\n",
        "        # self.hq_paths  = hq_img_paths\n",
        "        # self.lq_paths  = lq_img_paths\n",
        "        self.toTensor  = ToTensor()\n",
        "        # self.val       = val\n",
        "        # self.augs      = augmentations\n",
        "        # self.name      = name\n",
        "        # self.deg_name = deg_name\n",
        "        # self.deg_class = deg_class\n",
        "        self.noise_gradation_generator = NoiseDegradation(args)\n",
        "        self.args = args\n",
        "        self.de_type = args.de_type\n",
        "        print(self.de_type)\n",
        "\n",
        "        self._init_ids()\n",
        "        self._merge_ids()\n",
        "\n",
        "        self.crop_transform = Compose([\n",
        "            ToPILImage(),\n",
        "            RandomCrop(args.patch_size)\n",
        "        ])\n",
        "\n",
        "    def _crop_patch(self, img_1, img_2):\n",
        "        H = img_1.shape[0]\n",
        "        W = img_1.shape[1]\n",
        "        ind_H = random.randint(0, H - self.args.patch_size)\n",
        "        ind_W = random.randint(0, W - self.args.patch_size)\n",
        "\n",
        "        patch_1 = img_1[ind_H:ind_H + self.args.patch_size, ind_W:ind_W + self.args.patch_size]\n",
        "        patch_2 = img_2[ind_H:ind_H + self.args.patch_size, ind_W:ind_W + self.args.patch_size]\n",
        "\n",
        "        return patch_1, patch_2\n",
        "\n",
        "    def _get_original_rain_name(self, rainy_name):\n",
        "        og_name = rainy_name.split(\"rainy\")[0] + 'original/norain-' + rainy_name.split('rain-')[-1]\n",
        "        return og_name\n",
        "\n",
        "    def _init_clean_image_for_noise_degradation(self):\n",
        "        ref_file = self.args.data_file_dir + \"clean_image_for_denoise.txt\"\n",
        "        temp_ids = []\n",
        "        temp_ids+= [id_.strip() for id_ in open(ref_file)]\n",
        "        clean_ids = []\n",
        "        name_list = os.listdir(self.args.denoise_dir)\n",
        "        clean_ids += [self.args.denoise_dir + id_ for id_ in name_list if id_.strip() in temp_ids]\n",
        "\n",
        "        self.s15_ids = []\n",
        "        self.s25_ids = []\n",
        "        self.s50_ids = []\n",
        "\n",
        "        if 'denoise_15' in self.de_type:\n",
        "            self.s15_ids = [{\"clean_id\": x,\"de_type\":0} for x in clean_ids]\n",
        "            self.s15_ids = self.s15_ids * 1\n",
        "            random.shuffle(self.s15_ids)\n",
        "            self.s15_counter = 0\n",
        "        if 'denoise_25' in self.de_type:\n",
        "            self.s25_ids = [{\"clean_id\": x,\"de_type\":7} for x in clean_ids]\n",
        "            self.s25_ids = self.s25_ids * 1\n",
        "            random.shuffle(self.s25_ids)\n",
        "            self.s25_counter = 0\n",
        "        if 'denoise_50' in self.de_type:\n",
        "            self.s50_ids = [{\"clean_id\": x,\"de_type\":8} for x in clean_ids]\n",
        "            self.s50_ids = self.s50_ids * 1\n",
        "            random.shuffle(self.s50_ids)\n",
        "            self.s50_counter = 0\n",
        "\n",
        "\n",
        "        print(f\"Noisy Sigma 15 images len: {len(self.s15_ids)}\")\n",
        "        print(f\"Noisy Sigma 25 images len: {len(self.s25_ids)}\\n\")\n",
        "        print(f\"Noisy Sigma 50 images len: {len(self.s50_ids)}\\n\")\n",
        "\n",
        "\n",
        "    def _init_rs_ids(self):\n",
        "        temp_ids = []\n",
        "        rs = self.args.data_file_dir + \"/rainy.txt\"\n",
        "        temp_ids+= [self.args.derain_dir + id_.strip() for id_ in open(rs)]\n",
        "        self.rs_ids = [{\"clean_id\":x,\"de_type\":2} for x in temp_ids]\n",
        "        self.rs_ids = self.rs_ids * 1\n",
        "\n",
        "        self.rl_counter = 0\n",
        "        self.num_rl = len(self.rs_ids)\n",
        "        print(\"Total Rainy Ids : {}\".format(self.num_rl))\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset_ids)\n",
        "\n",
        "    def _init_ids(self):\n",
        "        if 'denoise_15' in self.de_type or 'denoise_25' in self.de_type or 'denoise_50' in self.de_type:\n",
        "            self._init_clean_image_for_noise_degradation()\n",
        "        if 'derain' in self.de_type:\n",
        "            self._init_rs_ids()\n",
        "\n",
        "        random.shuffle(self.de_type)\n",
        "\n",
        "    def _merge_ids(self):\n",
        "        self.dataset_ids = []\n",
        "        if \"denoise_15\" in self.de_type:\n",
        "            self.dataset_ids += self.s15_ids\n",
        "            self.dataset_ids += self.s25_ids\n",
        "            self.dataset_ids += self.s50_ids\n",
        "        if \"derain\" in self.de_type:\n",
        "            self.dataset_ids+= self.rs_ids\n",
        "\n",
        "        print(f\"Dataset_ids length: {len(self.dataset_ids)}\")\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        dataset = self.dataset_ids[idx]\n",
        "        hq_path = dataset[\"clean_id\"]\n",
        "        deg_id = dataset[\"de_type\"]\n",
        "\n",
        "        if deg_id == 0 or deg_id == 7 or deg_id == 8:\n",
        "            # noisy image removal\n",
        "            if deg_id == 0:\n",
        "                hq_path = dataset[\"clean_id\"]\n",
        "            elif deg_id == 7:\n",
        "                hq_path = dataset[\"clean_id\"]\n",
        "            elif deg_id == 8:\n",
        "                hq_path = dataset[\"clean_id\"]\n",
        "\n",
        "            clean_img = crop_img(np.array(Image.open(hq_path).convert('RGB')), base=16)\n",
        "            clean_patch = self.crop_transform(clean_img)\n",
        "            clean_patch= np.array(clean_patch)\n",
        "\n",
        "            clean_name = hq_path.split(\"/\")[-1].split('.')[0]\n",
        "\n",
        "            clean_patch = random_augmentation(clean_patch)[0]\n",
        "\n",
        "            degrad_patch = self.noise_gradation_generator.add_noise_degradation(clean_patch, deg_id)\n",
        "        else:\n",
        "            if deg_id == 2:\n",
        "                # Rain Streak Removal\n",
        "                    degrad_img = crop_img(np.array(Image.open(dataset[\"clean_id\"]).convert('RGB')), base=16)\n",
        "                    clean_name = self._get_original_rain_name(dataset[\"clean_id\"])\n",
        "                    clean_img = crop_img(np.array(Image.open(clean_name).convert('RGB')), base=16)\n",
        "\n",
        "            degrad_patch, clean_patch = random_augmentation(*self._crop_patch(degrad_img, clean_img))\n",
        "\n",
        "        clean_patch = self.toTensor(clean_patch)\n",
        "        degrad_patch = self.toTensor(degrad_patch)\n",
        "\n",
        "        return [clean_name, deg_id], degrad_patch, clean_patch\n",
        "\n",
        "\n",
        "        # noise_degradation_level = dataset[\"de_type\"]\n",
        "        # hq_image = load_img(hq_path)\n",
        "\n",
        "        # # randonly crop a patch from the input image\n",
        "        # clean_patch = self.crop_transform(hq_image)\n",
        "        # clean_patch= np.array(clean_patch)\n",
        "\n",
        "        # # randomly flip (vertically/horizontally) the patch\n",
        "        # clean_patch = random_augmentation(clean_patch)[0]\n",
        "        # degraded_patch = self.noise_gradation_generator.add_noise_degradation(clean_patch, noise_degradation_level)\n",
        "\n",
        "        # clean_patch = self.toTensor(clean_patch)\n",
        "        # degraded_patch = self.toTensor(degraded_patch)\n",
        "        # return degraded_patch, clean_patch, hq_path\n",
        "\n",
        "    # cannot have two differnt dataloaders\n",
        "    # so have to merge rainy, noisy images together.\n",
        "    # clean_ids are just path to image file\n",
        "    # so no need for us to pass in the actual lq_path and hq_path\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3OB24D-Btq_c"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "a8j1fmmn_cpU"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Scheduler.py\n"
      ],
      "metadata": {
        "id": "4eBow9JTvBUX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from torch.optim.lr_scheduler import _LRScheduler\n",
        "import warnings\n",
        "from typing import List\n",
        "from torch.optim import Optimizer\n",
        "\n",
        "class LinearWarmupCosineAnnealingLR(_LRScheduler):\n",
        "    \"\"\"Sets the learning rate of each parameter group to follow a linear warmup schedule between warmup_start_lr\n",
        "    and base_lr followed by a cosine annealing schedule between base_lr and eta_min.\n",
        "    .. warning::\n",
        "        It is recommended to call :func:`.step()` for :class:`LinearWarmupCosineAnnealingLR`\n",
        "        after each iteration as calling it after each epoch will keep the starting lr at\n",
        "        warmup_start_lr for the first epoch which is 0 in most cases.\n",
        "    .. warning::\n",
        "        passing epoch to :func:`.step()` is being deprecated and comes with an EPOCH_DEPRECATION_WARNING.\n",
        "        It calls the :func:`_get_closed_form_lr()` method for this scheduler instead of\n",
        "        :func:`get_lr()`. Though this does not change the behavior of the scheduler, when passing\n",
        "        epoch param to :func:`.step()`, the user should call the :func:`.step()` function before calling\n",
        "        train and validation methods.\n",
        "    Example:\n",
        "        >>> layer = nn.Linear(10, 1)\n",
        "        >>> optimizer = Adam(layer.parameters(), lr=0.02)\n",
        "        >>> scheduler = LinearWarmupCosineAnnealingLR(optimizer, warmup_epochs=10, max_epochs=40)\n",
        "        >>> #\n",
        "        >>> # the default case\n",
        "        >>> for epoch in range(40):\n",
        "        ...     # train(...)\n",
        "        ...     # validate(...)\n",
        "        ...     scheduler.step()\n",
        "        >>> #\n",
        "        >>> # passing epoch param case\n",
        "        >>> for epoch in range(40):\n",
        "        ...     scheduler.step(epoch)\n",
        "        ...     # train(...)\n",
        "        ...     # validate(...)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        optimizer: Optimizer,\n",
        "        warmup_epochs: int,\n",
        "        max_epochs: int,\n",
        "        warmup_start_lr: float = 0.0,\n",
        "        eta_min: float = 0.0,\n",
        "        last_epoch: int = -1,\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            optimizer (Optimizer): Wrapped optimizer.\n",
        "            warmup_epochs (int): Maximum number of iterations for linear warmup\n",
        "            max_epochs (int): Maximum number of iterations\n",
        "            warmup_start_lr (float): Learning rate to start the linear warmup. Default: 0.\n",
        "            eta_min (float): Minimum learning rate. Default: 0.\n",
        "            last_epoch (int): The index of last epoch. Default: -1.\n",
        "        \"\"\"\n",
        "        self.warmup_epochs = warmup_epochs\n",
        "        self.max_epochs = max_epochs\n",
        "        self.warmup_start_lr = warmup_start_lr\n",
        "        self.eta_min = eta_min\n",
        "\n",
        "        super().__init__(optimizer, last_epoch)\n",
        "\n",
        "    def get_lr(self) -> List[float]:\n",
        "        \"\"\"Compute learning rate using chainable form of the scheduler.\"\"\"\n",
        "        if not self._get_lr_called_within_step:\n",
        "            warnings.warn(\n",
        "                \"To get the last learning rate computed by the scheduler, \" \"please use `get_last_lr()`.\",\n",
        "                UserWarning,\n",
        "            )\n",
        "\n",
        "        if self.last_epoch == 0:\n",
        "            return [self.warmup_start_lr] * len(self.base_lrs)\n",
        "        if self.last_epoch < self.warmup_epochs:\n",
        "            return [\n",
        "                group[\"lr\"] + (base_lr - self.warmup_start_lr) / (self.warmup_epochs - 1)\n",
        "                for base_lr, group in zip(self.base_lrs, self.optimizer.param_groups)\n",
        "            ]\n",
        "        if self.last_epoch == self.warmup_epochs:\n",
        "            return self.base_lrs\n",
        "        if (self.last_epoch - 1 - self.max_epochs) % (2 * (self.max_epochs - self.warmup_epochs)) == 0:\n",
        "            return [\n",
        "                group[\"lr\"]\n",
        "                + (base_lr - self.eta_min) * (1 - math.cos(math.pi / (self.max_epochs - self.warmup_epochs))) / 2\n",
        "                for base_lr, group in zip(self.base_lrs, self.optimizer.param_groups)\n",
        "            ]\n",
        "\n",
        "        return [\n",
        "            (1 + math.cos(math.pi * (self.last_epoch - self.warmup_epochs) / (self.max_epochs - self.warmup_epochs)))\n",
        "            / (\n",
        "                1\n",
        "                + math.cos(\n",
        "                    math.pi * (self.last_epoch - self.warmup_epochs - 1) / (self.max_epochs - self.warmup_epochs)\n",
        "                )\n",
        "            )\n",
        "            * (group[\"lr\"] - self.eta_min)\n",
        "            + self.eta_min\n",
        "            for group in self.optimizer.param_groups\n",
        "        ]\n",
        "\n",
        "    def _get_closed_form_lr(self) -> List[float]:\n",
        "        \"\"\"Called when epoch is passed as a param to the `step` function of the scheduler.\"\"\"\n",
        "        if self.last_epoch < self.warmup_epochs:\n",
        "            return [\n",
        "                self.warmup_start_lr + self.last_epoch * (base_lr - self.warmup_start_lr) / (self.warmup_epochs - 1)\n",
        "                for base_lr in self.base_lrs\n",
        "            ]\n",
        "\n",
        "        return [\n",
        "            self.eta_min\n",
        "            + 0.5\n",
        "            * (base_lr - self.eta_min)\n",
        "            * (1 + math.cos(math.pi * (self.last_epoch - self.warmup_epochs) / (self.max_epochs - self.warmup_epochs)))\n",
        "            for base_lr in self.base_lrs\n",
        "        ]\n"
      ],
      "metadata": {
        "id": "jvy4r2TuvDMY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Offsetgenerator.py\n"
      ],
      "metadata": {
        "id": "0o4Doz6_vLkn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from torch.nn.modules.batchnorm import _BatchNorm\n",
        "from torch.nn  import functional as F\n",
        "import torch\n",
        "\n",
        "class OffsetGenerator(nn.Module):\n",
        "    def __init__(self, in_prompt_dim, out_conv_shapes):\n",
        "        \"\"\"\n",
        "        in_prompt_dim: The number of channels in the incoming prompt (e.g. prompt_dim).\n",
        "        out_conv_shapes: Some descriptor of how many weights or channels\n",
        "                         you need to offset (e.g. #channels_out * #channels_in * kernel_dim^2).\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "        out_channel = 32\n",
        "\n",
        "        # refine local patterns in the prompt\n",
        "        self.conv = nn.Conv2d(in_prompt_dim, out_channel, kernel_size=3, padding=1, stride=1)\n",
        "\n",
        "        self.global_average_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        self.output_conv_size = out_conv_shapes\n",
        "\n",
        "        self.flatten_layer = nn.Linear(in_features=out_channel, out_features=self.output_conv_size)\n",
        "\n",
        "\n",
        "    def forward(self, prompt):\n",
        "        \"\"\"\n",
        "        prompt: output from PGM with shape (B, prompt_dim, H, W)\n",
        "        returns: shape (B, out_conv_size)\n",
        "        \"\"\"\n",
        "\n",
        "        prompt = self.conv(prompt)\n",
        "\n",
        "        prompt = self.global_average_pool(prompt)\n",
        "\n",
        "        prompt = prompt.view(prompt.size(0), -1)\n",
        "\n",
        "        offset_vector = self.flatten_layer(prompt)\n",
        "\n",
        "        offset_vector = torch.tanh(offset_vector)\n",
        "\n",
        "        return offset_vector\n",
        "\n"
      ],
      "metadata": {
        "id": "HF7zIQVvvMa5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#nafnet.py\n"
      ],
      "metadata": {
        "id": "bpGe_xNBvLz2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------------------\n",
        "# Copyright (c) 2022 megvii-model. All Rights Reserved.\n",
        "# ------------------------------------------------------------------------\n",
        "# Source: https://github.com/megvii-research/NAFNet\n",
        "\n",
        "'''\n",
        "Simple Baselines for Image Restoration\n",
        "\n",
        "@article{chen2022simple,\n",
        "  title={Simple Baselines for Image Restoration},\n",
        "  author={Chen, Liangyu and Chu, Xiaojie and Zhang, Xiangyu and Sun, Jian},\n",
        "  journal={arXiv preprint arXiv:2204.04676},\n",
        "  year={2022}\n",
        "}\n",
        "'''\n",
        "\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import init as init\n",
        "# from torch.nn.modules.batchnorm import _BatchNorm\n",
        "# from models.nafnet_utils import Local_Base, LayerNorm2d\n",
        "# from models.cbin_weight import CBINorm_Conv2d\n",
        "\n",
        "# ------------------------------------------------------------------------\n",
        "# Copyright (c) 2022 megvii-model. All Rights Reserved.\n",
        "# ------------------------------------------------------------------------\n",
        "# Source: https://github.com/megvii-research/NAFNet\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "class LayerNormFunction(torch.autograd.Function):\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, weight, bias, eps):\n",
        "        ctx.eps = eps\n",
        "        N, C, H, W = x.size()\n",
        "        mu = x.mean(1, keepdim=True)\n",
        "        var = (x - mu).pow(2).mean(1, keepdim=True)\n",
        "        y = (x - mu) / (var + eps).sqrt()\n",
        "        ctx.save_for_backward(y, var, weight)\n",
        "        y = weight.view(1, C, 1, 1) * y + bias.view(1, C, 1, 1)\n",
        "        return y\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        eps = ctx.eps\n",
        "\n",
        "        N, C, H, W = grad_output.size()\n",
        "        y, var, weight = ctx.saved_variables\n",
        "        g = grad_output * weight.view(1, C, 1, 1)\n",
        "        mean_g = g.mean(dim=1, keepdim=True)\n",
        "\n",
        "        mean_gy = (g * y).mean(dim=1, keepdim=True)\n",
        "        gx = 1. / torch.sqrt(var + eps) * (g - y * mean_gy - mean_g)\n",
        "        return gx, (grad_output * y).sum(dim=3).sum(dim=2).sum(dim=0), grad_output.sum(dim=3).sum(dim=2).sum(\n",
        "            dim=0), None\n",
        "\n",
        "class LayerNorm2d(nn.Module):\n",
        "\n",
        "    def __init__(self, channels, eps=1e-6):\n",
        "        super(LayerNorm2d, self).__init__()\n",
        "        self.register_parameter('weight', nn.Parameter(torch.ones(channels)))\n",
        "        self.register_parameter('bias', nn.Parameter(torch.zeros(channels)))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        return LayerNormFunction.apply(x, self.weight, self.bias, self.eps)\n",
        "\n",
        "\n",
        "\n",
        "class AvgPool2d(nn.Module):\n",
        "    def __init__(self, kernel_size=None, base_size=None, auto_pad=True, fast_imp=False, train_size=None):\n",
        "        super().__init__()\n",
        "        self.kernel_size = kernel_size\n",
        "        self.base_size = base_size\n",
        "        self.auto_pad = auto_pad\n",
        "\n",
        "        # only used for fast implementation\n",
        "        self.fast_imp = fast_imp\n",
        "        self.rs = [5, 4, 3, 2, 1]\n",
        "        self.max_r1 = self.rs[0]\n",
        "        self.max_r2 = self.rs[0]\n",
        "        self.train_size = train_size\n",
        "\n",
        "    def extra_repr(self) -> str:\n",
        "        return 'kernel_size={}, base_size={}, stride={}, fast_imp={}'.format(\n",
        "            self.kernel_size, self.base_size, self.kernel_size, self.fast_imp\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.kernel_size is None and self.base_size:\n",
        "            train_size = self.train_size\n",
        "            if isinstance(self.base_size, int):\n",
        "                self.base_size = (self.base_size, self.base_size)\n",
        "            self.kernel_size = list(self.base_size)\n",
        "            self.kernel_size[0] = x.shape[2] * self.base_size[0] // train_size[-2]\n",
        "            self.kernel_size[1] = x.shape[3] * self.base_size[1] // train_size[-1]\n",
        "\n",
        "            # only used for fast implementation\n",
        "            self.max_r1 = max(1, self.rs[0] * x.shape[2] // train_size[-2])\n",
        "            self.max_r2 = max(1, self.rs[0] * x.shape[3] // train_size[-1])\n",
        "\n",
        "        if self.kernel_size[0] >= x.size(-2) and self.kernel_size[1] >= x.size(-1):\n",
        "            return F.adaptive_avg_pool2d(x, 1)\n",
        "\n",
        "        if self.fast_imp:  # Non-equivalent implementation but faster\n",
        "            h, w = x.shape[2:]\n",
        "            if self.kernel_size[0] >= h and self.kernel_size[1] >= w:\n",
        "                out = F.adaptive_avg_pool2d(x, 1)\n",
        "            else:\n",
        "                r1 = [r for r in self.rs if h % r == 0][0]\n",
        "                r2 = [r for r in self.rs if w % r == 0][0]\n",
        "                # reduction_constraint\n",
        "                r1 = min(self.max_r1, r1)\n",
        "                r2 = min(self.max_r2, r2)\n",
        "                s = x[:, :, ::r1, ::r2].cumsum(dim=-1).cumsum(dim=-2)\n",
        "                n, c, h, w = s.shape\n",
        "                k1, k2 = min(h - 1, self.kernel_size[0] // r1), min(w - 1, self.kernel_size[1] // r2)\n",
        "                out = (s[:, :, :-k1, :-k2] - s[:, :, :-k1, k2:] - s[:, :, k1:, :-k2] + s[:, :, k1:, k2:]) / (k1 * k2)\n",
        "                out = torch.nn.functional.interpolate(out, scale_factor=(r1, r2))\n",
        "        else:\n",
        "            n, c, h, w = x.shape\n",
        "            s = x.cumsum(dim=-1).cumsum_(dim=-2)\n",
        "            s = torch.nn.functional.pad(s, (1, 0, 1, 0))  # pad 0 for convenience\n",
        "            k1, k2 = min(h, self.kernel_size[0]), min(w, self.kernel_size[1])\n",
        "            s1, s2, s3, s4 = s[:, :, :-k1, :-k2], s[:, :, :-k1, k2:], s[:, :, k1:, :-k2], s[:, :, k1:, k2:]\n",
        "            out = s4 + s1 - s2 - s3\n",
        "            out = out / (k1 * k2)\n",
        "\n",
        "        if self.auto_pad:\n",
        "            n, c, h, w = x.shape\n",
        "            _h, _w = out.shape[2:]\n",
        "            # print(x.shape, self.kernel_size)\n",
        "            pad2d = ((w - _w) // 2, (w - _w + 1) // 2, (h - _h) // 2, (h - _h + 1) // 2)\n",
        "            out = torch.nn.functional.pad(out, pad2d, mode='replicate')\n",
        "\n",
        "        return out\n",
        "\n",
        "def replace_layers(model, base_size, train_size, fast_imp, **kwargs):\n",
        "    for n, m in model.named_children():\n",
        "        if len(list(m.children())) > 0:\n",
        "            ## compound module, go inside it\n",
        "            replace_layers(m, base_size, train_size, fast_imp, **kwargs)\n",
        "\n",
        "        if isinstance(m, nn.AdaptiveAvgPool2d):\n",
        "            pool = AvgPool2d(base_size=base_size, fast_imp=fast_imp, train_size=train_size)\n",
        "            assert m.output_size == 1\n",
        "            setattr(model, n, pool)\n",
        "\n",
        "\n",
        "'''\n",
        "ref.\n",
        "@article{chu2021tlsc,\n",
        "  title={Revisiting Global Statistics Aggregation for Improving Image Restoration},\n",
        "  author={Chu, Xiaojie and Chen, Liangyu and and Chen, Chengpeng and Lu, Xin},\n",
        "  journal={arXiv preprint arXiv:2112.04491},\n",
        "  year={2021}\n",
        "}\n",
        "'''\n",
        "class Local_Base():\n",
        "    def convert(self, *args, train_size, **kwargs):\n",
        "        replace_layers(self, *args, train_size=train_size, **kwargs)\n",
        "        imgs = torch.rand(train_size)\n",
        "        with torch.no_grad():\n",
        "            self.forward(imgs)\n",
        "\n",
        "\n",
        "class SimpleGate(nn.Module):\n",
        "    def forward(self, x):\n",
        "        x1, x2 = x.chunk(2, dim=1)\n",
        "        return x1 * x2\n",
        "\n",
        "class NAFBlock(nn.Module):\n",
        "    def __init__(self, c, DW_Expand=2, FFN_Expand=2, drop_out_rate=0., modify_conv_weights=False):\n",
        "        super().__init__()\n",
        "        dw_channel = c * DW_Expand\n",
        "        self.conv1 = nn.Conv2d(in_channels=c, out_channels=dw_channel, kernel_size=1, padding=0, stride=1, groups=1, bias=True)\n",
        "        self.conv2 = nn.Conv2d(in_channels=dw_channel, out_channels=dw_channel, kernel_size=3, padding=1, stride=1, groups=dw_channel,\n",
        "                               bias=True)\n",
        "        self.conv3 = nn.Conv2d(in_channels=dw_channel // 2, out_channels=c, kernel_size=1, padding=0, stride=1, groups=1, bias=True)\n",
        "\n",
        "        # Simplified Channel Attention\n",
        "        self.sca = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Conv2d(in_channels=dw_channel // 2, out_channels=dw_channel // 2, kernel_size=1, padding=0, stride=1,\n",
        "                      groups=1, bias=True),\n",
        "        )\n",
        "\n",
        "        # SimpleGate\n",
        "        self.sg = SimpleGate()\n",
        "\n",
        "        ffn_channel = FFN_Expand * c\n",
        "        self.conv4 = nn.Conv2d(in_channels=c, out_channels=ffn_channel, kernel_size=1, padding=0, stride=1, groups=1, bias=True)\n",
        "        self.conv5 = nn.Conv2d(in_channels=ffn_channel // 2, out_channels=c, kernel_size=1, padding=0, stride=1, groups=1, bias=True)\n",
        "\n",
        "        self.norm1 = LayerNorm2d(c)\n",
        "        self.norm2 = LayerNorm2d(c)\n",
        "\n",
        "        self.dropout1 = nn.Dropout(drop_out_rate) if drop_out_rate > 0. else nn.Identity()\n",
        "        self.dropout2 = nn.Dropout(drop_out_rate) if drop_out_rate > 0. else nn.Identity()\n",
        "\n",
        "        self.beta = nn.Parameter(torch.zeros((1, c, 1, 1)), requires_grad=True)\n",
        "        self.gamma = nn.Parameter(torch.zeros((1, c, 1, 1)), requires_grad=True)\n",
        "\n",
        "    def forward(self, inp, offset_vector=None):\n",
        "        x = inp\n",
        "\n",
        "        x = self.norm1(x)\n",
        "\n",
        "        x = self.conv1(x)\n",
        "\n",
        "        # # modifying the conv weights\n",
        "        # if self.modify_conv_weights:\n",
        "        #     self.conv1.weight.data = self.weights_modifier_1(self.conv1.weight, prompt)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.sg(x)\n",
        "        x = x * self.sca(x)\n",
        "        x = self.conv3(x)\n",
        "\n",
        "        x = self.dropout1(x)\n",
        "\n",
        "        y = inp + x * self.beta\n",
        "\n",
        "        x = self.conv4(self.norm2(y))\n",
        "        x = self.sg(x)\n",
        "\n",
        "        if offset_vector is not None:\n",
        "            base_weight = self.conv5.weight\n",
        "            # Correct the reshaping logic to match the offset vector size\n",
        "            offset_reshaped = offset_vector.view(offset_vector.shape[0], self.conv5.out_channels, self.conv5.in_channels, self.conv5.kernel_size[0], self.conv5.kernel_size[1])\n",
        "\n",
        "            # Apply offset to the base weights, considering the batch dimension\n",
        "            # This assumes you want to apply different offsets per item in the batch\n",
        "            modulated_weight = base_weight[None, ...]  + offset_reshaped\n",
        "\n",
        "\n",
        "            # Perform convolution using modulated weights for each item/image in the batch\n",
        "            x_list = []\n",
        "            for i in range(offset_vector.shape[0]):\n",
        "                x_list.append(F.conv2d(\n",
        "                    x[i:i+1], modulated_weight[i],\n",
        "                    bias=self.conv5.bias,\n",
        "                    stride=self.conv5.stride,\n",
        "                    padding=self.conv5.padding,\n",
        "                    dilation=self.conv5.dilation,\n",
        "                    groups=self.conv5.groups\n",
        "               ))\n",
        "            x = torch.cat(x_list, dim=0)\n",
        "        else:\n",
        "            x = self.conv5(x)\n",
        "\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "        return y + x * self.gamma\n",
        "\n",
        "\n",
        "class NAFNet(nn.Module):\n",
        "\n",
        "    def __init__(self, img_channel=3, width=16, middle_blk_num=1, enc_blk_nums=[], dec_blk_nums=[]):\n",
        "        super().__init__()\n",
        "\n",
        "        self.intro = nn.Conv2d(in_channels=img_channel, out_channels=width, kernel_size=3, padding=1, stride=1, groups=1,\n",
        "                              bias=True)\n",
        "        self.ending = nn.Conv2d(in_channels=width, out_channels=img_channel, kernel_size=3, padding=1, stride=1, groups=1,\n",
        "                              bias=True)\n",
        "\n",
        "        self.encoders = nn.ModuleList()\n",
        "        self.decoders = nn.ModuleList()\n",
        "        self.middle_blks = nn.ModuleList()\n",
        "        self.ups = nn.ModuleList()\n",
        "        self.downs = nn.ModuleList()\n",
        "\n",
        "        chan = width\n",
        "        for num in enc_blk_nums:\n",
        "            self.encoders.append(\n",
        "                nn.Sequential(\n",
        "                    *[NAFBlock(chan) for _ in range(num)]\n",
        "                )\n",
        "            )\n",
        "            self.downs.append(\n",
        "                nn.Conv2d(chan, 2*chan, 2, 2)\n",
        "            )\n",
        "            chan = chan * 2\n",
        "\n",
        "        self.middle_blks = \\\n",
        "            nn.Sequential(\n",
        "                *[NAFBlock(chan) for _ in range(middle_blk_num)]\n",
        "            )\n",
        "\n",
        "        for num in dec_blk_nums:\n",
        "            self.ups.append(\n",
        "                nn.Sequential(\n",
        "                    nn.Conv2d(chan, chan * 2, 1, bias=False),\n",
        "                    nn.PixelShuffle(2)\n",
        "                )\n",
        "            )\n",
        "            chan = chan // 2\n",
        "            self.decoders.append(\n",
        "                nn.Sequential(\n",
        "                    *[NAFBlock(chan) for _ in range(num)]\n",
        "                )\n",
        "            )\n",
        "\n",
        "        self.padder_size = 2 ** len(self.encoders)\n",
        "\n",
        "    def forward(self, inp):\n",
        "        B, C, H, W = inp.shape\n",
        "        inp = self.check_image_size(inp)\n",
        "\n",
        "        x = self.intro(inp)\n",
        "\n",
        "        encs = []\n",
        "\n",
        "        for encoder, down in zip(self.encoders, self.downs):\n",
        "            x = encoder(x)\n",
        "            encs.append(x)\n",
        "            x = down(x)\n",
        "\n",
        "        x = self.middle_blks(x)\n",
        "\n",
        "        for decoder, up, enc_skip in zip(self.decoders, self.ups, encs[::-1]):\n",
        "            x = up(x)\n",
        "            x = x + enc_skip\n",
        "            x = decoder(x)\n",
        "\n",
        "        x = self.ending(x)\n",
        "        x = x + inp\n",
        "\n",
        "        return x[:, :, :H, :W]\n",
        "\n",
        "    def check_image_size(self, x):\n",
        "        _, _, h, w = x.size()\n",
        "        mod_pad_h = (self.padder_size - h % self.padder_size) % self.padder_size\n",
        "        mod_pad_w = (self.padder_size - w % self.padder_size) % self.padder_size\n",
        "        x = F.pad(x, (0, mod_pad_w, 0, mod_pad_h))\n",
        "        return x\n",
        "\n",
        "class NAFNetLocal(Local_Base, NAFNet):\n",
        "    def __init__(self, *args, train_size=(1, 3, 256, 256), fast_imp=False, **kwargs):\n",
        "        Local_Base.__init__(self)\n",
        "        NAFNet.__init__(self, *args, **kwargs)\n",
        "\n",
        "        N, C, H, W = train_size\n",
        "        base_size = (int(H * 1.5), int(W * 1.5))\n",
        "\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            self.convert(base_size=base_size, train_size=train_size, fast_imp=fast_imp)\n",
        "\n",
        "\n",
        "def create_nafnet(input_channels = 3, width = 32, enc_blks = [2, 2, 4, 8], middle_blk_num = 12, dec_blks = [2, 2, 2, 2]):\n",
        "    \"\"\"\n",
        "    Create Nafnet model\n",
        "    https://github.com/megvii-research/NAFNet/blob/main/options/test/SIDD/NAFNet-width32.yml\n",
        "    \"\"\"\n",
        "\n",
        "    net = NAFNet(img_channel=input_channels, width=width, middle_blk_num=middle_blk_num,\n",
        "                      enc_blk_nums=enc_blks, dec_blk_nums=dec_blks)\n",
        "\n",
        "    # inp_shape = (3, 256, 256)\n",
        "\n",
        "    # from ptflops import get_model_complexity_info\n",
        "\n",
        "    # macs, params = get_model_complexity_info(net, inp_shape, verbose=False, print_per_layer_stat=False)\n",
        "\n",
        "    # params = float(params[:-3])\n",
        "    # macs = float(macs[:-4])\n",
        "\n",
        "    # print(macs, params)\n",
        "\n",
        "    return net"
      ],
      "metadata": {
        "id": "9G1Ldau8vM2b"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#InstructIR.py"
      ],
      "metadata": {
        "id": "7jktTXnLvFxn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import init as init\n",
        "# from models.OffsetGenerator import OffsetGenerator\n",
        "# from models.nafnet import NAFBlock\n",
        "\n",
        "# ------------------------------------------------------------------------\n",
        "# Copyright (c) 2023 va1shn9v. All Rights Reserved.\n",
        "# ------------------------------------------------------------------------\n",
        "# Source: https://github.com/va1shn9v/PromptIR\n",
        "\n",
        "'''\n",
        "@inproceedings{potlapalli2023promptir,\n",
        "  title={PromptIR: Prompting for All-in-One Image Restoration},\n",
        "  author={Potlapalli, Vaishnav and Zamir, Syed Waqas and Khan, Salman and Khan, Fahad},\n",
        "  booktitle={Thirty-seventh Conference on Neural Information Processing Systems},\n",
        "  year={2023}\n",
        "}\n",
        "'''\n",
        "##---------- Prompt Gen Module -----------------------\n",
        "class PromptGenBlock(nn.Module):\n",
        "    def __init__(self,prompt_dim=128,prompt_len=5,prompt_size = 96,lin_dim = 192):\n",
        "        super(PromptGenBlock,self).__init__()\n",
        "        self.prompt_dim= prompt_dim\n",
        "        # prompt_dim=128: Defines the number of channels in the prompt.\n",
        "        # prompt_len=5: The number of different prompts available.\n",
        "        # prompt_size=96: The spatial resolution of each prompt (assumed to be square: 96×96).\n",
        "        # lin_dim=192: The input dimension for the linear layer.\n",
        "        # prompt_param's size = 1 * N * C * H * W where N = number of prompt components\n",
        "        # prompt_param = learnable parameters\n",
        "        self.prompt_param = nn.Parameter(torch.rand(1,prompt_len,prompt_dim,prompt_size,prompt_size))\n",
        "        # A linear layer takes an input of size \"lin_dim\" and produces \"prompt_len\" outputs.\n",
        "        # This layer generates weights that determine the importance of each prompt.\n",
        "        self.linear_layer = nn.Linear(lin_dim,prompt_len)\n",
        "        # A 3×3 convolution with the same number of input and output channels (prompt_dim).\n",
        "        # Stride =1 and padding =1 ensure the spatial size remains unchanged.\n",
        "        self.conv3x3 = nn.Conv2d(prompt_dim,prompt_dim,kernel_size=3,stride=1,padding=1,bias=False)\n",
        "\n",
        "    # During training, the PromptGenBlock learns to encode prompt_len different degradations into the prompt_param tensor.\n",
        "    def forward(self,x):\n",
        "        # x = image feature representation\n",
        "        B,C,H,W = x.shape\n",
        "        # x is averaged over the last two dimensions (H, W). The output tensor is of shape (B, C) => global descriptor of the input\n",
        "        emb = x.mean(dim=(-2,-1))\n",
        "        # the embedding is passed through the linear layer to produce a tensor of shape (B, prompt_len)\n",
        "        # softmax() ensures values in the linear layer output sum to 1 across the \"prompt_len\" dimension\n",
        "        # meaning each prompt gets an importance weight for each sample in the batch\n",
        "        prompt_weights = F.softmax(self.linear_layer(emb),dim=1)\n",
        "        # prompt_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1) expands the dimensions of prompt_weights to shape (B, prompt_len, 1, 1, 1).\n",
        "        # self.prompt_param.unsqueeze(0).repeat(B, 1, 1, 1, 1, 1).squeeze(1):\n",
        "        # Expands prompt_param to (B, prompt_len, prompt_dim, prompt_size, prompt_size), so each batch element has its own copy.\n",
        "        # repeat(B, 1, 1, 1, 1, 1) replicates the prompts across the batch.\n",
        "        # The two tensors are multiplied element-wise, meaning each prompt is weighted by the computed prompt_weights.\n",
        "        prompt = prompt_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1) * self.prompt_param.unsqueeze(0).repeat(B,1,1,1,1,1).squeeze(1)\n",
        "        # The weighted sum across prompt_len produces a single prompt tensor for each batch element.\n",
        "        # The new shape is (B, prompt_dim, prompt_size, prompt_size).\n",
        "        prompt = torch.sum(prompt,dim=1)\n",
        "        # resize the spatial dimentions of prompt (prompt_size, prompt_size) to (H, W) while keeping B and prompt_dim unchanged.\n",
        "        prompt = F.interpolate(prompt,(H,W),mode=\"bilinear\")\n",
        "        # The learned prompt undergoes a 3×3 convolution for further processing without changing the shape of \"prompt\"\n",
        "        prompt = self.conv3x3(prompt)\n",
        "        # final shape of prompt = (B, prompt_dim, H, W)\n",
        "        return prompt\n",
        "        # If multiple degradations exist in the input image feature, the returned prompt will encode a mixture of the degradations it has learned during training\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##########################################################################\n",
        "\n",
        "class ICB(nn.Module):\n",
        "    \"\"\"\n",
        "    Instruction Condition Block (ICB)\n",
        "    Paper Section 3.3\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, feature_dim, text_dim=768):\n",
        "        super(ICB, self).__init__()\n",
        "        self.fc    = nn.Linear(text_dim, feature_dim)\n",
        "        self.block = NAFBlock(feature_dim)\n",
        "        self.beta  = nn.Parameter(torch.zeros((1, feature_dim, 1, 1)), requires_grad=True)\n",
        "        self.gamma = nn.Parameter(torch.zeros((1, feature_dim, 1, 1)), requires_grad=True)\n",
        "    # f' = Block(f * mc) + f\n",
        "    # mc = sigmoid(Wc * emb)\n",
        "    def forward(self, x, text_embedding):\n",
        "        gating_factors = torch.sigmoid(self.fc(text_embedding))\n",
        "        # mc\n",
        "        gating_factors = gating_factors.unsqueeze(-1).unsqueeze(-1)\n",
        "\n",
        "\n",
        "        f = x * self.gamma + self.beta  # 1) learned feature scaling/modulation\n",
        "        f = f * gating_factors          # 2) (soft) feature routing based on text\n",
        "        f = self.block(f)               # 3) block feature enhancement\n",
        "        return f + x # skip connection\n",
        "\n",
        "\n",
        "class InstructIR(nn.Module):\n",
        "    \"\"\"\n",
        "    InstructIR model using NAFNet (ECCV 2022) as backbone.\n",
        "    The model takes as input an RGB image and a text embedding (encoded instruction).\n",
        "    Described in Paper Section 3.3\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, img_channel=3, width=16, middle_blk_num=1, enc_blk_nums=[], dec_blk_nums=[], txtdim=768, include_offset=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.intro  = nn.Conv2d(in_channels=img_channel, out_channels=width, kernel_size=3, padding=1, stride=1, groups=1,\n",
        "                              bias=True)\n",
        "        self.ending = nn.Conv2d(in_channels=width, out_channels=img_channel, kernel_size=3, padding=1, stride=1, groups=1,\n",
        "                              bias=True)\n",
        "\n",
        "        self.encoders    = nn.ModuleList()\n",
        "        self.decoders    = nn.ModuleList()\n",
        "        self.middle_blks = nn.ModuleList()\n",
        "        self.ups         = nn.ModuleList()\n",
        "        self.downs       = nn.ModuleList()\n",
        "        self.enc_cond    = nn.ModuleList()\n",
        "        self.dec_cond    = nn.ModuleList()\n",
        "\n",
        "        self.include_offset = include_offset\n",
        "\n",
        "        chan = width\n",
        "\n",
        "        # if include_offset is True:\n",
        "        self.prompt_block_level1 = PromptGenBlock(prompt_dim=chan*2,prompt_len=3,prompt_size = 128,lin_dim = chan*2)\n",
        "        self.prompt_block_level2 = PromptGenBlock(prompt_dim=chan*4,prompt_len=3,prompt_size = 64,lin_dim = chan*4)\n",
        "        self.prompt_block_level3 = PromptGenBlock(prompt_dim=chan*8,prompt_len=3,prompt_size = 32,lin_dim = chan*8)\n",
        "\n",
        "        # prompt_dim_level3 = chan*2**3\n",
        "\n",
        "        self.promptBlocks = nn.ModuleList()\n",
        "        self.promptBlocks.append(self.prompt_block_level3)\n",
        "        self.promptBlocks.append(self.prompt_block_level2)\n",
        "        self.promptBlocks.append(self.prompt_block_level1)\n",
        "\n",
        "        # self.promptBlocks = [self.prompt_block_level3, self.prompt_block_level2, self.prompt_block_level1]\n",
        "\n",
        "        for num in enc_blk_nums:\n",
        "            #  Each encoder applies multiple NAFBlocks\n",
        "            self.encoders.append(\n",
        "                nn.Sequential(\n",
        "                    *[NAFBlock(chan) for _ in range(num)]\n",
        "                )\n",
        "            )\n",
        "            # Each encoding layer is modulated using a corresponding ICB\n",
        "            # which incorporates the provided text embeddings.\n",
        "            self.enc_cond.append(ICB(chan, txtdim))\n",
        "            # Downsampling layers that reduce spatial resolution while increasing channel dimensions.\n",
        "            self.downs.append(\n",
        "                nn.Conv2d(chan, 2*chan, 2, 2)\n",
        "            )\n",
        "            chan = chan * 2\n",
        "        # Middle blocks: a series of NAFBlocks applied to the deepest,\n",
        "        # most abstract representation of the image features.\n",
        "        # print(f\"middle block chan: {chan}\")\n",
        "        self.middle_blks = nn.Sequential(\n",
        "            *[NAFBlock(chan) for _ in range(middle_blk_num)]\n",
        "        )\n",
        "\n",
        "        naf_block = self.middle_blks[0]\n",
        "        cIn = naf_block.conv5.in_channels\n",
        "        cOut = naf_block.conv5.out_channels\n",
        "        kernel_size = naf_block.conv5.kernel_size\n",
        "        vector_size = cIn * cOut * kernel_size[0] * kernel_size[1]\n",
        "\n",
        "        self.middleblock_offsetGen=OffsetGenerator(in_prompt_dim=chan, out_conv_shapes=vector_size)\n",
        "\n",
        "\n",
        "        self.prompt_block_middle_blks = PromptGenBlock(prompt_dim=chan,prompt_len=3,prompt_size = 32,lin_dim = chan)\n",
        "\n",
        "        # decoding path\n",
        "        for num in dec_blk_nums:\n",
        "            # Upsampling layers that increase the spatial resolution and\n",
        "            # decrease the channel dimensions\n",
        "            self.ups.append(\n",
        "                nn.Sequential(\n",
        "                    nn.Conv2d(chan, chan * 2, 1, bias=False),\n",
        "                    nn.PixelShuffle(2)\n",
        "                )\n",
        "            )\n",
        "            chan = chan // 2\n",
        "            # sequantially processes upsampled features using multiple NAFBlocks\n",
        "            self.decoders.append(\n",
        "                nn.Sequential(\n",
        "                    *[NAFBlock(chan) for _ in range(num)]\n",
        "                )\n",
        "            )\n",
        "            # Add text embedding as modulation\n",
        "            self.dec_cond.append(ICB(chan, txtdim))\n",
        "\n",
        "        self.padder_size = 2 ** len(self.encoders)\n",
        "\n",
        "\n",
        "        # self.offset_generators = []\n",
        "        self.offset_generators = nn.ModuleList()\n",
        "\n",
        "        # if include_offset is True:\n",
        "        for i, decoder in enumerate(self.decoders):\n",
        "            if(i <3):\n",
        "                naf_block = decoder[0]\n",
        "\n",
        "                cIn = naf_block.conv5.in_channels\n",
        "                cOut = naf_block.conv5.out_channels\n",
        "                kernel_size = naf_block.conv5.kernel_size\n",
        "\n",
        "                vector_size = cIn * cOut * kernel_size[0] * kernel_size[1]\n",
        "\n",
        "                # in_prompt_dim = self.promptBlocks[i].prompt_dim\n",
        "\n",
        "                self.offset_generators.append(OffsetGenerator(in_prompt_dim=self.promptBlocks[i].prompt_dim, out_conv_shapes=vector_size))\n",
        "\n",
        "\n",
        "    def forward(self, inp, txtembd):\n",
        "        B, C, H, W = inp.shape\n",
        "        inp = self.check_image_size(inp)\n",
        "        # intro = a convolutional layer to preprocess the input image\n",
        "        x = self.intro(inp)\n",
        "        encs = []\n",
        "\n",
        "        for encoder, enc_mod, down in zip(self.encoders, self.enc_cond, self.downs):\n",
        "            x = encoder(x)\n",
        "            x = enc_mod(x, txtembd)\n",
        "            encs.append(x)\n",
        "            x = down(x)\n",
        "\n",
        "\n",
        "        if self.include_offset:\n",
        "            # print(\"middle block include_offset\")\n",
        "            middle_block_prompt = self.prompt_block_middle_blks(x)\n",
        "            # print(\"after prompt_block_level3\")\n",
        "            offset_vector = self.middleblock_offsetGen(middle_block_prompt)\n",
        "            # print(\"after offset_generators[0](middle_block_prompt)\")\n",
        "            for naf_block in self.middle_blks:\n",
        "                x = naf_block(x, offset_vector)\n",
        "        else:\n",
        "            x = self.middle_blks(x)\n",
        "\n",
        "        index = 0\n",
        "        for decoder, up, enc_skip, dec_mod in zip(self.decoders, self.ups, encs[::-1], self.dec_cond):\n",
        "\n",
        "            if self.include_offset is True:\n",
        "                # print(f\"decoder include_offset: {self.include_offset}\")\n",
        "                x = up(x)\n",
        "                # offset_vector = None\n",
        "                # if (index < 3):\n",
        "                #     degradation_aware_prompt = self.promptBlocks[index](x)\n",
        "                #     offset_vector = self.offset_generators[index](degradation_aware_prompt)\n",
        "                #     index += 1\n",
        "\n",
        "                x = x + enc_skip\n",
        "\n",
        "                offset_vector = None\n",
        "                if (index < 3):\n",
        "                    degradation_aware_prompt = self.promptBlocks[index](x)\n",
        "                    offset_vector = self.offset_generators[index](degradation_aware_prompt)\n",
        "                    index += 1\n",
        "\n",
        "                if offset_vector is not None:\n",
        "                    # print(\"inside 'if offset_vector is not None:'\")\n",
        "                    for naf_block in decoder:\n",
        "                        x = naf_block(x, offset_vector)\n",
        "                else:\n",
        "                    x = decoder(x)\n",
        "\n",
        "                x = dec_mod(x, txtembd)\n",
        "            else:\n",
        "                x = up(x)\n",
        "                x = x + enc_skip\n",
        "                x = decoder(x)\n",
        "                x = dec_mod(x, txtembd)\n",
        "\n",
        "        # ending = conv layer to postprocess the final decoded feature into the desired image format.\n",
        "        x = self.ending(x)\n",
        "        x = x + inp\n",
        "\n",
        "        return x[:, :, :H, :W]\n",
        "\n",
        "    def check_image_size(self, x):\n",
        "        _, _, h, w = x.size()\n",
        "        mod_pad_h = (self.padder_size - h % self.padder_size) % self.padder_size\n",
        "        mod_pad_w = (self.padder_size - w % self.padder_size) % self.padder_size\n",
        "        x = F.pad(x, (0, mod_pad_w, 0, mod_pad_h))\n",
        "        return x\n",
        "\n",
        "\n",
        "def create_model(input_channels = 3, width = 32, enc_blks = [2, 2, 4, 8], middle_blk_num = 12, dec_blks = [2, 2, 2, 2], txtdim=768, include_offset=False):\n",
        "\n",
        "    net = InstructIR(img_channel=input_channels, width=width, middle_blk_num=middle_blk_num,\n",
        "                      enc_blk_nums=enc_blks, dec_blk_nums=dec_blks, txtdim=txtdim, include_offset=include_offset)\n",
        "\n",
        "    return net"
      ],
      "metadata": {
        "id": "BsYQBi97vJS1"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Text model"
      ],
      "metadata": {
        "id": "vcctxh72vfpm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import DistilBertModel, DistilBertTokenizer, AutoModel, AutoTokenizer\n",
        "import os\n",
        "\n",
        "# Models that use mean pooling\n",
        "POOL_MODELS = {\"sentence-transformers/all-MiniLM-L6-v2\", \"TaylorAI/bge-micro-v2\"}\n",
        "\n",
        "#Mean Pooling - Take attention mask into account for correct averaging\n",
        "def mean_pooling(model_output, attention_mask):\n",
        "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
        "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "\n",
        "\n",
        "class LanguageModel(nn.Module):\n",
        "    def __init__(self, model='distilbert-base-uncased'):\n",
        "        super(LanguageModel, self).__init__()\n",
        "\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "        self.model = AutoModel.from_pretrained(model)\n",
        "        self.model_name = model\n",
        "        # Remove the CLIP vision tower\n",
        "        if \"clip\" in self.model_name:\n",
        "            self.model.vision_model = None\n",
        "        # Freeze the pre-trained parameters (very important)\n",
        "        for param in self.model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Make sure to set evaluation mode (also important)\n",
        "        self.model.eval()\n",
        "\n",
        "    def forward(self, text_batch):\n",
        "        inputs = self.tokenizer(text_batch, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "        # Get the device of the model's parameters (assumes all parameters are on the same device)\n",
        "        device = next(self.model.parameters()).device\n",
        "        # Move all inputs to that device\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "        with torch.no_grad(): # Ensure no gradients are computed for this forward pass\n",
        "\n",
        "            if \"clip\" in self.model_name:\n",
        "                sentence_embedding = self.model.get_text_features(**inputs)\n",
        "                return sentence_embedding\n",
        "\n",
        "            outputs = self.model(**inputs)\n",
        "\n",
        "        if any(model in self.model_name for model in POOL_MODELS):\n",
        "            sentence_embeddings = mean_pooling(outputs, inputs['attention_mask'])\n",
        "            # Normalize embeddings\n",
        "            sentence_embedding = F.normalize(sentence_embeddings, p=2, dim=1)\n",
        "        else:\n",
        "            sentence_embedding = outputs.last_hidden_state[:, 0, :]\n",
        "        return sentence_embedding\n",
        "\n",
        "\n",
        "class LMHead(nn.Module):\n",
        "    def __init__(self, embedding_dim=384, hidden_dim=256, num_classes=4):\n",
        "        super(LMHead, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n",
        "        #self.gelu = nn.GELU()\n",
        "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embd = self.fc1(x)\n",
        "        embd = F.normalize(embd, p=2, dim=1)\n",
        "        deg_pred = self.fc2(embd)\n",
        "        return embd, deg_pred"
      ],
      "metadata": {
        "id": "OjsxiX_Tvfwg"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Install transformers & setup \"config\""
      ],
      "metadata": {
        "id": "-j51A8RlvgEx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install pytorch-msssim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOYVD3y2vgM3",
        "outputId": "2a1ecadd-c512-4c1c-a22a-55485ad38950",
        "collapsed": true
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: pytorch-msssim in /usr/local/lib/python3.11/dist-packages (1.0.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from pytorch-msssim) (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->pytorch-msssim) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch-msssim) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->pytorch-msssim) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch-msssim) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->pytorch-msssim) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch-msssim) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch-msssim) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch-msssim) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch-msssim) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch-msssim) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch-msssim) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch-msssim) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch-msssim) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch-msssim) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch-msssim) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch-msssim) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch-msssim) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch-msssim) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch-msssim) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch-msssim) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->pytorch-msssim) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->pytorch-msssim) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Test Performance"
      ],
      "metadata": {
        "id": "dRZIsJzsRfIf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#metrics.py\n"
      ],
      "metadata": {
        "id": "2mffJo8lXv2Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import cv2\n",
        "import torch\n",
        "\n",
        "\n",
        "def np_psnr(y_true, y_pred, maxval=1.):\n",
        "    mse = np.mean((y_true - y_pred) ** 2)\n",
        "    if(mse == 0):\n",
        "        return np.inf\n",
        "\n",
        "    psnr = 20 * np.log10(maxval / np.sqrt(mse))\n",
        "    return psnr\n",
        "\n",
        "def pt_psnr(y_true, y_pred, maxval=1.):\n",
        "    mse = torch.mean((y_true - y_pred) ** 2, dim=(1, 2, 3))\n",
        "    psnr = 20 * torch.log10(maxval / torch.sqrt(mse))\n",
        "    return psnr.unsqueeze(1)\n",
        "\n",
        "\n",
        "############# SWINIR METRICS\n",
        "# https://github.com/JingyunLiang/SwinIR/blob/6545850fbf8df298df73d81f3e8cba638787c8bd/utils/util_calculate_psnr_ssim.py#L243\n",
        "\n",
        "\n",
        "def calculate_psnr(img1, img2, crop_border, input_order='HWC', test_y_channel=False):\n",
        "    \"\"\"Calculate PSNR (Peak Signal-to-Noise Ratio).\n",
        "\n",
        "    Ref: https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio\n",
        "\n",
        "    Args:\n",
        "        img1 (ndarray): Images with range [0, 255].\n",
        "        img2 (ndarray): Images with range [0, 255].\n",
        "        crop_border (int): Cropped pixels in each edge of an image. These\n",
        "            pixels are not involved in the PSNR calculation.\n",
        "        input_order (str): Whether the input order is 'HWC' or 'CHW'.\n",
        "            Default: 'HWC'.\n",
        "        test_y_channel (bool): Test on Y channel of YCbCr. Default: False.\n",
        "\n",
        "    Returns:\n",
        "        float: psnr result.\n",
        "    \"\"\"\n",
        "\n",
        "    assert img1.shape == img2.shape, (f'Image shapes are differnet: {img1.shape}, {img2.shape}.')\n",
        "    if input_order not in ['HWC', 'CHW']:\n",
        "        raise ValueError(f'Wrong input_order {input_order}. Supported input_orders are ' '\"HWC\" and \"CHW\"')\n",
        "    img1 = reorder_image(img1, input_order=input_order)\n",
        "    img2 = reorder_image(img2, input_order=input_order)\n",
        "    img1 = img1.astype(np.float64)\n",
        "    img2 = img2.astype(np.float64)\n",
        "\n",
        "    if crop_border != 0:\n",
        "        img1 = img1[crop_border:-crop_border, crop_border:-crop_border, ...]\n",
        "        img2 = img2[crop_border:-crop_border, crop_border:-crop_border, ...]\n",
        "\n",
        "    if test_y_channel:\n",
        "        img1 = to_y_channel(img1)\n",
        "        img2 = to_y_channel(img2)\n",
        "\n",
        "    mse = np.mean((img1 - img2) ** 2)\n",
        "    if mse == 0:\n",
        "        return float('inf')\n",
        "    return 20. * np.log10(255. / np.sqrt(mse))\n",
        "\n",
        "\n",
        "def _ssim(img1, img2):\n",
        "    \"\"\"Calculate SSIM (structural similarity) for one channel images.\n",
        "\n",
        "    It is called by func:`calculate_ssim`.\n",
        "\n",
        "    Args:\n",
        "        img1 (ndarray): Images with range [0, 255] with order 'HWC'.\n",
        "        img2 (ndarray): Images with range [0, 255] with order 'HWC'.\n",
        "\n",
        "    Returns:\n",
        "        float: ssim result.\n",
        "    \"\"\"\n",
        "\n",
        "    C1 = (0.01 * 255) ** 2\n",
        "    C2 = (0.03 * 255) ** 2\n",
        "\n",
        "    img1 = img1.astype(np.float64)\n",
        "    img2 = img2.astype(np.float64)\n",
        "    kernel = cv2.getGaussianKernel(11, 1.5)\n",
        "    window = np.outer(kernel, kernel.transpose())\n",
        "\n",
        "    mu1 = cv2.filter2D(img1, -1, window)[5:-5, 5:-5]\n",
        "    mu2 = cv2.filter2D(img2, -1, window)[5:-5, 5:-5]\n",
        "    mu1_sq = mu1 ** 2\n",
        "    mu2_sq = mu2 ** 2\n",
        "    mu1_mu2 = mu1 * mu2\n",
        "    sigma1_sq = cv2.filter2D(img1 ** 2, -1, window)[5:-5, 5:-5] - mu1_sq\n",
        "    sigma2_sq = cv2.filter2D(img2 ** 2, -1, window)[5:-5, 5:-5] - mu2_sq\n",
        "    sigma12 = cv2.filter2D(img1 * img2, -1, window)[5:-5, 5:-5] - mu1_mu2\n",
        "\n",
        "    ssim_map = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / ((mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2))\n",
        "    return ssim_map.mean()\n",
        "\n",
        "\n",
        "def calculate_ssim(img1, img2, crop_border, input_order='HWC', test_y_channel=False):\n",
        "    \"\"\"Calculate SSIM (structural similarity).\n",
        "\n",
        "    Ref:\n",
        "    Image quality assessment: From error visibility to structural similarity\n",
        "\n",
        "    The results are the same as that of the official released MATLAB code in\n",
        "    https://ece.uwaterloo.ca/~z70wang/research/ssim/.\n",
        "\n",
        "    For three-channel images, SSIM is calculated for each channel and then\n",
        "    averaged.\n",
        "\n",
        "    Args:\n",
        "        img1 (ndarray): Images with range [0, 255].\n",
        "        img2 (ndarray): Images with range [0, 255].\n",
        "        crop_border (int): Cropped pixels in each edge of an image. These\n",
        "            pixels are not involved in the SSIM calculation.\n",
        "        input_order (str): Whether the input order is 'HWC' or 'CHW'.\n",
        "            Default: 'HWC'.\n",
        "        test_y_channel (bool): Test on Y channel of YCbCr. Default: False.\n",
        "\n",
        "    Returns:\n",
        "        float: ssim result.\n",
        "    \"\"\"\n",
        "\n",
        "    assert img1.shape == img2.shape, (f'Image shapes are differnet: {img1.shape}, {img2.shape}.')\n",
        "    if input_order not in ['HWC', 'CHW']:\n",
        "        raise ValueError(f'Wrong input_order {input_order}. Supported input_orders are ' '\"HWC\" and \"CHW\"')\n",
        "    img1 = reorder_image(img1, input_order=input_order)\n",
        "    img2 = reorder_image(img2, input_order=input_order)\n",
        "    img1 = img1.astype(np.float64)\n",
        "    img2 = img2.astype(np.float64)\n",
        "\n",
        "    if crop_border != 0:\n",
        "        img1 = img1[crop_border:-crop_border, crop_border:-crop_border, ...]\n",
        "        img2 = img2[crop_border:-crop_border, crop_border:-crop_border, ...]\n",
        "\n",
        "    if test_y_channel:\n",
        "        img1 = to_y_channel(img1)\n",
        "        img2 = to_y_channel(img2)\n",
        "\n",
        "    ssims = []\n",
        "    for i in range(img1.shape[2]):\n",
        "        ssims.append(_ssim(img1[..., i], img2[..., i]))\n",
        "    return np.array(ssims).mean()\n",
        "\n",
        "\n",
        "def reorder_image(img, input_order='HWC'):\n",
        "    \"\"\"Reorder images to 'HWC' order.\n",
        "\n",
        "    If the input_order is (h, w), return (h, w, 1);\n",
        "    If the input_order is (c, h, w), return (h, w, c);\n",
        "    If the input_order is (h, w, c), return as it is.\n",
        "\n",
        "    Args:\n",
        "        img (ndarray): Input image.\n",
        "        input_order (str): Whether the input order is 'HWC' or 'CHW'.\n",
        "            If the input image shape is (h, w), input_order will not have\n",
        "            effects. Default: 'HWC'.\n",
        "\n",
        "    Returns:\n",
        "        ndarray: reordered image.\n",
        "    \"\"\"\n",
        "\n",
        "    if input_order not in ['HWC', 'CHW']:\n",
        "        raise ValueError(f'Wrong input_order {input_order}. Supported input_orders are ' \"'HWC' and 'CHW'\")\n",
        "    if len(img.shape) == 2:\n",
        "        img = img[..., None]\n",
        "    if input_order == 'CHW':\n",
        "        img = img.transpose(1, 2, 0)\n",
        "    return img\n",
        "\n",
        "\n",
        "def to_y_channel(img):\n",
        "    \"\"\"Change to Y channel of YCbCr.\n",
        "\n",
        "    Args:\n",
        "        img (ndarray): Images with range [0, 255].\n",
        "\n",
        "    Returns:\n",
        "        (ndarray): Images with range [0, 255] (float type) without round.\n",
        "    \"\"\"\n",
        "    if np.max(img) > 1.:\n",
        "        img = img.astype(np.float32) / 255.\n",
        "\n",
        "    if img.ndim == 3 and img.shape[2] == 3:\n",
        "        img = bgr2ycbcr(img, y_only=True)\n",
        "        img = img[..., None]\n",
        "    return img * 255.\n",
        "\n",
        "\n",
        "def _convert_input_type_range(img):\n",
        "    \"\"\"Convert the type and range of the input image.\n",
        "\n",
        "    It converts the input image to np.float32 type and range of [0, 1].\n",
        "    It is mainly used for pre-processing the input image in colorspace\n",
        "    convertion functions such as rgb2ycbcr and ycbcr2rgb.\n",
        "\n",
        "    Args:\n",
        "        img (ndarray): The input image. It accepts:\n",
        "            1. np.uint8 type with range [0, 255];\n",
        "            2. np.float32 type with range [0, 1].\n",
        "\n",
        "    Returns:\n",
        "        (ndarray): The converted image with type of np.float32 and range of\n",
        "            [0, 1].\n",
        "    \"\"\"\n",
        "    img_type = img.dtype\n",
        "    img = img.astype(np.float32)\n",
        "    if img_type == np.float32:\n",
        "        pass\n",
        "    elif img_type == np.uint8:\n",
        "        img /= 255.\n",
        "    else:\n",
        "        raise TypeError('The img type should be np.float32 or np.uint8, ' f'but got {img_type}')\n",
        "    return img\n",
        "\n",
        "\n",
        "def _convert_output_type_range(img, dst_type):\n",
        "    \"\"\"Convert the type and range of the image according to dst_type.\n",
        "\n",
        "    It converts the image to desired type and range. If `dst_type` is np.uint8,\n",
        "    images will be converted to np.uint8 type with range [0, 255]. If\n",
        "    `dst_type` is np.float32, it converts the image to np.float32 type with\n",
        "    range [0, 1].\n",
        "    It is mainly used for post-processing images in colorspace convertion\n",
        "    functions such as rgb2ycbcr and ycbcr2rgb.\n",
        "\n",
        "    Args:\n",
        "        img (ndarray): The image to be converted with np.float32 type and\n",
        "            range [0, 255].\n",
        "        dst_type (np.uint8 | np.float32): If dst_type is np.uint8, it\n",
        "            converts the image to np.uint8 type with range [0, 255]. If\n",
        "            dst_type is np.float32, it converts the image to np.float32 type\n",
        "            with range [0, 1].\n",
        "\n",
        "    Returns:\n",
        "        (ndarray): The converted image with desired type and range.\n",
        "    \"\"\"\n",
        "    if dst_type not in (np.uint8, np.float32):\n",
        "        raise TypeError('The dst_type should be np.float32 or np.uint8, ' f'but got {dst_type}')\n",
        "    if dst_type == np.uint8:\n",
        "        img = img.round()\n",
        "    else:\n",
        "        img /= 255.\n",
        "    return img.astype(dst_type)\n",
        "\n",
        "\n",
        "def bgr2ycbcr(img, y_only=False):\n",
        "    \"\"\"Convert a BGR image to YCbCr image.\n",
        "\n",
        "    The bgr version of rgb2ycbcr.\n",
        "    It implements the ITU-R BT.601 conversion for standard-definition\n",
        "    television. See more details in\n",
        "    https://en.wikipedia.org/wiki/YCbCr#ITU-R_BT.601_conversion.\n",
        "\n",
        "    It differs from a similar function in cv2.cvtColor: `BGR <-> YCrCb`.\n",
        "    In OpenCV, it implements a JPEG conversion. See more details in\n",
        "    https://en.wikipedia.org/wiki/YCbCr#JPEG_conversion.\n",
        "\n",
        "    Args:\n",
        "        img (ndarray): The input image. It accepts:\n",
        "            1. np.uint8 type with range [0, 255];\n",
        "            2. np.float32 type with range [0, 1].\n",
        "        y_only (bool): Whether to only return Y channel. Default: False.\n",
        "\n",
        "    Returns:\n",
        "        ndarray: The converted YCbCr image. The output image has the same type\n",
        "            and range as input image.\n",
        "    \"\"\"\n",
        "    img_type = img.dtype\n",
        "    img = _convert_input_type_range(img)\n",
        "    if y_only:\n",
        "        out_img = np.dot(img, [24.966, 128.553, 65.481]) + 16.0\n",
        "    else:\n",
        "        out_img = np.matmul(\n",
        "            img, [[24.966, 112.0, -18.214], [128.553, -74.203, -93.786], [65.481, -37.797, 112.0]]) + [16, 128, 128]\n",
        "    out_img = _convert_output_type_range(out_img, img_type)\n",
        "    return out_img\n",
        "\n"
      ],
      "metadata": {
        "id": "QC2CFuWYXyLX"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#test.py"
      ],
      "metadata": {
        "id": "wpUozWs3UDd3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gc\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import os\n",
        "from datetime import datetime\n",
        "# from metrics import pt_psnr, calculate_ssim, calculate_psnr\n",
        "from pytorch_msssim import ssim\n",
        "# from utils import save_rgb\n",
        "today = datetime.today().strftime(\"%d_%m_%Y\")\n",
        "\n",
        "DEST_PATH = \"/content/drive/MyDrive/FYPData/eval_results/instructir_wm_enabled_performance.txt\"\n",
        "\n",
        "print(f\"DEST_PATH: {DEST_PATH}\")\n",
        "\n",
        "def save_performance_to_file(dest_path, text):\n",
        "    \"\"\"\n",
        "    Append the given text to the file at dest_path.\n",
        "    If the file does not exist, it is created.\n",
        "    If the folder does not exist, it is created.\n",
        "\n",
        "    Args:\n",
        "        dest_path (str): The path to the destination file.\n",
        "        text (str): The text to be appended to the file.\n",
        "    \"\"\"\n",
        "    # Extract the directory from the destination path.\n",
        "    directory = os.path.dirname(dest_path)\n",
        "    # if directory and not os.path.exists(directory):\n",
        "    os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "    print(f\"Saving performance statistics to {dest_path}\")\n",
        "    with open(dest_path, \"a\") as f:\n",
        "        f.write(text)\n",
        "        f.write(\"\\n\")\n",
        "    print(\"Saving performance statistics done!\")\n",
        "\n",
        "def return_map(deg):\n",
        "    if \"rain\" in deg:\n",
        "        return \"deraining\"\n",
        "    elif \"noise\" in deg or \"nois\" in deg:\n",
        "        return \"denoising\"\n",
        "    elif \"haze\" in deg:\n",
        "        return \"dehazing\"\n",
        "    else:\n",
        "        return \"deraining\"\n",
        "\n",
        "def get_wrong_degradation(prompt):\n",
        "    degradations = [\"noise\", \"rain\", \"haze\"]\n",
        "    degradations.remove(prompt)\n",
        "    return random.choice(degradations)\n",
        "\n",
        "def augment_prompt(prompt):\n",
        "    ### special prompts\n",
        "    lol_prompts = [\"fix the illumination\", \"increase the exposure of the photo\", \"the image is too dark to see anything, correct the photo\", \"poor illumination, improve the shot\", \"brighten dark regions\", \"make it HDR\", \"improve the light of the image\", \"Can you make the image brighter?\"]\n",
        "    sr_prompts  = [\"I need to enhance the size and quality of this image.\", \"My photo is lacking size and clarity; can you improve it?\", \"I'd appreciate it if you could upscale this photo.\", \"My picture is too little, enlarge it.\", \"upsample this image\", \"increase the resolution of this photo\", \"increase the number of pixels\", \"upsample this photo\", \"Add details to this image\", \"improve the quality of this photo\"]\n",
        "    en_prompts  = [\"make my image look like DSLR\", \"improve the colors of my image\", \"improve the contrast of this photo\", \"apply tonemapping\", \"enhance the colors of the image\", \"retouch the photo like a photograper\"]\n",
        "\n",
        "    init = np.random.choice([\"Remove the\", \"Reduce the\", \"Clean the\", \"Fix the\", \"Remove\", \"Improve the\", \"Correct the\",])\n",
        "    end  = np.random.choice([\"please\", \"fast\", \"now\", \"in the photo\", \"in the picture\", \"in the image\", \"\"])\n",
        "    newp = f\"{init} {prompt} {end}\"\n",
        "\n",
        "    if \"lol\" in prompt:\n",
        "        newp = np.random.choice(lol_prompts)\n",
        "    elif \"sr\" in prompt:\n",
        "        newp = np.random.choice(sr_prompts)\n",
        "    elif \"en\" in prompt:\n",
        "        newp = np.random.choice(en_prompts)\n",
        "\n",
        "    newp = newp.strip().replace(\"  \", \" \").replace(\"\\n\", \"\")\n",
        "    return newp\n",
        "\n",
        "def test_model(model, language_model, lm_head, testsets, device, promptify, savepath=\"results/\", initial_message=\"\"):\n",
        "\n",
        "    model.eval()\n",
        "    if language_model:\n",
        "        language_model.eval()\n",
        "        lm_head.eval()\n",
        "\n",
        "    DEG_ACC = []\n",
        "    derain_datasets = ['Rain100L', 'Rain100H', 'Test100', 'Test1200', 'Test2800']\n",
        "\n",
        "    statistics_message = initial_message\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for testset in testsets:\n",
        "\n",
        "            if savepath:\n",
        "                dt_results_path = os.path.join(savepath, testset.name)\n",
        "                if not os.path.exists(dt_results_path):\n",
        "                    os.makedirs(dt_results_path, exist_ok=True)\n",
        "\n",
        "            eval_message = f\"\\n>>> Eval on {testset.name} for {testset.degradation}(class={testset.deg_class})\\n\"\n",
        "            statistics_message += eval_message\n",
        "\n",
        "            print(eval_message)\n",
        "\n",
        "            testset_name = testset.name\n",
        "            test_dataloader = DataLoader(testset, batch_size=1, num_workers=4, drop_last=True, shuffle=False)\n",
        "            psnr_dataset = []\n",
        "            ssim_dataset = []\n",
        "            psnr_noisy   = []\n",
        "            use_y_channel= False\n",
        "\n",
        "            if testset.name in derain_datasets:\n",
        "                use_y_channel = True\n",
        "                psnr_y_dataset = []\n",
        "                ssim_y_dataset = []\n",
        "\n",
        "            statistics_message += \"The input human instructions (first 5):\\n\"\n",
        "\n",
        "            for idx, batch in enumerate(test_dataloader):\n",
        "\n",
        "                x = batch[0].to(device) # HQ image\n",
        "                y = batch[1].to(device) # LQ image\n",
        "                f = batch[2][0]         # filename\n",
        "                # print(f\"\"\n",
        "                t = [promptify(testset.degradation) for _ in range(x.shape[0])]\n",
        "                # t = t.to(device)\n",
        "                # statistics_message += \"The input human instructions (first 5):\\n\"\n",
        "                if language_model:\n",
        "                    # statistics_message += \"The input human instructions (first 5):\\n\"\n",
        "                    if idx < 5:\n",
        "                        # print the input prompt for debugging\n",
        "                        statistics_message += f\"{t}\\n\"\n",
        "                        print(\"\\nInput prompt:\", t)\n",
        "\n",
        "\n",
        "                    lm_embd = language_model(t)\n",
        "                    lm_embd = lm_embd.to(device)\n",
        "                    text_embd, deg_pred = lm_head(lm_embd)\n",
        "                    # text_embd = text_embd.to(device)\n",
        "                    x_hat = model(y, text_embd)\n",
        "\n",
        "                psnr_restore = torch.mean(pt_psnr(x, x_hat))\n",
        "                psnr_dataset.append(psnr_restore.item())\n",
        "                ssim_restore = ssim(x, x_hat, data_range=1., size_average=True)\n",
        "                ssim_dataset.append(ssim_restore.item())\n",
        "                psnr_base    = torch.mean(pt_psnr(x, y))\n",
        "                psnr_noisy.append(psnr_base.item())\n",
        "\n",
        "                if use_y_channel:\n",
        "                    _x_hat = np.clip(x_hat[0].permute(1,2,0).cpu().detach().numpy(), 0, 1).astype(np.float32)\n",
        "                    _x     = np.clip(x[0].permute(1,2,0).cpu().detach().numpy(), 0, 1).astype(np.float32)\n",
        "                    _x_hat = (_x_hat*255).astype(np.uint8)\n",
        "                    _x     = (_x*255).astype(np.uint8)\n",
        "\n",
        "                    psnr_y = calculate_psnr(_x, _x_hat, crop_border=0, input_order='HWC', test_y_channel=True)\n",
        "                    ssim_y = calculate_ssim(_x, _x_hat, crop_border=0, input_order='HWC', test_y_channel=True)\n",
        "                    psnr_y_dataset.append(psnr_y)\n",
        "                    ssim_y_dataset.append(ssim_y)\n",
        "\n",
        "                ## SAVE RESULTS\n",
        "                if savepath:\n",
        "                    restored_img = np.clip(x_hat[0].permute(1,2,0).cpu().detach().numpy(), 0, 1).astype(np.float32)\n",
        "                    img_name = f.split(\"/\")[-1]\n",
        "                    save_rgb(restored_img, os.path.join(dt_results_path, img_name))\n",
        "\n",
        "            if len(psnr_dataset) > 0:\n",
        "                result_str = f\"{testset_name}_base {np.mean(psnr_noisy)} Total images: {len(psnr_dataset)}\\n{testset_name}_psnr {np.mean(psnr_dataset)}\\n{testset_name}_ssim {np.mean(ssim_dataset)}\\n\"\n",
        "                print(result_str)\n",
        "                y_channel_result_str = \"\"\n",
        "                if use_y_channel:\n",
        "                    y_channel_result_str =f\"{testset_name}_psnr-Y {np.mean(psnr_y_dataset)} {len(psnr_y_dataset)}\\n{testset_name}_ssim-Y {np.mean(ssim_y_dataset)}\\n\"\n",
        "                    print(y_channel_result_str)\n",
        "\n",
        "                statistics_message += result_str\n",
        "                statistics_message += y_channel_result_str\n",
        "                divide_line = 25 * \"***\"\n",
        "                statistics_message += divide_line\n",
        "                statistics_message += \"\\n\\n\"\n",
        "                print(); print(divide_line)\n",
        "\n",
        "                del test_dataloader,psnr_dataset, psnr_noisy; gc.collect()\n",
        "    save_performance_to_file(DEST_PATH, statistics_message)\n",
        "\n",
        "        # END OF FUNCTION"
      ],
      "metadata": {
        "id": "4oc_zsI-T-7X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffe410f8-5529-4373-ca53-624affb0f685"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEST_PATH: /content/drive/MyDrive/FYPData/eval_results/instructir_wm_enabled_performance.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#datasets.py\n"
      ],
      "metadata": {
        "id": "fskbArRIULks"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchvision\n",
        "import torchvision.transforms.functional as TF\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "from glob import glob\n",
        "from PIL import Image\n",
        "\n",
        "# from utils import load_img, modcrop\n",
        "from torchvision import transforms\n",
        "\n",
        "DEG_MAP = {\n",
        "    \"noise\": 0,\n",
        "    \"blur\" : 1,\n",
        "    \"rain\" : 2,\n",
        "    \"haze\" : 3,\n",
        "    \"lol\"  : 4,\n",
        "    \"sr\"   : 5,\n",
        "    \"en\"   : 6,\n",
        "}\n",
        "\n",
        "DEG2TASK = {\n",
        "    \"noise\": \"denoising\",\n",
        "    \"blur\" : \"deblurring\",\n",
        "    \"rain\" : \"deraining\",\n",
        "    \"haze\" : \"dehazing\",\n",
        "    \"lol\"  : \"lol\",\n",
        "    \"sr\"   : \"sr\",\n",
        "    \"en\"   : \"enhancement\"\n",
        "}\n",
        "\n",
        "def augment_prompt(prompt):\n",
        "    ### special prompts\n",
        "    lol_prompts = [\"fix the illumination\", \"increase the exposure of the photo\", \"the image is too dark to see anything, correct the photo\", \"poor illumination, improve the shot\", \"brighten dark regions\", \"make it HDR\", \"improve the light of the image\", \"Can you make the image brighter?\"]\n",
        "    sr_prompts  = [\"I need to enhance the size and quality of this image.\", \"My photo is lacking size and clarity; can you improve it?\", \"I'd appreciate it if you could upscale this photo.\", \"My picture is too little, enlarge it.\", \"upsample this image\", \"increase the resolution of this photo\", \"increase the number of pixels\", \"upsample this photo\", \"Add details to this image\", \"improve the quality of this photo\"]\n",
        "    en_prompts  = [\"make my image look like DSLR\", \"improve the colors of my image\", \"improve the contrast of this photo\", \"apply tonemapping\", \"enhance the colors of the image\", \"retouch the photo like a photograper\"]\n",
        "\n",
        "    init = np.random.choice([\"Remove the\", \"Reduce the\", \"Clean the\", \"Fix the\", \"Remove\", \"Improve the\", \"Correct the\",])\n",
        "    end  = np.random.choice([\"please\", \"fast\", \"now\", \"in the photo\", \"in the picture\", \"in the image\", \"\"])\n",
        "    newp = f\"{init} {prompt} {end}\"\n",
        "\n",
        "    if \"lol\" in prompt:\n",
        "        newp = np.random.choice(lol_prompts)\n",
        "    elif \"sr\" in prompt:\n",
        "        newp = np.random.choice(sr_prompts)\n",
        "    elif \"en\" in prompt:\n",
        "        newp = np.random.choice(en_prompts)\n",
        "\n",
        "    newp = newp.strip().replace(\"  \", \" \").replace(\"\\n\", \"\")\n",
        "    return newp\n",
        "\n",
        "def get_deg_name(path):\n",
        "    \"\"\"\n",
        "    Get the degradation name from the path\n",
        "    \"\"\"\n",
        "\n",
        "    if (\"gopro\" in path) or (\"GoPro\" in path) or (\"blur\" in path) or (\"Blur\" in path) or (\"RealBlur\" in path):\n",
        "        return \"blur\"\n",
        "    elif (\"SOTS\" in path) or (\"haze\" in path) or (\"sots\" in path) or (\"RESIDE\" in path):\n",
        "        return \"haze\"\n",
        "    elif (\"LOL\" in path):\n",
        "        return \"lol\"\n",
        "    elif (\"fiveK\" in path):\n",
        "        return \"en\"\n",
        "    elif (\"super\" in path) or (\"classicalSR\" in path):\n",
        "        return \"sr\"\n",
        "    elif (\"Rain100\" in path) or (\"rain13k\" in path) or (\"Rain13k\" in path):\n",
        "        return \"rain\"\n",
        "    else:\n",
        "        return \"noise\"\n",
        "\n",
        "def crop_img(image, base=16):\n",
        "    \"\"\"\n",
        "    Mod crop the image to ensure the dimension is divisible by base. Also done by SwinIR, Restormer and others.\n",
        "    \"\"\"\n",
        "    h = image.shape[0]\n",
        "    w = image.shape[1]\n",
        "    crop_h = h % base\n",
        "    crop_w = w % base\n",
        "    return image[crop_h // 2:h - crop_h + crop_h // 2, crop_w // 2:w - crop_w + crop_w // 2, :]\n",
        "\n",
        "\n",
        "################# DATASETS\n",
        "\n",
        "\n",
        "class RefDegImage(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset for Image Restoration having low-quality image and the reference image.\n",
        "    Tasks: synthetic denoising, deblurring, super-res, etc.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hq_img_paths, lq_img_paths, augmentations=None, val=False, name=\"test\", deg_name=\"noise\", deg_class=0):\n",
        "\n",
        "        assert len(hq_img_paths) == len(lq_img_paths)\n",
        "\n",
        "        self.hq_paths  = hq_img_paths\n",
        "        self.lq_paths  = lq_img_paths\n",
        "        self.totensor  = torchvision.transforms.ToTensor()\n",
        "        self.val       = val\n",
        "        self.augs      = augmentations\n",
        "        self.name      = name\n",
        "        self.degradation = deg_name\n",
        "        self.deg_class = deg_class\n",
        "\n",
        "\n",
        "        # New code start\n",
        "        # # Default resize and ToTensor transformations\n",
        "        self.resize_and_totensor = transforms.Compose([\n",
        "            transforms.Resize((320, 480)),  # Resize to fixed dimensions\n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        "        # new code end\n",
        "        if self.val:\n",
        "            self.augs = None # No augmentations during validation/test\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.hq_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        hq_path = self.hq_paths[idx]\n",
        "        lq_path = self.lq_paths[idx]\n",
        "\n",
        "        hq_image = load_img(hq_path)\n",
        "        lq_image = load_img(lq_path)\n",
        "\n",
        "        if self.val:\n",
        "            # if an image has an odd number dimension we trim for example from [321, 189] to [320, 188].\n",
        "            hq_image = crop_img(hq_image)\n",
        "            lq_image = crop_img(lq_image)\n",
        "\n",
        "        hq_image = self.totensor(hq_image.astype(np.float32))\n",
        "        lq_image = self.totensor(lq_image.astype(np.float32))\n",
        "\n",
        "        return hq_image, lq_image, hq_path\n",
        "\n",
        "\n",
        "\n",
        "def create_testsets(testsets, debug=False):\n",
        "    \"\"\"\n",
        "    Given a list of testsets create pytorch datasets for each.\n",
        "    The method requires the paths to references and noisy images.\n",
        "    \"\"\"\n",
        "    assert len(testsets) > 0\n",
        "\n",
        "    if debug:\n",
        "        print (20*'****')\n",
        "        print (\"Creating Testsets\", len(testsets))\n",
        "\n",
        "    datasets = []\n",
        "    for testdt in testsets:\n",
        "\n",
        "        path_hq , path_lq = testdt[0], testdt[1]\n",
        "        if debug: print (path_hq , path_lq)\n",
        "\n",
        "        if (\"denoising\" in path_hq) or (\"jpeg\" in path_hq):\n",
        "            dataset_name  = path_hq.split(\"/\")[-1]\n",
        "            dataset_sigma = path_lq.split(\"/\")[-1].split(\"_\")[-1].split(\".\")[0]\n",
        "            dataset_name  = dataset_name+ f\"_{dataset_sigma}\"\n",
        "        elif \"Rain\" in path_hq:\n",
        "            if \"Rain100L\" in path_hq:\n",
        "                dataset_name  = \"Rain100L\"\n",
        "            else:\n",
        "                dataset_name  = path_hq.split(\"/\")[3]\n",
        "\n",
        "        elif (\"gopro\" in path_hq) or (\"GoPro\" in path_hq):\n",
        "            dataset_name  = \"GoPro\"\n",
        "        elif \"LOL\" in path_hq:\n",
        "            dataset_name  = \"LOL\"\n",
        "        elif \"SOTS\" in path_hq:\n",
        "            dataset_name  = \"SOTS\"\n",
        "        elif \"fiveK\" in path_hq:\n",
        "            dataset_name  = \"MIT5K\"\n",
        "        else:\n",
        "            assert False, f\"{path_hq} - unknown dataset\"\n",
        "\n",
        "        hq_img_paths = sorted(glob(os.path.join(path_hq, \"*\")))\n",
        "        lq_img_paths = sorted(glob(os.path.join(path_lq, \"*\")))\n",
        "\n",
        "        if \"SOTS\" in path_hq:\n",
        "            # Haze removal SOTS test dataset\n",
        "            dataset_name  = \"SOTS\"\n",
        "            hq_img_paths = sorted(glob(os.path.join(path_hq, \"*.jpg\")))\n",
        "            assert len(hq_img_paths) == 500\n",
        "\n",
        "            lq_img_paths = [file.replace(\"GT\", \"IN\") for file in hq_img_paths]\n",
        "\n",
        "        if \"fiveK\" in path_hq:\n",
        "            dataset_name  = \"MIT5K\"\n",
        "            testf = \"test-data/mit5k/test.txt\"\n",
        "            f = open(testf, \"r\")\n",
        "            test_ids = f.readlines()\n",
        "            test_ids = [x.strip() for x in test_ids]\n",
        "            f.close()\n",
        "            hq_img_paths = [os.path.join(path_hq, f\"{x}.jpg\") for x in test_ids]\n",
        "            lq_img_paths = [x.replace(\"expertC\", \"input\") for x in hq_img_paths]\n",
        "            assert len(hq_img_paths) == 498\n",
        "\n",
        "        if \"gopro\" in path_hq:\n",
        "            assert len(hq_img_paths) == 1111\n",
        "\n",
        "        if \"LOL\" in path_hq:\n",
        "            assert len(hq_img_paths) == 15\n",
        "\n",
        "        assert len(hq_img_paths) == len(lq_img_paths)\n",
        "\n",
        "        deg_name  = get_deg_name(path_hq)\n",
        "        deg_class = DEG_MAP[deg_name]\n",
        "\n",
        "        valdts = RefDegImage(hq_img_paths = hq_img_paths,\n",
        "                            lq_img_paths  = lq_img_paths,\n",
        "                            val = True, name= dataset_name, deg_name=deg_name, deg_class=deg_class)\n",
        "\n",
        "        datasets.append(valdts)\n",
        "\n",
        "    assert len(datasets) == len(testsets)\n",
        "    print (20*'****')\n",
        "\n",
        "    return datasets"
      ],
      "metadata": {
        "id": "F8GHkONYT_YN"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rXod9IPYT_eR"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3b4QaueERgfX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#options.py"
      ],
      "metadata": {
        "id": "-N2Ct2SNvf3x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New section"
      ],
      "metadata": {
        "id": "bAlxgy77UTOZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "\n",
        "parser.add_argument('--patch_size', type=int, default=256, help='patchsize of input.')\n",
        "parser.add_argument('--batch_size', type=int, default=32, help='batch size of input.')\n",
        "\n",
        "parser.add_argument('--num_workers', type=int, default=8, help='number of workers.')\n",
        "parser.add_argument(\"--checkpoint_dir\",type=str, default=\"/content/drive/MyDrive/FYPData/train_ckpt\",help = \"Name of the Directory where the checkpoint is to be saved\")\n",
        "parser.add_argument('--lm',      type=str, default=\"/content/drive/MyDrive/FYPData/models/lm_instructir-7d.pt\", help='Path to the language model weights')\n",
        "parser.add_argument('--config',  type=str, default='configs/eval5d.yml', help='Path to config file')\n",
        "\n",
        "parser.add_argument('--promptify', type=str, default=\"simple_augment\")\n",
        "parser.add_argument('--debug',   action='store_true', help=\"Debug mode\")\n",
        "parser.add_argument('--save',    type=str, default='/content/drive/MyDrive/FYPData/performance_results/', help=\"Path to save the resultant images\")\n",
        "args = parser.parse_args([])"
      ],
      "metadata": {
        "id": "DVepEgIrvf-V"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#eval_instructir.py"
      ],
      "metadata": {
        "id": "D_NXZsIHRgkH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchvision\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "import json\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import yaml\n",
        "import random\n",
        "import gc\n",
        "from datetime import datetime\n",
        "\n",
        "# from utils import *\n",
        "# from models import instructir\n",
        "\n",
        "# from text.models import LanguageModel, LMHead\n",
        "\n",
        "config = {\n",
        "    \"llm\": {\n",
        "        \"model\": \"TaylorAI/bge-micro-v2\",  # See Paper Sec. 3.2 and Appendix\n",
        "        \"model_dim\": 384,\n",
        "        \"embd_dim\": 256,\n",
        "        \"nclasses\": 7,  # noise, blur, rain, haze, lol, enhancement, upsampling (Paper Sec. 4.3)\n",
        "        \"weights\": False\n",
        "    },\n",
        "    \"model\": {\n",
        "        \"arch\": \"instructir\",\n",
        "        \"use_text\": True,\n",
        "        \"in_ch\": 3,\n",
        "        \"out_ch\": 3,\n",
        "        \"width\": 32,\n",
        "        \"enc_blks\": [2, 2, 4, 8],\n",
        "        \"middle_blk_num\": 4,\n",
        "        \"dec_blks\": [2, 2, 2, 2],\n",
        "        \"textdim\": 256,\n",
        "        \"weights\": False\n",
        "    },\n",
        "    \"test\": {\n",
        "        \"batch_size\": 1,\n",
        "        \"num_workers\": 8,\n",
        "        \"dn_datapath\": \"/content/drive/MyDrive/FYPData/test-data/denoising_testsets/\",\n",
        "        \"dn_datasets\": [\"CBSD68\", \"Kodak24\"],\n",
        "        \"dn_sigmas\": [15, 25, 50],\n",
        "        \"rain_targets\": [\"/content/drive/MyDrive/FYPData/test-data/Rain100L/original/\"],\n",
        "        \"rain_inputs\": [\"/content/drive/MyDrive/FYPData/test-data/Rain100L/rainy/\"],\n",
        "        \"haze_targets\": \"/content/drive/MyDrive/FYPData/test-data/SOTS/GT/\",\n",
        "        \"haze_inputs\": \"/content/drive/MyDrive/FYPData/test-data/SOTS/IN/\",\n",
        "        \"lol_targets\": \"/content/drive/MyDrive/FYPData/test-data/LOL/high/\",\n",
        "        \"lol_inputs\": \"/content/drive/MyDrive/FYPData/test-data/LOL/low/\",\n",
        "        \"gopro_targets\": \"/content/drive/MyDrive/FYPData/test-data/GoPro/target/\",\n",
        "        \"gopro_inputs\": \"/content/drive/MyDrive/FYPData/test-data/GoPro/input/\"\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "def seed_everything(SEED=42):\n",
        "    random.seed(SEED)\n",
        "    np.random.seed(SEED)\n",
        "    torch.manual_seed(SEED)\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "\n",
        "    SEED=42\n",
        "    seed_everything(SEED=SEED)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "    now = datetime.now()\n",
        "\n",
        "    # testing_message = f\"{now.strftime('%Y-%m-%d %H:%M:%S')} Testing\"\n",
        "\n",
        "    # GPU        = args.device\n",
        "    DEBUG      = args.debug\n",
        "\n",
        "    IMAGE_MODEL_NAME = \"/content/drive/MyDrive/FYPData/models/im_instructir-7d.pt\"\n",
        "    # IMAGE_MODEL_NAME = \"/content/drive/MyDrive/FYPData/models/instructir_with_weight_modulation_32chan.pt\"\n",
        "    # instructIR_with_weight_modulation_20epochs_4mid.pth\n",
        "    CONFIG     = args.config\n",
        "    LM_HEAD_MODEL   = args.lm\n",
        "    SAVE_PATH  = args.save\n",
        "\n",
        "\n",
        "    testing_message = f\"{now.strftime('%Y-%m-%d %H:%M:%S')} Testing Original InstructIR Correct Human Instructions -Ablation study on training human instructions as input\\n\"\n",
        "\n",
        "    print ('CUDA GPU available: ', torch.cuda.is_available())\n",
        "\n",
        "    # torch.cuda.set_device(f'cuda:{GPU}')\n",
        "    device = torch.device(f'cuda' if torch.cuda.is_available() else \"cpu\")\n",
        "    print('CUDA visible devices: ' + str(torch.cuda.device_count()))\n",
        "    if torch.cuda.is_available():\n",
        "        print('CUDA current device: ', torch.cuda.current_device(), torch.cuda.get_device_name(torch.cuda.current_device()))\n",
        "\n",
        "    # parse config file\n",
        "    # with open(os.path.join(CONFIG), \"r\") as f:\n",
        "    #     config = yaml.safe_load(f)\n",
        "\n",
        "    cfg = dict2namespace(config)\n",
        "\n",
        "    use_offset = False\n",
        "\n",
        "    testing_message += f\"---- Weight Modification Enabled: {use_offset}\\n\"\n",
        "\n",
        "    print(f\"---- Weight Modification Enabled: {use_offset}\\n\")\n",
        "\n",
        "    infor = f\"Image Model Name: {IMAGE_MODEL_NAME}, Projection Head: {LM_HEAD_MODEL}\\nDevice: {device}, Human Instruction Source: {args.promptify}\\nConfig: {CONFIG}\\n\\n\"\n",
        "\n",
        "    testing_message += infor\n",
        "\n",
        "    print(20*\"****\")\n",
        "    print(\"EVALUATION\")\n",
        "    print(infor)\n",
        "    print(20*\"****\")\n",
        "\n",
        "    ################### TESTING DATASET\n",
        "\n",
        "    TESTSETS = []\n",
        "    denoise_testsets   = []\n",
        "    rain_testsets = []\n",
        "    haze_testsets = []\n",
        "    # Denoising\n",
        "    printed_noise_path = True\n",
        "    try:\n",
        "        for testset in cfg.test.dn_datasets:\n",
        "            for sigma in cfg.test.dn_sigmas:\n",
        "                noisy_testpath = os.path.join(cfg.test.dn_datapath, testset+ f\"_{sigma}\")\n",
        "                clean_testpath = os.path.join(cfg.test.dn_datapath, testset)\n",
        "                if printed_noise_path:\n",
        "                    print(f\"clean_testpath:{clean_testpath}, noisy_testpath:{noisy_testpath} \")\n",
        "                    printed_noise_path = False\n",
        "                denoise_testsets.append([clean_testpath, noisy_testpath])\n",
        "    except:\n",
        "        denoise_testsets = []\n",
        "\n",
        "    printed_rain_path = True\n",
        "    # RAIN\n",
        "    try:\n",
        "        for noisy_testpath, clean_testpath in zip(cfg.test.rain_inputs, cfg.test.rain_targets):\n",
        "            if printed_rain_path:\n",
        "                print(f\"clean_testpath:{clean_testpath}, noisy_testpath:{noisy_testpath} \")\n",
        "                printed_rain_path = False\n",
        "            rain_testsets.append([clean_testpath, noisy_testpath])\n",
        "    except:\n",
        "        rain_testsets = []\n",
        "\n",
        "    # HAZE\n",
        "    try:\n",
        "        haze_testsets = [[cfg.test.haze_targets, cfg.test.haze_inputs]]\n",
        "    except:\n",
        "        haze_testsets = []\n",
        "\n",
        "    # # BLUR\n",
        "    # try:\n",
        "    #     blur_testsets = [[cfg.test.gopro_targets, cfg.test.gopro_inputs]]\n",
        "    # except:\n",
        "    #     blur_testsets = []\n",
        "\n",
        "    # # LOL\n",
        "    # try:\n",
        "    #     lol_testsets = [[cfg.test.lol_targets, cfg.test.lol_inputs]]\n",
        "    # except:\n",
        "    #     lol_testsets = []\n",
        "\n",
        "    # # MIT5K\n",
        "    # try:\n",
        "    #     mit_testsets = [[cfg.test.mit_targets, cfg.test.mit_inputs]]\n",
        "    # except:\n",
        "    #     mit_testsets = []\n",
        "\n",
        "    TESTSETS += denoise_testsets\n",
        "    TESTSETS += rain_testsets\n",
        "    TESTSETS += haze_testsets\n",
        "    # TESTSETS += blur_testsets\n",
        "    # TESTSETS += lol_testsets\n",
        "    # TESTSETS += mit_testsets\n",
        "\n",
        "    # print (\"Tests:\", TESTSETS)\n",
        "\n",
        "    if len(denoise_testsets) > 0:\n",
        "        testing_message += f\"Denoise testset length: {len(denoise_testsets)}\\n\"\n",
        "\n",
        "    if len(rain_testsets) > 0:\n",
        "        testing_message += f\"Derain testset length: {len(rain_testsets)}\\n\"\n",
        "\n",
        "    if len(haze_testsets) > 0:\n",
        "        testing_message += f\"Dehaze testset length: {len(haze_testsets)}\\n\"\n",
        "\n",
        "    testset_len = f\"Total testsets length: {len(TESTSETS)}\\n\"\n",
        "\n",
        "    testing_message += testset_len\n",
        "\n",
        "    print (testset_len)\n",
        "    print (20 * \"----\")\n",
        "\n",
        "\n",
        "    ################### RESTORATION MODEL\n",
        "\n",
        "    print (\"Creating InstructIR\")\n",
        "    model = create_model(input_channels =cfg.model.in_ch, width=cfg.model.width, enc_blks = cfg.model.enc_blks,\n",
        "                    middle_blk_num = cfg.model.middle_blk_num, dec_blks = cfg.model.dec_blks,\n",
        "                    txtdim=cfg.model.textdim, include_offset=use_offset)\n",
        "\n",
        "    ################### LOAD IMAGE MODEL\n",
        "\n",
        "    assert IMAGE_MODEL_NAME, \"Model weights required for evaluation\"\n",
        "\n",
        "    print (\"IMAGE MODEL CKPT:\", IMAGE_MODEL_NAME)\n",
        "    model.load_state_dict(torch.load(IMAGE_MODEL_NAME, map_location=device), strict=False)\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = True\n",
        "    model = model.to(device)\n",
        "    total_params = count_params(model)\n",
        "\n",
        "    # Freeze prompt generator blocks\n",
        "    for prompt_block in model.promptBlocks:\n",
        "        for param in prompt_block.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    # Freeze offset generators\n",
        "    # (Assuming self.offset_generators is a list of OffsetGenerator modules)\n",
        "    for offset_gen in model.offset_generators:\n",
        "        for param in offset_gen.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "\n",
        "    for param in model.prompt_block_middle_blks.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    for param in model.middleblock_offsetGen.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    base_ir_model_params = count_params(model)\n",
        "\n",
        "    for param in model.parameters():\n",
        "      param.requires_grad = False\n",
        "\n",
        "    # enable prompt generator blocks\n",
        "    for prompt_block in model.promptBlocks:\n",
        "        for param in prompt_block.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "    # enable offset generators\n",
        "    for offset_gen in model.offset_generators:\n",
        "        for param in offset_gen.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "    for param in model.prompt_block_middle_blks.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "    for param in model.middleblock_offsetGen.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "\n",
        "    params_of_modification = count_params(model)\n",
        "\n",
        "    for param in model.parameters():\n",
        "      param.requires_grad = True\n",
        "\n",
        "\n",
        "    model_params_message = f\"\"\"\\nTotal params: {total_params/1e6}M,\n",
        "    base image restoration model params: {base_ir_model_params/1e6}M\\n\n",
        "    OffsetGenerator & PromptGenBlock params: {params_of_modification/1e6}M\\n\"\"\"\n",
        "\n",
        "    testing_message += model_params_message\n",
        "\n",
        "    # nparams = count_params(model)\n",
        "    print(model_params_message)\n",
        "    ################### LANGUAGE MODEL\n",
        "\n",
        "    # try:\n",
        "    #     PROMPT_DB  = cfg.llm.text_db\n",
        "    # except:\n",
        "    #     PROMPT_DB  = None\n",
        "\n",
        "    if cfg.model.use_text:\n",
        "        os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "        # Initialize the LanguageModel class\n",
        "        LMODEL = cfg.llm.model\n",
        "        language_model = LanguageModel(model=LMODEL).to(device)\n",
        "        lm_head = LMHead(embedding_dim=cfg.llm.model_dim, hidden_dim=cfg.llm.embd_dim, num_classes=cfg.llm.nclasses)\n",
        "        lm_head = lm_head.to(device)\n",
        "        # language_model = language_model.to(device)\n",
        "        lm_nparams   = count_params(lm_head)\n",
        "\n",
        "        print(\"LMHEAD MODEL CKPT:\", LM_HEAD_MODEL)\n",
        "        lm_head.load_state_dict(torch.load(LM_HEAD_MODEL, map_location=device), strict=True)\n",
        "        print(\"Loaded weights!\")\n",
        "\n",
        "    else:\n",
        "        LMODEL = None\n",
        "        language_model = None\n",
        "        lm_head = None\n",
        "        lm_nparams = 0\n",
        "\n",
        "    print (20 * \"----\")\n",
        "\n",
        "    ################### TESTING !!\n",
        "\n",
        "    # from datasets import RefDegImage, augment_prompt, create_testsets\n",
        "\n",
        "    if args.promptify == \"simple_augment\":\n",
        "        promptify = augment_prompt\n",
        "    elif args.promptify == \"chatgpt\":\n",
        "        instruction_file_path = \"/content/drive/MyDrive/FYPData/human_instructions.json\"\n",
        "        with open(instruction_file_path, \"r\") as f:\n",
        "            prompts = json.load(f)\n",
        "        for deg in prompts.keys():\n",
        "            random.shuffle(prompts[deg])\n",
        "\n",
        "        def promptify(deg):\n",
        "            return random.choice(prompts[deg])\n",
        "\n",
        "        print(\"--- Using ChatGPT generated human instructions\\n\")\n",
        "        testing_message += \"--- Using ChatGPT generated human instructions\\n\"\n",
        "    else:\n",
        "        def promptify(deg):\n",
        "            return args.promptify\n",
        "\n",
        "\n",
        "    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = \"expandable_segments:True\"\n",
        "\n",
        "    test_datasets = create_testsets(TESTSETS, debug=True)\n",
        "\n",
        "    test_model(model, language_model, lm_head, test_datasets, device, promptify, savepath=None, initial_message=testing_message)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 873
        },
        "id": "rYSxWOs7R9-_",
        "outputId": "42787410-0e09-45d1-e36e-83e7002abd4a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA GPU available:  True\n",
            "CUDA visible devices: 1\n",
            "CUDA current device:  0 NVIDIA A100-SXM4-40GB\n",
            "---- Weight Modification Enabled: False\n",
            "\n",
            "********************************************************************************\n",
            "EVALUATION\n",
            "Image Model Name: /content/drive/MyDrive/FYPData/models/im_instructir-7d.pt, Projection Head: /content/drive/MyDrive/FYPData/models/lm_instructir-7d.pt\n",
            "Device: cuda, Human Instruction Source: simple_augment\n",
            "Config: configs/eval5d.yml\n",
            "\n",
            "\n",
            "********************************************************************************\n",
            "clean_testpath:/content/drive/MyDrive/FYPData/test-data/denoising_testsets/CBSD68, noisy_testpath:/content/drive/MyDrive/FYPData/test-data/denoising_testsets/CBSD68_15 \n",
            "clean_testpath:/content/drive/MyDrive/FYPData/test-data/Rain100L/original/, noisy_testpath:/content/drive/MyDrive/FYPData/test-data/Rain100L/rainy/ \n",
            "Total testsets length: 8\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Creating InstructIR\n",
            "IMAGE MODEL CKPT: /content/drive/MyDrive/FYPData/models/im_instructir-7d.pt\n",
            "\n",
            "Total params: 37.823471M,\n",
            "    base image restoration model params: 15.843363M\n",
            "\n",
            "    OffsetGenerator & PromptGenBlock params: 21.980108M\n",
            "\n",
            "LMHEAD MODEL CKPT: /content/drive/MyDrive/FYPData/models/lm_instructir-7d.pt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-0522ca12ae79>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LMHEAD MODEL CKPT:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLM_HEAD_MODEL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m         \u001b[0mlm_head\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLM_HEAD_MODEL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loaded weights!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1430\u001b[0m             \u001b[0morig_position\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m             \u001b[0moverall_storage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1432\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1433\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0m_is_torchscript_zip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1434\u001b[0m                     warnings.warn(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name_or_buffer)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_zipfile_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPyTorchFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UcInCUGxJBSY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FT3W12jjloRf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}