{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ts74yIzxyW1P"
      },
      "source": [
        "# New section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3wCsX-qtnL6"
      },
      "source": [
        "# Load the utils files:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drlNPVmwuxY9",
        "outputId": "b2c132f9-928a-4054-b466-dc7fe5ad7a86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# !cp -r /content/drive/MyDrive/FYPData/ /content/\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "\n",
        "# !cp /content/drive/MyDrive/FYPData/Train.zip /content/\n",
        "# !unzip /content/Train.zip -d /content/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3T7Zggbux3T"
      },
      "source": [
        "#NoiseDegradation.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "tWd00-oru1IJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision.transforms import ToPILImage, Compose, RandomCrop, ToTensor\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "class NoiseDegradation(object):\n",
        "    def __init__(self, args):\n",
        "        super(NoiseDegradation, self).__init__()\n",
        "        self.args = args\n",
        "        self.toTensor = ToTensor()\n",
        "        self.crop_transform = Compose([\n",
        "            ToPILImage(),\n",
        "            RandomCrop(args.patch_size),\n",
        "        ])\n",
        "    def _add_gaussian_noise(self, clean_patch, sigma):\n",
        "        noise = np.random.randn(*clean_patch.shape)\n",
        "        noisy_patch = np.clip(clean_patch + noise * sigma, 0, 255).astype(np.uint8)\n",
        "\n",
        "        return noisy_patch, clean_patch\n",
        "\n",
        "    def _add_noise_degradation_by_level(self, clean_patch, degrade_type):\n",
        "        degraded_patch = None\n",
        "        if degrade_type == 0:\n",
        "            # noise level (sigma) =15\n",
        "            degraded_patch, clean_patch = self._add_gaussian_noise(clean_patch, sigma=15)\n",
        "        elif degrade_type == 7:\n",
        "            # noise level (sigma) =25\n",
        "            degraded_patch, clean_patch = self._add_gaussian_noise(clean_patch, sigma=25)\n",
        "        elif degrade_type == 8:\n",
        "            # noise level (sigma) =50\n",
        "            degraded_patch, clean_patch = self._add_gaussian_noise(clean_patch, sigma=50)\n",
        "\n",
        "         # If degraded_patch is still None (meaning degrade_type wasn't 0, 7, or 8)\n",
        "        # handle the case (raise an error, return the original, etc.)\n",
        "        if degraded_patch is None:\n",
        "            degraded_patch = clean_patch # or raise ValueError(f\"Invalid degrade_type: {degrade_type}\")\n",
        "\n",
        "        return degraded_patch, clean_patch\n",
        "\n",
        "    def add_noise_degradation(self,clean_patch,degrade_type = None):\n",
        "        if degrade_type == 0 or degrade_type == 7 or degrade_type == 8:\n",
        "            degrade_type= degrade_type\n",
        "        else:\n",
        "            degrade_type = random.choices([0,7,8])\n",
        "\n",
        "\n",
        "        degraded_patch, _ = self._add_noise_degradation_by_level(clean_patch, degrade_type)\n",
        "        return degraded_patch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qseY2cW3uBNg"
      },
      "source": [
        "#utils.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XyDUDVtvubBC"
      },
      "outputs": [],
      "source": [
        "import imageio\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import argparse\n",
        "import random\n",
        "import torch\n",
        "\n",
        "\n",
        "def seed_everything(SEED=42):\n",
        "    random.seed(SEED)\n",
        "    np.random.seed(SEED)\n",
        "    torch.manual_seed(SEED)\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "# def saveImage(filename, image):\n",
        "#     imageTMP = np.clip(image * 255.0, 0, 255).astype('uint8')\n",
        "#     imageio.imwrite(filename, imageTMP)\n",
        "\n",
        "def save_rgb (img, filename):\n",
        "\n",
        "    img = np.clip(img, 0., 1.)\n",
        "    if np.max(img) <= 1:\n",
        "        img = img * 255\n",
        "\n",
        "    img = img.astype(np.float32)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
        "    cv2.imwrite(filename, img)\n",
        "\n",
        "\n",
        "def load_img (filename, norm=True,):\n",
        "\n",
        "    # original code: img = np.array(Image.open(filename).convert(\"RGB\"))\n",
        "    # changed to:\n",
        "    img = np.array(Image.open(filename).convert(\"RGB\").resize((320,480)))\n",
        "\n",
        "    if norm:\n",
        "        img = img / 255.\n",
        "        img = img.astype(np.float32)\n",
        "    return img\n",
        "\n",
        "def plot_all (images, figsize=(20,10), axis='off', names=None):\n",
        "    nplots = len(images)\n",
        "    fig, axs = plt.subplots(1,nplots, figsize=figsize, dpi=80,constrained_layout=True)\n",
        "    for i in range(nplots):\n",
        "        axs[i].imshow(images[i])\n",
        "        if names: axs[i].set_title(names[i])\n",
        "        axs[i].axis(axis)\n",
        "    plt.show()\n",
        "\n",
        "def modcrop(img_in, scale=2):\n",
        "    # img_in: Numpy, HWC or HW\n",
        "    img = np.copy(img_in)\n",
        "    if img.ndim == 2:\n",
        "        H, W = img.shape\n",
        "        H_r, W_r = H % scale, W % scale\n",
        "        img = img[:H - H_r, :W - W_r]\n",
        "    elif img.ndim == 3:\n",
        "        H, W, C = img.shape\n",
        "        H_r, W_r = H % scale, W % scale\n",
        "        img = img[:H - H_r, :W - W_r, :]\n",
        "    else:\n",
        "        raise ValueError('Wrong img ndim: [{:d}].'.format(img.ndim))\n",
        "    return img\n",
        "\n",
        "def dict2namespace(config):\n",
        "    namespace = argparse.Namespace()\n",
        "    for key, value in config.items():\n",
        "        if isinstance(value, dict):\n",
        "            new_value = dict2namespace(value)\n",
        "        else:\n",
        "            new_value = value\n",
        "        setattr(namespace, key, new_value)\n",
        "    return namespace\n",
        "\n",
        "\n",
        "########## MODEL\n",
        "\n",
        "def count_params(model):\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    return trainable_params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEXc1LhItuSf"
      },
      "source": [
        "#TrainDataset.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "3OB24D-Btq_c"
      },
      "outputs": [],
      "source": [
        "\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "import torchvision.transforms.functional as TF\n",
        "from torchvision.transforms import ToPILImage, Compose, RandomCrop, ToTensor\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "\n",
        "# from utils import load_img, modcrop\n",
        "# from torchvision import transforms\n",
        "# from training_utils.NoiseDegradation import NoiseDegradation\n",
        "\n",
        "\n",
        "\n",
        "DEG_MAP = {\n",
        "    \"noise_15\" : 0,\n",
        "    \"blur\"     : 1,\n",
        "    \"rain\"     : 2,\n",
        "    \"haze\"     : 3,\n",
        "    \"lol\"      : 4,\n",
        "    \"sr\"       : 5,\n",
        "    \"en\"       : 6,\n",
        "    \"noise_25\" : 7,\n",
        "    \"noise_50\" : 8\n",
        "}\n",
        "\n",
        "DEG2TASK = {\n",
        "    \"noise\": \"denoising\",\n",
        "    \"blur\" : \"deblurring\",\n",
        "    \"rain\" : \"deraining\",\n",
        "    \"haze\" : \"dehazing\",\n",
        "    \"lol\"  : \"lol\",\n",
        "    \"sr\"   : \"sr\",\n",
        "    \"en\"   : \"enhancement\"\n",
        "}\n",
        "\n",
        "def crop_img(image, base=16):\n",
        "    \"\"\"\n",
        "    Mod crop the image to ensure the dimension is divisible by base. Also done by SwinIR, Restormer and others.\n",
        "    \"\"\"\n",
        "    h = image.shape[0]\n",
        "    w = image.shape[1]\n",
        "    crop_h = h % base\n",
        "    crop_w = w % base\n",
        "    return image[crop_h // 2:h - crop_h + crop_h // 2, crop_w // 2:w - crop_w + crop_w // 2, :]\n",
        "\n",
        "def data_augmentation(image, mode):\n",
        "    if mode == 0:\n",
        "        # original\n",
        "        out = image.numpy()\n",
        "    elif mode == 1:\n",
        "        # flip up and down\n",
        "        out = np.flipud(image)\n",
        "    elif mode == 2:\n",
        "        # rotate counterwise 90 degree\n",
        "        out = np.rot90(image)\n",
        "    elif mode == 3:\n",
        "        # rotate 90 degree and flip up and down\n",
        "        out = np.rot90(image)\n",
        "        out = np.flipud(out)\n",
        "    elif mode == 4:\n",
        "        # rotate 180 degree\n",
        "        out = np.rot90(image, k=2)\n",
        "    elif mode == 5:\n",
        "        # rotate 180 degree and flip\n",
        "        out = np.rot90(image, k=2)\n",
        "        out = np.flipud(out)\n",
        "    elif mode == 6:\n",
        "        # rotate 270 degree\n",
        "        out = np.rot90(image, k=3)\n",
        "    elif mode == 7:\n",
        "        # rotate 270 degree and flip\n",
        "        out = np.rot90(image, k=3)\n",
        "        out = np.flipud(out)\n",
        "    else:\n",
        "        raise Exception('Invalid choice of image transformation')\n",
        "    return out\n",
        "\n",
        "def random_augmentation(*args):\n",
        "    out = []\n",
        "    flag_aug = random.randint(1, 7)\n",
        "    for data in args:\n",
        "        out.append(data_augmentation(data, flag_aug).copy())\n",
        "    return out\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "################# DATASETS\n",
        "\n",
        "\n",
        "class InstructIRTrainDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset for Image Restoration having low-quality image and the reference image.\n",
        "    Tasks: synthetic denoising, deblurring, super-res, etc.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, args):\n",
        "\n",
        "        super(InstructIRTrainDataset, self).__init__()\n",
        "\n",
        "        self.toTensor  = ToTensor()\n",
        "        self.noise_gradation_generator = NoiseDegradation(args)\n",
        "        self.args = args\n",
        "        self.de_type = args.de_type\n",
        "        print(self.de_type)\n",
        "\n",
        "        self._init_ids()\n",
        "        self._merge_ids()\n",
        "\n",
        "        self.crop_transform = Compose([\n",
        "            ToPILImage(),\n",
        "            RandomCrop(args.patch_size)\n",
        "        ])\n",
        "\n",
        "    def _crop_patch(self, img_1, img_2):\n",
        "        H = img_1.shape[0]\n",
        "        W = img_1.shape[1]\n",
        "        ind_H = random.randint(0, H - self.args.patch_size)\n",
        "        ind_W = random.randint(0, W - self.args.patch_size)\n",
        "\n",
        "        patch_1 = img_1[ind_H:ind_H + self.args.patch_size, ind_W:ind_W + self.args.patch_size]\n",
        "        patch_2 = img_2[ind_H:ind_H + self.args.patch_size, ind_W:ind_W + self.args.patch_size]\n",
        "\n",
        "        return patch_1, patch_2\n",
        "\n",
        "    def _get_original_rain_name(self, rainy_name):\n",
        "        og_name = rainy_name.split(\"rainy\")[0] + 'original/norain-' + rainy_name.split('rain-')[-1]\n",
        "        return og_name\n",
        "\n",
        "    def _init_clean_image_for_noise_degradation(self):\n",
        "        ref_file = self.args.data_file_dir + \"clean_image_for_denoise.txt\"\n",
        "        temp_ids = []\n",
        "        temp_ids+= [id_.strip() for id_ in open(ref_file)]\n",
        "        clean_ids = []\n",
        "        name_list = os.listdir(self.args.denoise_dir)\n",
        "        clean_ids += [self.args.denoise_dir + id_ for id_ in name_list if id_.strip() in temp_ids]\n",
        "\n",
        "        self.s15_ids = []\n",
        "        self.s25_ids = []\n",
        "        self.s50_ids = []\n",
        "\n",
        "        if 'denoise_15' in self.de_type:\n",
        "            self.s15_ids = [{\"clean_id\": x,\"de_type\":0} for x in clean_ids]\n",
        "            self.s15_ids = self.s15_ids\n",
        "            random.shuffle(self.s15_ids)\n",
        "            self.s15_counter = 0\n",
        "        if 'denoise_25' in self.de_type:\n",
        "            self.s25_ids = [{\"clean_id\": x,\"de_type\":7} for x in clean_ids]\n",
        "            self.s25_ids = self.s25_ids\n",
        "            random.shuffle(self.s25_ids)\n",
        "            self.s25_counter = 0\n",
        "        if 'denoise_50' in self.de_type:\n",
        "            self.s50_ids = [{\"clean_id\": x,\"de_type\":8} for x in clean_ids]\n",
        "            self.s50_ids = self.s50_ids\n",
        "            random.shuffle(self.s50_ids)\n",
        "            self.s50_counter = 0\n",
        "\n",
        "\n",
        "        print(f\"Noisy Sigma 15 images len: {len(self.s15_ids)}\")\n",
        "        print(f\"Noisy Sigma 25 images len: {len(self.s25_ids)}\\n\")\n",
        "        print(f\"Noisy Sigma 50 images len: {len(self.s50_ids)}\\n\")\n",
        "\n",
        "\n",
        "    def _init_rs_ids(self):\n",
        "        temp_ids = []\n",
        "        rs = self.args.data_file_dir + \"/rainy.txt\"\n",
        "        temp_ids+= [self.args.derain_dir + id_.strip() for id_ in open(rs)]\n",
        "        self.rs_ids = [{\"clean_id\":x, \"de_type\":2} for x in temp_ids]\n",
        "        self.rs_ids = self.rs_ids\n",
        "\n",
        "        self.rl_counter = 0\n",
        "        self.num_rl = len(self.rs_ids)\n",
        "        print(\"Total Rainy Images : {}\".format(self.num_rl))\n",
        "\n",
        "    def _init_hazy_ids(self):\n",
        "        temp_ids = []\n",
        "        hazy = self.args.data_file_dir + \"hazy_outside.txt\"\n",
        "        temp_ids+= [self.args.dehaze_dir + id_.strip() for id_ in open(hazy)]\n",
        "        self.hazy_ids = [{\"clean_id\" : x,\"de_type\":3} for x in temp_ids]\n",
        "\n",
        "        self.hazy_counter = 0\n",
        "\n",
        "        self.num_hazy = len(self.hazy_ids)\n",
        "        print(\"Total Hazy Images : {}\".format(self.num_hazy))\n",
        "\n",
        "    def _get_nonhazy_name(self, hazy_name):\n",
        "        dir_name = hazy_name.split(\"synthetic\")[0] + 'original/'\n",
        "        name = hazy_name.split('/')[-1].split('_')[0]\n",
        "        suffix = '.' + hazy_name.split('.')[-1]\n",
        "        nonhazy_name = dir_name + name + suffix\n",
        "        return nonhazy_name\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset_ids)\n",
        "\n",
        "    def _init_ids(self):\n",
        "        if 'denoise_15' in self.de_type or 'denoise_25' in self.de_type or 'denoise_50' in self.de_type:\n",
        "            self._init_clean_image_for_noise_degradation()\n",
        "        if 'derain' in self.de_type:\n",
        "            self._init_rs_ids()\n",
        "        if 'dehaze' in self.de_type:\n",
        "            self._init_hazy_ids()\n",
        "\n",
        "        random.shuffle(self.de_type)\n",
        "\n",
        "    def _merge_ids(self):\n",
        "        self.dataset_ids = []\n",
        "        if \"denoise_15\" in self.de_type:\n",
        "            self.dataset_ids += self.s15_ids\n",
        "            self.dataset_ids += self.s25_ids\n",
        "            self.dataset_ids += self.s50_ids\n",
        "        if \"derain\" in self.de_type:\n",
        "            self.dataset_ids+= self.rs_ids\n",
        "        if \"dehaze\" in self.de_type:\n",
        "            self.dataset_ids += self.hazy_ids\n",
        "\n",
        "        print(f\"Dataset_ids length: {len(self.dataset_ids)}\")\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        dataset = self.dataset_ids[idx]\n",
        "        hq_path = dataset[\"clean_id\"]\n",
        "        deg_id = dataset[\"de_type\"]\n",
        "\n",
        "        if deg_id == 0 or deg_id == 7 or deg_id == 8:\n",
        "            # noisy image removal\n",
        "            if deg_id == 0:\n",
        "                hq_path = dataset[\"clean_id\"]\n",
        "            elif deg_id == 7:\n",
        "                hq_path = dataset[\"clean_id\"]\n",
        "            elif deg_id == 8:\n",
        "                hq_path = dataset[\"clean_id\"]\n",
        "\n",
        "            clean_img = crop_img(np.array(Image.open(hq_path).convert('RGB')), base=16)\n",
        "            clean_patch = self.crop_transform(clean_img)\n",
        "            clean_patch= np.array(clean_patch)\n",
        "\n",
        "            clean_name = hq_path.split(\"/\")[-1].split('.')[0]\n",
        "\n",
        "            clean_patch = random_augmentation(clean_patch)[0]\n",
        "\n",
        "            degrad_patch = self.noise_gradation_generator.add_noise_degradation(clean_patch, deg_id)\n",
        "        else:\n",
        "            if deg_id == 2:\n",
        "                # Rain Streak Removal\n",
        "                degrad_img = crop_img(np.array(Image.open(dataset[\"clean_id\"]).convert('RGB')), base=16)\n",
        "                clean_name = self._get_original_rain_name(dataset[\"clean_id\"])\n",
        "                clean_img = crop_img(np.array(Image.open(clean_name).convert('RGB')), base=16)\n",
        "            elif deg_id == 3:\n",
        "                # Dehazing with SOTS outdoor training set\n",
        "                degrad_img = crop_img(np.array(Image.open(dataset[\"clean_id\"]).convert('RGB')), base=16)\n",
        "                clean_name = self._get_nonhazy_name(dataset[\"clean_id\"])\n",
        "                # print(f\"hazy: clean_name: {clean_name}, degraded name: {dataset['clean_id']}\")\n",
        "                clean_img = crop_img(np.array(Image.open(clean_name).convert('RGB')), base=16)\n",
        "\n",
        "            degrad_patch, clean_patch = random_augmentation(*self._crop_patch(degrad_img, clean_img))\n",
        "\n",
        "        clean_patch = self.toTensor(clean_patch)\n",
        "        degrad_patch = self.toTensor(degrad_patch)\n",
        "\n",
        "        return [clean_name, deg_id], degrad_patch, clean_patch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eBow9JTvBUX"
      },
      "source": [
        "#Scheduler.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "jvy4r2TuvDMY"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from torch.optim.lr_scheduler import _LRScheduler\n",
        "import warnings\n",
        "from typing import List\n",
        "from torch.optim import Optimizer\n",
        "\n",
        "# ------------------------------------------------------------------------\n",
        "# Copyright (c) 2023 va1shn9v. All Rights Reserved.\n",
        "# ------------------------------------------------------------------------\n",
        "# Source: https://github.com/va1shn9v/PromptIR\n",
        "\n",
        "'''\n",
        "@inproceedings{potlapalli2023promptir,\n",
        "  title={PromptIR: Prompting for All-in-One Image Restoration},\n",
        "  author={Potlapalli, Vaishnav and Zamir, Syed Waqas and Khan, Salman and Khan, Fahad},\n",
        "  booktitle={Thirty-seventh Conference on Neural Information Processing Systems},\n",
        "  year={2023}\n",
        "}\n",
        "'''\n",
        "class LinearWarmupCosineAnnealingLR(_LRScheduler):\n",
        "    \"\"\"Sets the learning rate of each parameter group to follow a linear warmup schedule between warmup_start_lr\n",
        "    and base_lr followed by a cosine annealing schedule between base_lr and eta_min.\n",
        "    .. warning::\n",
        "        It is recommended to call :func:`.step()` for :class:`LinearWarmupCosineAnnealingLR`\n",
        "        after each iteration as calling it after each epoch will keep the starting lr at\n",
        "        warmup_start_lr for the first epoch which is 0 in most cases.\n",
        "    .. warning::\n",
        "        passing epoch to :func:`.step()` is being deprecated and comes with an EPOCH_DEPRECATION_WARNING.\n",
        "        It calls the :func:`_get_closed_form_lr()` method for this scheduler instead of\n",
        "        :func:`get_lr()`. Though this does not change the behavior of the scheduler, when passing\n",
        "        epoch param to :func:`.step()`, the user should call the :func:`.step()` function before calling\n",
        "        train and validation methods.\n",
        "    Example:\n",
        "        >>> layer = nn.Linear(10, 1)\n",
        "        >>> optimizer = Adam(layer.parameters(), lr=0.02)\n",
        "        >>> scheduler = LinearWarmupCosineAnnealingLR(optimizer, warmup_epochs=10, max_epochs=40)\n",
        "        >>> #\n",
        "        >>> # the default case\n",
        "        >>> for epoch in range(40):\n",
        "        ...     # train(...)\n",
        "        ...     # validate(...)\n",
        "        ...     scheduler.step()\n",
        "        >>> #\n",
        "        >>> # passing epoch param case\n",
        "        >>> for epoch in range(40):\n",
        "        ...     scheduler.step(epoch)\n",
        "        ...     # train(...)\n",
        "        ...     # validate(...)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        optimizer: Optimizer,\n",
        "        warmup_epochs: int,\n",
        "        max_epochs: int,\n",
        "        warmup_start_lr: float = 0.0,\n",
        "        eta_min: float = 0.0,\n",
        "        last_epoch: int = -1,\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            optimizer (Optimizer): Wrapped optimizer.\n",
        "            warmup_epochs (int): Maximum number of iterations for linear warmup\n",
        "            max_epochs (int): Maximum number of iterations\n",
        "            warmup_start_lr (float): Learning rate to start the linear warmup. Default: 0.\n",
        "            eta_min (float): Minimum learning rate. Default: 0.\n",
        "            last_epoch (int): The index of last epoch. Default: -1.\n",
        "        \"\"\"\n",
        "        self.warmup_epochs = warmup_epochs\n",
        "        self.max_epochs = max_epochs\n",
        "        self.warmup_start_lr = warmup_start_lr\n",
        "        self.eta_min = eta_min\n",
        "\n",
        "        super().__init__(optimizer, last_epoch)\n",
        "\n",
        "    def get_lr(self) -> List[float]:\n",
        "        \"\"\"Compute learning rate using chainable form of the scheduler.\"\"\"\n",
        "        if not self._get_lr_called_within_step:\n",
        "            warnings.warn(\n",
        "                \"To get the last learning rate computed by the scheduler, \" \"please use `get_last_lr()`.\",\n",
        "                UserWarning,\n",
        "            )\n",
        "\n",
        "        if self.last_epoch == 0:\n",
        "            return [self.warmup_start_lr] * len(self.base_lrs)\n",
        "        if self.last_epoch < self.warmup_epochs:\n",
        "            return [\n",
        "                group[\"lr\"] + (base_lr - self.warmup_start_lr) / (self.warmup_epochs - 1)\n",
        "                for base_lr, group in zip(self.base_lrs, self.optimizer.param_groups)\n",
        "            ]\n",
        "        if self.last_epoch == self.warmup_epochs:\n",
        "            return self.base_lrs\n",
        "        if (self.last_epoch - 1 - self.max_epochs) % (2 * (self.max_epochs - self.warmup_epochs)) == 0:\n",
        "            return [\n",
        "                group[\"lr\"]\n",
        "                + (base_lr - self.eta_min) * (1 - math.cos(math.pi / (self.max_epochs - self.warmup_epochs))) / 2\n",
        "                for base_lr, group in zip(self.base_lrs, self.optimizer.param_groups)\n",
        "            ]\n",
        "\n",
        "        return [\n",
        "            (1 + math.cos(math.pi * (self.last_epoch - self.warmup_epochs) / (self.max_epochs - self.warmup_epochs)))\n",
        "            / (\n",
        "                1\n",
        "                + math.cos(\n",
        "                    math.pi * (self.last_epoch - self.warmup_epochs - 1) / (self.max_epochs - self.warmup_epochs)\n",
        "                )\n",
        "            )\n",
        "            * (group[\"lr\"] - self.eta_min)\n",
        "            + self.eta_min\n",
        "            for group in self.optimizer.param_groups\n",
        "        ]\n",
        "\n",
        "    def _get_closed_form_lr(self) -> List[float]:\n",
        "        \"\"\"Called when epoch is passed as a param to the `step` function of the scheduler.\"\"\"\n",
        "        if self.last_epoch < self.warmup_epochs:\n",
        "            return [\n",
        "                self.warmup_start_lr + self.last_epoch * (base_lr - self.warmup_start_lr) / (self.warmup_epochs - 1)\n",
        "                for base_lr in self.base_lrs\n",
        "            ]\n",
        "\n",
        "        return [\n",
        "            self.eta_min\n",
        "            + 0.5\n",
        "            * (base_lr - self.eta_min)\n",
        "            * (1 + math.cos(math.pi * (self.last_epoch - self.warmup_epochs) / (self.max_epochs - self.warmup_epochs)))\n",
        "            for base_lr in self.base_lrs\n",
        "        ]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0o4Doz6_vLkn"
      },
      "source": [
        "#Offsetgenerator.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "HF7zIQVvvMa5"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from torch.nn.modules.batchnorm import _BatchNorm\n",
        "from torch.nn  import functional as F\n",
        "import torch\n",
        "\n",
        "class OffsetGenerator(nn.Module):\n",
        "    def __init__(self, in_prompt_dim, out_conv_shapes):\n",
        "        \"\"\"\n",
        "        in_prompt_dim: The number of channels in the incoming prompt (e.g. prompt_dim).\n",
        "        out_conv_shapes: Some descriptor of how many weights or channels\n",
        "                         you need to offset (e.g. #channels_out * #channels_in * kernel_dim^2).\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "        out_channel = 32\n",
        "\n",
        "        # refine local patterns in the prompt\n",
        "        self.conv = nn.Conv2d(in_prompt_dim, out_channel, kernel_size=3, padding=1, stride=1)\n",
        "\n",
        "        self.global_average_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        self.output_conv_size = out_conv_shapes\n",
        "\n",
        "        self.flatten_layer = nn.Linear(in_features=out_channel, out_features=self.output_conv_size)\n",
        "\n",
        "\n",
        "    def forward(self, prompt):\n",
        "        \"\"\"\n",
        "        prompt: output from PGM with shape (B, prompt_dim, H, W)\n",
        "        returns: shape (B, out_conv_size)\n",
        "        \"\"\"\n",
        "\n",
        "        prompt = self.conv(prompt)\n",
        "\n",
        "        prompt = self.global_average_pool(prompt)\n",
        "\n",
        "        prompt = prompt.view(prompt.size(0), -1)\n",
        "\n",
        "        offset_vector = self.flatten_layer(prompt)\n",
        "\n",
        "        offset_vector = torch.tanh(offset_vector)\n",
        "\n",
        "        return offset_vector\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpGe_xNBvLz2"
      },
      "source": [
        "#nafnet.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "9G1Ldau8vM2b"
      },
      "outputs": [],
      "source": [
        "# ------------------------------------------------------------------------\n",
        "# Copyright (c) 2022 megvii-model. All Rights Reserved.\n",
        "# ------------------------------------------------------------------------\n",
        "# Source: https://github.com/megvii-research/NAFNet\n",
        "\n",
        "'''\n",
        "Simple Baselines for Image Restoration\n",
        "\n",
        "@article{chen2022simple,\n",
        "  title={Simple Baselines for Image Restoration},\n",
        "  author={Chen, Liangyu and Chu, Xiaojie and Zhang, Xiangyu and Sun, Jian},\n",
        "  journal={arXiv preprint arXiv:2204.04676},\n",
        "  year={2022}\n",
        "}\n",
        "'''\n",
        "\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import init as init\n",
        "# from torch.nn.modules.batchnorm import _BatchNorm\n",
        "# from models.nafnet_utils import Local_Base, LayerNorm2d\n",
        "# from models.cbin_weight import CBINorm_Conv2d\n",
        "\n",
        "# ------------------------------------------------------------------------\n",
        "# Copyright (c) 2022 megvii-model. All Rights Reserved.\n",
        "# ------------------------------------------------------------------------\n",
        "# Source: https://github.com/megvii-research/NAFNet\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "class LayerNormFunction(torch.autograd.Function):\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, weight, bias, eps):\n",
        "        ctx.eps = eps\n",
        "        N, C, H, W = x.size()\n",
        "        mu = x.mean(1, keepdim=True)\n",
        "        var = (x - mu).pow(2).mean(1, keepdim=True)\n",
        "        y = (x - mu) / (var + eps).sqrt()\n",
        "        ctx.save_for_backward(y, var, weight)\n",
        "        y = weight.view(1, C, 1, 1) * y + bias.view(1, C, 1, 1)\n",
        "        return y\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        eps = ctx.eps\n",
        "\n",
        "        N, C, H, W = grad_output.size()\n",
        "        y, var, weight = ctx.saved_variables\n",
        "        g = grad_output * weight.view(1, C, 1, 1)\n",
        "        mean_g = g.mean(dim=1, keepdim=True)\n",
        "\n",
        "        mean_gy = (g * y).mean(dim=1, keepdim=True)\n",
        "        gx = 1. / torch.sqrt(var + eps) * (g - y * mean_gy - mean_g)\n",
        "        return gx, (grad_output * y).sum(dim=3).sum(dim=2).sum(dim=0), grad_output.sum(dim=3).sum(dim=2).sum(\n",
        "            dim=0), None\n",
        "\n",
        "class LayerNorm2d(nn.Module):\n",
        "\n",
        "    def __init__(self, channels, eps=1e-6):\n",
        "        super(LayerNorm2d, self).__init__()\n",
        "        self.register_parameter('weight', nn.Parameter(torch.ones(channels)))\n",
        "        self.register_parameter('bias', nn.Parameter(torch.zeros(channels)))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        return LayerNormFunction.apply(x, self.weight, self.bias, self.eps)\n",
        "\n",
        "\n",
        "\n",
        "class AvgPool2d(nn.Module):\n",
        "    def __init__(self, kernel_size=None, base_size=None, auto_pad=True, fast_imp=False, train_size=None):\n",
        "        super().__init__()\n",
        "        self.kernel_size = kernel_size\n",
        "        self.base_size = base_size\n",
        "        self.auto_pad = auto_pad\n",
        "\n",
        "        # only used for fast implementation\n",
        "        self.fast_imp = fast_imp\n",
        "        self.rs = [5, 4, 3, 2, 1]\n",
        "        self.max_r1 = self.rs[0]\n",
        "        self.max_r2 = self.rs[0]\n",
        "        self.train_size = train_size\n",
        "\n",
        "    def extra_repr(self) -> str:\n",
        "        return 'kernel_size={}, base_size={}, stride={}, fast_imp={}'.format(\n",
        "            self.kernel_size, self.base_size, self.kernel_size, self.fast_imp\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.kernel_size is None and self.base_size:\n",
        "            train_size = self.train_size\n",
        "            if isinstance(self.base_size, int):\n",
        "                self.base_size = (self.base_size, self.base_size)\n",
        "            self.kernel_size = list(self.base_size)\n",
        "            self.kernel_size[0] = x.shape[2] * self.base_size[0] // train_size[-2]\n",
        "            self.kernel_size[1] = x.shape[3] * self.base_size[1] // train_size[-1]\n",
        "\n",
        "            # only used for fast implementation\n",
        "            self.max_r1 = max(1, self.rs[0] * x.shape[2] // train_size[-2])\n",
        "            self.max_r2 = max(1, self.rs[0] * x.shape[3] // train_size[-1])\n",
        "\n",
        "        if self.kernel_size[0] >= x.size(-2) and self.kernel_size[1] >= x.size(-1):\n",
        "            return F.adaptive_avg_pool2d(x, 1)\n",
        "\n",
        "        if self.fast_imp:  # Non-equivalent implementation but faster\n",
        "            h, w = x.shape[2:]\n",
        "            if self.kernel_size[0] >= h and self.kernel_size[1] >= w:\n",
        "                out = F.adaptive_avg_pool2d(x, 1)\n",
        "            else:\n",
        "                r1 = [r for r in self.rs if h % r == 0][0]\n",
        "                r2 = [r for r in self.rs if w % r == 0][0]\n",
        "                # reduction_constraint\n",
        "                r1 = min(self.max_r1, r1)\n",
        "                r2 = min(self.max_r2, r2)\n",
        "                s = x[:, :, ::r1, ::r2].cumsum(dim=-1).cumsum(dim=-2)\n",
        "                n, c, h, w = s.shape\n",
        "                k1, k2 = min(h - 1, self.kernel_size[0] // r1), min(w - 1, self.kernel_size[1] // r2)\n",
        "                out = (s[:, :, :-k1, :-k2] - s[:, :, :-k1, k2:] - s[:, :, k1:, :-k2] + s[:, :, k1:, k2:]) / (k1 * k2)\n",
        "                out = torch.nn.functional.interpolate(out, scale_factor=(r1, r2))\n",
        "        else:\n",
        "            n, c, h, w = x.shape\n",
        "            s = x.cumsum(dim=-1).cumsum_(dim=-2)\n",
        "            s = torch.nn.functional.pad(s, (1, 0, 1, 0))  # pad 0 for convenience\n",
        "            k1, k2 = min(h, self.kernel_size[0]), min(w, self.kernel_size[1])\n",
        "            s1, s2, s3, s4 = s[:, :, :-k1, :-k2], s[:, :, :-k1, k2:], s[:, :, k1:, :-k2], s[:, :, k1:, k2:]\n",
        "            out = s4 + s1 - s2 - s3\n",
        "            out = out / (k1 * k2)\n",
        "\n",
        "        if self.auto_pad:\n",
        "            n, c, h, w = x.shape\n",
        "            _h, _w = out.shape[2:]\n",
        "            # print(x.shape, self.kernel_size)\n",
        "            pad2d = ((w - _w) // 2, (w - _w + 1) // 2, (h - _h) // 2, (h - _h + 1) // 2)\n",
        "            out = torch.nn.functional.pad(out, pad2d, mode='replicate')\n",
        "\n",
        "        return out\n",
        "\n",
        "def replace_layers(model, base_size, train_size, fast_imp, **kwargs):\n",
        "    for n, m in model.named_children():\n",
        "        if len(list(m.children())) > 0:\n",
        "            ## compound module, go inside it\n",
        "            replace_layers(m, base_size, train_size, fast_imp, **kwargs)\n",
        "\n",
        "        if isinstance(m, nn.AdaptiveAvgPool2d):\n",
        "            pool = AvgPool2d(base_size=base_size, fast_imp=fast_imp, train_size=train_size)\n",
        "            assert m.output_size == 1\n",
        "            setattr(model, n, pool)\n",
        "\n",
        "\n",
        "'''\n",
        "ref.\n",
        "@article{chu2021tlsc,\n",
        "  title={Revisiting Global Statistics Aggregation for Improving Image Restoration},\n",
        "  author={Chu, Xiaojie and Chen, Liangyu and and Chen, Chengpeng and Lu, Xin},\n",
        "  journal={arXiv preprint arXiv:2112.04491},\n",
        "  year={2021}\n",
        "}\n",
        "'''\n",
        "class Local_Base():\n",
        "    def convert(self, *args, train_size, **kwargs):\n",
        "        replace_layers(self, *args, train_size=train_size, **kwargs)\n",
        "        imgs = torch.rand(train_size)\n",
        "        with torch.no_grad():\n",
        "            self.forward(imgs)\n",
        "\n",
        "\n",
        "class SimpleGate(nn.Module):\n",
        "    def forward(self, x):\n",
        "        x1, x2 = x.chunk(2, dim=1)\n",
        "        return x1 * x2\n",
        "\n",
        "class NAFBlock(nn.Module):\n",
        "    def __init__(self, c, DW_Expand=2, FFN_Expand=2, drop_out_rate=0., modify_conv_weights=False):\n",
        "        super().__init__()\n",
        "        dw_channel = c * DW_Expand\n",
        "        self.conv1 = nn.Conv2d(in_channels=c, out_channels=dw_channel, kernel_size=1, padding=0, stride=1, groups=1, bias=True)\n",
        "        self.conv2 = nn.Conv2d(in_channels=dw_channel, out_channels=dw_channel, kernel_size=3, padding=1, stride=1, groups=dw_channel,\n",
        "                               bias=True)\n",
        "        self.conv3 = nn.Conv2d(in_channels=dw_channel // 2, out_channels=c, kernel_size=1, padding=0, stride=1, groups=1, bias=True)\n",
        "\n",
        "        # Simplified Channel Attention\n",
        "        self.sca = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Conv2d(in_channels=dw_channel // 2, out_channels=dw_channel // 2, kernel_size=1, padding=0, stride=1,\n",
        "                      groups=1, bias=True),\n",
        "        )\n",
        "\n",
        "        # SimpleGate\n",
        "        self.sg = SimpleGate()\n",
        "\n",
        "        ffn_channel = FFN_Expand * c\n",
        "        self.conv4 = nn.Conv2d(in_channels=c, out_channels=ffn_channel, kernel_size=1, padding=0, stride=1, groups=1, bias=True)\n",
        "        self.conv5 = nn.Conv2d(in_channels=ffn_channel // 2, out_channels=c, kernel_size=1, padding=0, stride=1, groups=1, bias=True)\n",
        "\n",
        "        self.norm1 = LayerNorm2d(c)\n",
        "        self.norm2 = LayerNorm2d(c)\n",
        "\n",
        "        self.dropout1 = nn.Dropout(drop_out_rate) if drop_out_rate > 0. else nn.Identity()\n",
        "        self.dropout2 = nn.Dropout(drop_out_rate) if drop_out_rate > 0. else nn.Identity()\n",
        "\n",
        "        self.beta = nn.Parameter(torch.zeros((1, c, 1, 1)), requires_grad=True)\n",
        "        self.gamma = nn.Parameter(torch.zeros((1, c, 1, 1)), requires_grad=True)\n",
        "\n",
        "    def forward(self, inp, offset_vector=None):\n",
        "        x = inp\n",
        "\n",
        "        x = self.norm1(x)\n",
        "\n",
        "        x = self.conv1(x)\n",
        "\n",
        "        # # modifying the conv weights\n",
        "        # if self.modify_conv_weights:\n",
        "        #     self.conv1.weight.data = self.weights_modifier_1(self.conv1.weight, prompt)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.sg(x)\n",
        "        x = x * self.sca(x)\n",
        "        x = self.conv3(x)\n",
        "\n",
        "        x = self.dropout1(x)\n",
        "\n",
        "        y = inp + x * self.beta\n",
        "\n",
        "        x = self.conv4(self.norm2(y))\n",
        "        x = self.sg(x)\n",
        "\n",
        "        if offset_vector is not None:\n",
        "            base_weight = self.conv5.weight\n",
        "            # Correct the reshaping logic to match the offset vector size\n",
        "            offset_reshaped = offset_vector.view(offset_vector.shape[0], self.conv5.out_channels, self.conv5.in_channels, self.conv5.kernel_size[0], self.conv5.kernel_size[1])\n",
        "\n",
        "            # Apply offset to the base weights, considering the batch dimension\n",
        "            # This assumes you want to apply different offsets per item in the batch\n",
        "            modulated_weight = base_weight[None, ...]  + offset_reshaped\n",
        "\n",
        "\n",
        "            # Perform convolution using modulated weights for each item/image in the batch\n",
        "            x_list = []\n",
        "            for i in range(offset_vector.shape[0]):\n",
        "                x_list.append(F.conv2d(\n",
        "                    x[i:i+1], modulated_weight[i],\n",
        "                    bias=self.conv5.bias,\n",
        "                    stride=self.conv5.stride,\n",
        "                    padding=self.conv5.padding,\n",
        "                    dilation=self.conv5.dilation,\n",
        "                    groups=self.conv5.groups\n",
        "               ))\n",
        "            x = torch.cat(x_list, dim=0)\n",
        "        else:\n",
        "            x = self.conv5(x)\n",
        "\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "        return y + x * self.gamma\n",
        "\n",
        "\n",
        "class NAFNet(nn.Module):\n",
        "\n",
        "    def __init__(self, img_channel=3, width=16, middle_blk_num=1, enc_blk_nums=[], dec_blk_nums=[]):\n",
        "        super().__init__()\n",
        "\n",
        "        self.intro = nn.Conv2d(in_channels=img_channel, out_channels=width, kernel_size=3, padding=1, stride=1, groups=1,\n",
        "                              bias=True)\n",
        "        self.ending = nn.Conv2d(in_channels=width, out_channels=img_channel, kernel_size=3, padding=1, stride=1, groups=1,\n",
        "                              bias=True)\n",
        "\n",
        "        self.encoders = nn.ModuleList()\n",
        "        self.decoders = nn.ModuleList()\n",
        "        self.middle_blks = nn.ModuleList()\n",
        "        self.ups = nn.ModuleList()\n",
        "        self.downs = nn.ModuleList()\n",
        "\n",
        "        chan = width\n",
        "        for num in enc_blk_nums:\n",
        "            self.encoders.append(\n",
        "                nn.Sequential(\n",
        "                    *[NAFBlock(chan) for _ in range(num)]\n",
        "                )\n",
        "            )\n",
        "            self.downs.append(\n",
        "                nn.Conv2d(chan, 2*chan, 2, 2)\n",
        "            )\n",
        "            chan = chan * 2\n",
        "\n",
        "        self.middle_blks = \\\n",
        "            nn.Sequential(\n",
        "                *[NAFBlock(chan) for _ in range(middle_blk_num)]\n",
        "            )\n",
        "\n",
        "        for num in dec_blk_nums:\n",
        "            self.ups.append(\n",
        "                nn.Sequential(\n",
        "                    nn.Conv2d(chan, chan * 2, 1, bias=False),\n",
        "                    nn.PixelShuffle(2)\n",
        "                )\n",
        "            )\n",
        "            chan = chan // 2\n",
        "            self.decoders.append(\n",
        "                nn.Sequential(\n",
        "                    *[NAFBlock(chan) for _ in range(num)]\n",
        "                )\n",
        "            )\n",
        "\n",
        "        self.padder_size = 2 ** len(self.encoders)\n",
        "\n",
        "    def forward(self, inp):\n",
        "        B, C, H, W = inp.shape\n",
        "        inp = self.check_image_size(inp)\n",
        "\n",
        "        x = self.intro(inp)\n",
        "\n",
        "        encs = []\n",
        "\n",
        "        for encoder, down in zip(self.encoders, self.downs):\n",
        "            x = encoder(x)\n",
        "            encs.append(x)\n",
        "            x = down(x)\n",
        "\n",
        "        x = self.middle_blks(x)\n",
        "\n",
        "        for decoder, up, enc_skip in zip(self.decoders, self.ups, encs[::-1]):\n",
        "            x = up(x)\n",
        "            x = x + enc_skip\n",
        "            x = decoder(x)\n",
        "\n",
        "        x = self.ending(x)\n",
        "        x = x + inp\n",
        "\n",
        "        return x[:, :, :H, :W]\n",
        "\n",
        "    def check_image_size(self, x):\n",
        "        _, _, h, w = x.size()\n",
        "        mod_pad_h = (self.padder_size - h % self.padder_size) % self.padder_size\n",
        "        mod_pad_w = (self.padder_size - w % self.padder_size) % self.padder_size\n",
        "        x = F.pad(x, (0, mod_pad_w, 0, mod_pad_h))\n",
        "        return x\n",
        "\n",
        "class NAFNetLocal(Local_Base, NAFNet):\n",
        "    def __init__(self, *args, train_size=(1, 3, 256, 256), fast_imp=False, **kwargs):\n",
        "        Local_Base.__init__(self)\n",
        "        NAFNet.__init__(self, *args, **kwargs)\n",
        "\n",
        "        N, C, H, W = train_size\n",
        "        base_size = (int(H * 1.5), int(W * 1.5))\n",
        "\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            self.convert(base_size=base_size, train_size=train_size, fast_imp=fast_imp)\n",
        "\n",
        "\n",
        "def create_nafnet(input_channels = 3, width = 32, enc_blks = [2, 2, 4, 8], middle_blk_num = 12, dec_blks = [2, 2, 2, 2]):\n",
        "    \"\"\"\n",
        "    Create Nafnet model\n",
        "    https://github.com/megvii-research/NAFNet/blob/main/options/test/SIDD/NAFNet-width32.yml\n",
        "    \"\"\"\n",
        "\n",
        "    net = NAFNet(img_channel=input_channels, width=width, middle_blk_num=middle_blk_num,\n",
        "                      enc_blk_nums=enc_blks, dec_blk_nums=dec_blks)\n",
        "\n",
        "    # inp_shape = (3, 256, 256)\n",
        "\n",
        "    # from ptflops import get_model_complexity_info\n",
        "\n",
        "    # macs, params = get_model_complexity_info(net, inp_shape, verbose=False, print_per_layer_stat=False)\n",
        "\n",
        "    # params = float(params[:-3])\n",
        "    # macs = float(macs[:-4])\n",
        "\n",
        "    # print(macs, params)\n",
        "\n",
        "    return net"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jktTXnLvFxn"
      },
      "source": [
        "#InstructIR.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prhO0owVfP3m"
      },
      "source": [
        "# New section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "BsYQBi97vJS1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import init as init\n",
        "# from models.OffsetGenerator import OffsetGenerator\n",
        "# from models.nafnet import NAFBlock\n",
        "\n",
        "# ------------------------------------------------------------------------\n",
        "# Copyright (c) 2023 va1shn9v. All Rights Reserved.\n",
        "# ------------------------------------------------------------------------\n",
        "# Source: https://github.com/va1shn9v/PromptIR\n",
        "\n",
        "'''\n",
        "@inproceedings{potlapalli2023promptir,\n",
        "  title={PromptIR: Prompting for All-in-One Image Restoration},\n",
        "  author={Potlapalli, Vaishnav and Zamir, Syed Waqas and Khan, Salman and Khan, Fahad},\n",
        "  booktitle={Thirty-seventh Conference on Neural Information Processing Systems},\n",
        "  year={2023}\n",
        "}\n",
        "'''\n",
        "##---------- Prompt Gen Module -----------------------\n",
        "class PromptGenBlock(nn.Module):\n",
        "    def __init__(self,prompt_dim=128,prompt_len=5,prompt_size = 96,lin_dim = 192):\n",
        "        super(PromptGenBlock,self).__init__()\n",
        "        self.prompt_dim= prompt_dim\n",
        "        # prompt_dim=128: Defines the number of channels in the prompt.\n",
        "        # prompt_len=5: The number of different prompts available.\n",
        "        # prompt_size=96: The spatial resolution of each prompt (assumed to be square: 9696).\n",
        "        # lin_dim=192: The input dimension for the linear layer.\n",
        "        # prompt_param's size = 1 * N * C * H * W where N = number of prompt components\n",
        "        # prompt_param = learnable parameters\n",
        "        self.prompt_param = nn.Parameter(torch.rand(1,prompt_len,prompt_dim,prompt_size,prompt_size))\n",
        "        # A linear layer takes an input of size \"lin_dim\" and produces \"prompt_len\" outputs.\n",
        "        # This layer generates weights that determine the importance of each prompt.\n",
        "        self.linear_layer = nn.Linear(lin_dim,prompt_len)\n",
        "        # A 33 convolution with the same number of input and output channels (prompt_dim).\n",
        "        # Stride =1 and padding =1 ensure the spatial size remains unchanged.\n",
        "        self.conv3x3 = nn.Conv2d(prompt_dim,prompt_dim,kernel_size=3,stride=1,padding=1,bias=False)\n",
        "\n",
        "    # During training, the PromptGenBlock learns to encode prompt_len different degradations into the prompt_param tensor.\n",
        "    def forward(self,x):\n",
        "        # x = image feature representation\n",
        "        B,C,H,W = x.shape\n",
        "        # x is averaged over the last two dimensions (H, W). The output tensor is of shape (B, C) => global descriptor of the input\n",
        "        emb = x.mean(dim=(-2,-1))\n",
        "        # the embedding is passed through the linear layer to produce a tensor of shape (B, prompt_len)\n",
        "        # softmax() ensures values in the linear layer output sum to 1 across the \"prompt_len\" dimension\n",
        "        # meaning each prompt gets an importance weight for each sample in the batch\n",
        "        prompt_weights = F.softmax(self.linear_layer(emb),dim=1)\n",
        "        # prompt_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1) expands the dimensions of prompt_weights to shape (B, prompt_len, 1, 1, 1).\n",
        "        # self.prompt_param.unsqueeze(0).repeat(B, 1, 1, 1, 1, 1).squeeze(1):\n",
        "        # Expands prompt_param to (B, prompt_len, prompt_dim, prompt_size, prompt_size), so each batch element has its own copy.\n",
        "        # repeat(B, 1, 1, 1, 1, 1) replicates the prompts across the batch.\n",
        "        # The two tensors are multiplied element-wise, meaning each prompt is weighted by the computed prompt_weights.\n",
        "        prompt = prompt_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1) * self.prompt_param.unsqueeze(0).repeat(B,1,1,1,1,1).squeeze(1)\n",
        "        # The weighted sum across prompt_len produces a single prompt tensor for each batch element.\n",
        "        # The new shape is (B, prompt_dim, prompt_size, prompt_size).\n",
        "        prompt = torch.sum(prompt,dim=1)\n",
        "        # resize the spatial dimentions of prompt (prompt_size, prompt_size) to (H, W) while keeping B and prompt_dim unchanged.\n",
        "        prompt = F.interpolate(prompt,(H,W),mode=\"bilinear\")\n",
        "        # The learned prompt undergoes a 33 convolution for further processing without changing the shape of \"prompt\"\n",
        "        prompt = self.conv3x3(prompt)\n",
        "        # final shape of prompt = (B, prompt_dim, H, W)\n",
        "        return prompt\n",
        "        # If multiple degradations exist in the input image feature, the returned prompt will encode a mixture of the degradations it has learned during training\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##########################################################################\n",
        "\n",
        "class ICB(nn.Module):\n",
        "    \"\"\"\n",
        "    Instruction Condition Block (ICB)\n",
        "    Paper Section 3.3\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, feature_dim, text_dim=768):\n",
        "        super(ICB, self).__init__()\n",
        "        self.fc    = nn.Linear(text_dim, feature_dim)\n",
        "        self.block = NAFBlock(feature_dim)\n",
        "        self.beta  = nn.Parameter(torch.zeros((1, feature_dim, 1, 1)), requires_grad=True)\n",
        "        self.gamma = nn.Parameter(torch.zeros((1, feature_dim, 1, 1)), requires_grad=True)\n",
        "    # f' = Block(f * mc) + f\n",
        "    # mc = sigmoid(Wc * emb)\n",
        "    def forward(self, x, text_embedding):\n",
        "        gating_factors = torch.sigmoid(self.fc(text_embedding))\n",
        "        # mc\n",
        "        gating_factors = gating_factors.unsqueeze(-1).unsqueeze(-1)\n",
        "\n",
        "\n",
        "        f = x * self.gamma + self.beta  # 1) learned feature scaling/modulation\n",
        "        f = f * gating_factors          # 2) (soft) feature routing based on text\n",
        "        f = self.block(f)               # 3) block feature enhancement\n",
        "        return f + x # skip connection\n",
        "\n",
        "\n",
        "class InstructIR(nn.Module):\n",
        "    \"\"\"\n",
        "    InstructIR model using NAFNet (ECCV 2022) as backbone.\n",
        "    The model takes as input an RGB image and a text embedding (encoded instruction).\n",
        "    Described in Paper Section 3.3\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, img_channel=3, width=16, middle_blk_num=1, enc_blk_nums=[], dec_blk_nums=[], txtdim=768, include_offset=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.intro  = nn.Conv2d(in_channels=img_channel, out_channels=width, kernel_size=3, padding=1, stride=1, groups=1,\n",
        "                              bias=True)\n",
        "        self.ending = nn.Conv2d(in_channels=width, out_channels=img_channel, kernel_size=3, padding=1, stride=1, groups=1,\n",
        "                              bias=True)\n",
        "\n",
        "        self.encoders    = nn.ModuleList()\n",
        "        self.decoders    = nn.ModuleList()\n",
        "        self.middle_blks = nn.ModuleList()\n",
        "        self.ups         = nn.ModuleList()\n",
        "        self.downs       = nn.ModuleList()\n",
        "        self.enc_cond    = nn.ModuleList()\n",
        "        self.dec_cond    = nn.ModuleList()\n",
        "\n",
        "        self.include_offset = include_offset\n",
        "\n",
        "        chan = width\n",
        "\n",
        "        # if include_offset is True:\n",
        "        self.prompt_block_level1 = PromptGenBlock(prompt_dim=chan*2,prompt_len=3,prompt_size = 128,lin_dim = chan*2)\n",
        "        self.prompt_block_level2 = PromptGenBlock(prompt_dim=chan*4,prompt_len=3,prompt_size = 64,lin_dim = chan*4)\n",
        "        self.prompt_block_level3 = PromptGenBlock(prompt_dim=chan*8,prompt_len=3,prompt_size = 32,lin_dim = chan*8)\n",
        "\n",
        "        # prompt_dim_level3 = chan*2**3\n",
        "\n",
        "        self.promptBlocks = nn.ModuleList()\n",
        "        self.promptBlocks.append(self.prompt_block_level3)\n",
        "        self.promptBlocks.append(self.prompt_block_level2)\n",
        "        self.promptBlocks.append(self.prompt_block_level1)\n",
        "\n",
        "        # self.promptBlocks = [self.prompt_block_level3, self.prompt_block_level2, self.prompt_block_level1]\n",
        "\n",
        "        for num in enc_blk_nums:\n",
        "            #  Each encoder applies multiple NAFBlocks\n",
        "            self.encoders.append(\n",
        "                nn.Sequential(\n",
        "                    *[NAFBlock(chan) for _ in range(num)]\n",
        "                )\n",
        "            )\n",
        "            # Each encoding layer is modulated using a corresponding ICB\n",
        "            # which incorporates the provided text embeddings.\n",
        "            self.enc_cond.append(ICB(chan, txtdim))\n",
        "            # Downsampling layers that reduce spatial resolution while increasing channel dimensions.\n",
        "            self.downs.append(\n",
        "                nn.Conv2d(chan, 2*chan, 2, 2)\n",
        "            )\n",
        "            chan = chan * 2\n",
        "        # Middle blocks: a series of NAFBlocks applied to the deepest,\n",
        "        # most abstract representation of the image features.\n",
        "        # print(f\"middle block chan: {chan}\")\n",
        "        self.middle_blks = nn.Sequential(\n",
        "            *[NAFBlock(chan) for _ in range(middle_blk_num)]\n",
        "        )\n",
        "\n",
        "        naf_block = self.middle_blks[0]\n",
        "        cIn = naf_block.conv5.in_channels\n",
        "        cOut = naf_block.conv5.out_channels\n",
        "        kernel_size = naf_block.conv5.kernel_size\n",
        "        vector_size = cIn * cOut * kernel_size[0] * kernel_size[1]\n",
        "\n",
        "        self.middleblock_offsetGen=OffsetGenerator(in_prompt_dim=chan, out_conv_shapes=vector_size)\n",
        "\n",
        "\n",
        "        self.prompt_block_middle_blks = PromptGenBlock(prompt_dim=chan,prompt_len=3,prompt_size = 32,lin_dim = chan)\n",
        "\n",
        "        # decoding path\n",
        "        for num in dec_blk_nums:\n",
        "            # Upsampling layers that increase the spatial resolution and\n",
        "            # decrease the channel dimensions\n",
        "            self.ups.append(\n",
        "                nn.Sequential(\n",
        "                    nn.Conv2d(chan, chan * 2, 1, bias=False),\n",
        "                    nn.PixelShuffle(2)\n",
        "                )\n",
        "            )\n",
        "            chan = chan // 2\n",
        "            # sequantially processes upsampled features using multiple NAFBlocks\n",
        "            self.decoders.append(\n",
        "                nn.Sequential(\n",
        "                    *[NAFBlock(chan) for _ in range(num)]\n",
        "                )\n",
        "            )\n",
        "            # Add text embedding as modulation\n",
        "            self.dec_cond.append(ICB(chan, txtdim))\n",
        "\n",
        "        self.padder_size = 2 ** len(self.encoders)\n",
        "\n",
        "\n",
        "        # self.offset_generators = []\n",
        "        self.offset_generators = nn.ModuleList()\n",
        "\n",
        "        # if include_offset is True:\n",
        "        for i, decoder in enumerate(self.decoders):\n",
        "            if(i <3):\n",
        "                naf_block = decoder[0]\n",
        "\n",
        "                cIn = naf_block.conv5.in_channels\n",
        "                cOut = naf_block.conv5.out_channels\n",
        "                kernel_size = naf_block.conv5.kernel_size\n",
        "\n",
        "                vector_size = cIn * cOut * kernel_size[0] * kernel_size[1]\n",
        "\n",
        "                # in_prompt_dim = self.promptBlocks[i].prompt_dim\n",
        "\n",
        "                self.offset_generators.append(OffsetGenerator(in_prompt_dim=self.promptBlocks[i].prompt_dim, out_conv_shapes=vector_size))\n",
        "\n",
        "\n",
        "    def forward(self, inp, txtembd):\n",
        "        B, C, H, W = inp.shape\n",
        "        inp = self.check_image_size(inp)\n",
        "        # intro = a convolutional layer to preprocess the input image\n",
        "        x = self.intro(inp)\n",
        "        encs = []\n",
        "\n",
        "        for encoder, enc_mod, down in zip(self.encoders, self.enc_cond, self.downs):\n",
        "            x = encoder(x)\n",
        "            x = enc_mod(x, txtembd)\n",
        "            encs.append(x)\n",
        "            x = down(x)\n",
        "\n",
        "\n",
        "        if self.include_offset:\n",
        "            # print(\"middle block include_offset\")\n",
        "            middle_block_prompt = self.prompt_block_middle_blks(x)\n",
        "            # print(\"after prompt_block_level3\")\n",
        "            offset_vector = self.middleblock_offsetGen(middle_block_prompt)\n",
        "            # print(\"after offset_generators[0](middle_block_prompt)\")\n",
        "            for naf_block in self.middle_blks:\n",
        "                x = naf_block(x, offset_vector)\n",
        "        else:\n",
        "            x = self.middle_blks(x)\n",
        "\n",
        "        index = 0\n",
        "        for decoder, up, enc_skip, dec_mod in zip(self.decoders, self.ups, encs[::-1], self.dec_cond):\n",
        "\n",
        "            if self.include_offset is True:\n",
        "                # print(f\"self.include_offset: {self.include_offset}\")\n",
        "                x = up(x)\n",
        "                # offset_vector = None\n",
        "                # if (index < 3):\n",
        "                #     degradation_aware_prompt = self.promptBlocks[index](x)\n",
        "                #     offset_vector = self.offset_generators[index](degradation_aware_prompt)\n",
        "                #     index += 1\n",
        "\n",
        "                x = x + enc_skip\n",
        "\n",
        "                offset_vector = None\n",
        "                if (index < 3):\n",
        "                    degradation_aware_prompt = self.promptBlocks[index](x)\n",
        "                    offset_vector = self.offset_generators[index](degradation_aware_prompt)\n",
        "                    index += 1\n",
        "\n",
        "                if offset_vector is not None:\n",
        "                    for naf_block in decoder:\n",
        "                        x = naf_block(x, offset_vector)\n",
        "                else:\n",
        "                    x = decoder(x)\n",
        "\n",
        "                x = dec_mod(x, txtembd)\n",
        "            else:\n",
        "                x = up(x)\n",
        "                x = x + enc_skip\n",
        "                x = decoder(x)\n",
        "                x = dec_mod(x, txtembd)\n",
        "\n",
        "        # ending = conv layer to postprocess the final decoded feature into the desired image format.\n",
        "        x = self.ending(x)\n",
        "        x = x + inp\n",
        "\n",
        "        return x[:, :, :H, :W]\n",
        "\n",
        "    def check_image_size(self, x):\n",
        "        _, _, h, w = x.size()\n",
        "        mod_pad_h = (self.padder_size - h % self.padder_size) % self.padder_size\n",
        "        mod_pad_w = (self.padder_size - w % self.padder_size) % self.padder_size\n",
        "        x = F.pad(x, (0, mod_pad_w, 0, mod_pad_h))\n",
        "        return x\n",
        "\n",
        "\n",
        "def create_model(input_channels = 3, width = 32, enc_blks = [2, 2, 4, 8], middle_blk_num = 12, dec_blks = [2, 2, 2, 2], txtdim=768, include_offset=False):\n",
        "\n",
        "    net = InstructIR(img_channel=input_channels, width=width, middle_blk_num=middle_blk_num,\n",
        "                      enc_blk_nums=enc_blks, dec_blk_nums=dec_blks, txtdim=txtdim, include_offset=include_offset)\n",
        "\n",
        "    return net"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcctxh72vfpm"
      },
      "source": [
        "#Text model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "OjsxiX_Tvfwg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import DistilBertModel, DistilBertTokenizer, AutoModel, AutoTokenizer\n",
        "import os\n",
        "\n",
        "# Models that use mean pooling\n",
        "POOL_MODELS = {\"sentence-transformers/all-MiniLM-L6-v2\", \"TaylorAI/bge-micro-v2\"}\n",
        "\n",
        "#Mean Pooling - Take attention mask into account for correct averaging\n",
        "def mean_pooling(model_output, attention_mask):\n",
        "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
        "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "\n",
        "\n",
        "class LanguageModel(nn.Module):\n",
        "    def __init__(self, model='distilbert-base-uncased'):\n",
        "        super(LanguageModel, self).__init__()\n",
        "\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "        self.model = AutoModel.from_pretrained(model)\n",
        "        self.model_name = model\n",
        "        # Remove the CLIP vision tower\n",
        "        if \"clip\" in self.model_name:\n",
        "            self.model.vision_model = None\n",
        "        # Freeze the pre-trained parameters (very important)\n",
        "        for param in self.model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Make sure to set evaluation mode (also important)\n",
        "        self.model.eval()\n",
        "\n",
        "    def forward(self, text_batch):\n",
        "        inputs = self.tokenizer(text_batch, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "        # Get the device of the model's parameters (assumes all parameters are on the same device)\n",
        "        device = next(self.model.parameters()).device\n",
        "        # Move all inputs to that device\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "        with torch.no_grad(): # Ensure no gradients are computed for this forward pass\n",
        "\n",
        "            if \"clip\" in self.model_name:\n",
        "                sentence_embedding = self.model.get_text_features(**inputs)\n",
        "                return sentence_embedding\n",
        "\n",
        "            outputs = self.model(**inputs)\n",
        "\n",
        "        if any(model in self.model_name for model in POOL_MODELS):\n",
        "            sentence_embeddings = mean_pooling(outputs, inputs['attention_mask'])\n",
        "            # Normalize embeddings\n",
        "            sentence_embedding = F.normalize(sentence_embeddings, p=2, dim=1)\n",
        "        else:\n",
        "            sentence_embedding = outputs.last_hidden_state[:, 0, :]\n",
        "        return sentence_embedding\n",
        "\n",
        "\n",
        "class LMHead(nn.Module):\n",
        "    def __init__(self, embedding_dim=384, hidden_dim=256, num_classes=4):\n",
        "        super(LMHead, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n",
        "        #self.gelu = nn.GELU()\n",
        "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embd = self.fc1(x)\n",
        "        embd = F.normalize(embd, p=2, dim=1)\n",
        "        deg_pred = self.fc2(embd)\n",
        "        return embd, deg_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-N2Ct2SNvf3x"
      },
      "source": [
        "#options.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVepEgIrvf-V",
        "outputId": "83a1e4ff-9afc-4ec7-99e3-3ae3dc89607c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_StoreAction(option_strings=['--chkpt_epoch'], dest='chkpt_epoch', nargs=None, const=None, default=20, type=<class 'int'>, choices=None, required=False, help='Epochs for each Checkpoint ', metavar=None)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "import argparse\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "\n",
        "parser.add_argument('--patch_size', type=int, default=256, help='patchsize of input.')\n",
        "parser.add_argument('--instructir_batch_size', type=int, default=32, help='batch size of InstructIR.')\n",
        "parser.add_argument('--dataloader_batch_size', type=int, default=32, help='batch size of dataloader.')\n",
        "\n",
        "\n",
        "parser.add_argument('--epochs', type=int, default=300, help='Number of epochs for training')\n",
        "\n",
        "parser.add_argument('--num_workers', type=int, default=8, help='number of workers.')\n",
        "parser.add_argument(\"--checkpoint_dir\",type=str, default=\"/content/drive/MyDrive/FYPData/train_ckpt\",help = \"Name of the Directory where the checkpoint is to be saved\")\n",
        "parser.add_argument('--lm_head',      type=str, default=\"/content/drive/MyDrive/FYPData/models/lm_instructir-7d.pt\", help='Path to the language model weights')\n",
        "parser.add_argument('--config',  type=str, default='configs/eval5d.yml', help='Path to config file')\n",
        "parser.add_argument('--device',  type=int, default=0, help=\"GPU device\")\n",
        "\n",
        "parser.add_argument(\"--wblogger\",type=str,default=\"instructir\",help = \"Determine to log to wandb or not and the project name\")\n",
        "\n",
        "parser.add_argument('--data_file_dir',  type=str, default=\"/content/drive/MyDrive/FYPData/train_data_names/\", help=\"Files that contains the training file names\")\n",
        "\n",
        "# Reading from Google Drive Directly\n",
        "parser.add_argument('--denoise_dir',  type=str, default=\"/content/drive/MyDrive/FYPData/Train/denoise/\", help=\"Directory containing the images for denoise\")\n",
        "\n",
        "parser.add_argument('--dehaze_dir',  type=str, default=\"/content/drive/MyDrive/FYPData/Train/dehaze/\", help=\"Directory containing the images for dehaze\")\n",
        "\n",
        "parser.add_argument('--derain_dir',  type=str, default=\"/content/drive/MyDrive/FYPData/Train/derain/\", help=\"Directory containing the images for derain\")\n",
        "\n",
        "\n",
        "parser.add_argument('--de_type', nargs='+', default=['derain', 'denoise_15', 'dehaze', 'denoise_25', 'denoise_50'],\n",
        "                    help='which type of degradations is training and testing for.')\n",
        "# \"denoise_15\", \"dehaze\", \"denoise_25\", \"denoise_50\"\n",
        "parser.add_argument('--trained_model_weights', type=str, default=\"/content/drive/MyDrive/FYPData/trained_weights\", help=\"File name of model state_dict()\")\n",
        "\n",
        "parser.add_argument('--image_model', type=str, default=\"/content/drive/MyDrive/FYPData/models/im_instructir-7d.pt\", help='Path to the language model weights')\n",
        "\n",
        "\n",
        "parser.add_argument('--initial_lr',  type=float, default=5e-4, help=\"Learning Rate for optimizer\")\n",
        "\n",
        "parser.add_argument('--warmup_lr',  type=float, default=5e-6, help=\"Learning Rate for Scheduler\")\n",
        "\n",
        "parser.add_argument('--eta_min',  type=float, default=5e-6, help=\"Final Learning Rate for Scheduler\")\n",
        "# 30\n",
        "parser.add_argument('--warmup_epochs',  type=int, default=20, help=\"warmup epochs for scheduler\")\n",
        "# 20\n",
        "parser.add_argument('--chkpt_epoch',  type=int, default=20, help=\"Epochs for each Checkpoint \")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-j51A8RlvgEx"
      },
      "source": [
        "#Install \"lightning\" module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "SOYVD3y2vgM3",
        "outputId": "45799f05-2c40-42b0-ddaa-7f9447b36878"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Qy9PE9SAzycl"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    \"llm\": {\n",
        "        \"model\": \"TaylorAI/bge-micro-v2\",  # See Paper Sec. 3.2 and Appendix\n",
        "        \"model_dim\": 384,\n",
        "        \"embd_dim\": 256,\n",
        "        \"nclasses\": 7,  # noise, blur, rain, haze, lol, enhancement, upsampling (Paper Sec. 4.3)\n",
        "        \"weights\": False\n",
        "    },\n",
        "    \"model\": {\n",
        "        \"arch\": \"instructir\",\n",
        "        \"use_text\": True,\n",
        "        \"in_ch\": 3,\n",
        "        \"out_ch\": 3,\n",
        "        \"width\": 32,\n",
        "        \"enc_blks\": [2, 2, 4, 8],\n",
        "        \"middle_blk_num\": 4,\n",
        "        \"dec_blks\": [2, 2, 2, 2],\n",
        "        \"textdim\": 256,\n",
        "        \"weights\": False\n",
        "    },\n",
        "    \"test\": {\n",
        "        \"batch_size\": 1,\n",
        "        \"num_workers\": 3,\n",
        "        \"dn_datapath\": \"test-data/denoising_testsets/\",\n",
        "        \"dn_datasets\": [\"CBSD68\", \"urban100\", \"Kodak24\"],\n",
        "        \"dn_sigmas\": [15, 25, 50],\n",
        "        \"rain_targets\": [\"test-data/Rain100L/target/\"],\n",
        "        \"rain_inputs\": [\"test-data/Rain100L/input/\"],\n",
        "        \"haze_targets\": \"test-data/SOTS/GT/\",\n",
        "        \"haze_inputs\": \"test-data/SOTS/IN/\",\n",
        "        \"lol_targets\": \"test-data/LOL/high/\",\n",
        "        \"lol_inputs\": \"test-data/LOL/low/\",\n",
        "        \"gopro_targets\": \"test-data/GoPro/target/\",\n",
        "        \"gopro_inputs\": \"test-data/GoPro/input/\"\n",
        "    }\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRTXTcp50Gjw"
      },
      "source": [
        "#Train Modification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n79Qjd7HyZSE",
        "outputId": "d3825db1-9f83-4d96-dde1-de23707d80f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEST_PATH: /content/drive/MyDrive/FYPData/train_loss/instructir_wm_train_loss.txt\n",
            "Torch version: 2.6.0+cu124\n",
            "cuda version: 12.4\n",
            "CUDA available: True\n",
            "Options:\n",
            "Namespace(patch_size=256, instructir_batch_size=32, dataloader_batch_size=32, epochs=300, num_workers=8, checkpoint_dir='/content/drive/MyDrive/FYPData/train_ckpt', lm_head='/content/drive/MyDrive/FYPData/models/lm_instructir-7d.pt', config='configs/eval5d.yml', device=0, wblogger='instructir', data_file_dir='/content/drive/MyDrive/FYPData/train_data_names/', denoise_dir='/content/drive/MyDrive/FYPData/Train/denoise/', dehaze_dir='/content/drive/MyDrive/FYPData/Train/dehaze/', derain_dir='/content/drive/MyDrive/FYPData/Train/derain/', de_type=['derain', 'denoise_15', 'dehaze', 'denoise_25', 'denoise_50'], trained_model_weights='/content/drive/MyDrive/FYPData/trained_weights', image_model='/content/drive/MyDrive/FYPData/models/im_instructir-7d.pt', initial_lr=0.0005, warmup_lr=5e-06, eta_min=5e-06, warmup_epochs=20, chkpt_epoch=20)\n",
            "\n",
            "\n",
            "*************** device: cuda ***************\n",
            "\n",
            "\n",
            "Human Instruction Set Length: 1500\n",
            "\n",
            "['derain', 'denoise_15', 'dehaze', 'denoise_25', 'denoise_50']\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import argparse\n",
        "import os\n",
        "import numpy as np\n",
        "import yaml\n",
        "import random\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "# from training_utils.TrainDataset import InstructIRTrainDataset\n",
        "# from training_utils.scheduler import LinearWarmupCosineAnnealingLR\n",
        "# import sys\n",
        "# from options import parser\n",
        "# from models.instructir import InstructIR, create_model\n",
        "# from text.models import LanguageModel, LMHead\n",
        "\n",
        "\n",
        "def add_date_to_filename(filepath):\n",
        "    # Get today's date formatted as day_month_year (e.g. 08_03_2025)\n",
        "    today = datetime.today().strftime(\"%d_%m_%Y\")\n",
        "    # Split the filepath into the base and extension parts\n",
        "    base, ext = os.path.splitext(filepath)\n",
        "    # Construct the new filepath by inserting the date before the extension\n",
        "    new_filepath = f\"{base}_{today}{ext}\"\n",
        "    return new_filepath\n",
        "\n",
        "def create_training_checkpoint_folder(base_folder):\n",
        "    \"\"\"\n",
        "    Given a base folder path, this function creates a directory structure:\n",
        "    base_folder/train_date_DD_MM_YYYY/train_time_HH_MM\n",
        "    using today's date and the current time, then returns the new folder path.\n",
        "\n",
        "    Args:\n",
        "        base_folder (str): The path to the base checkpoint folder.\n",
        "\n",
        "    Returns:\n",
        "        str: The full path of the created training checkpoint folder.\n",
        "    \"\"\"\n",
        "    # Format today's date as DD_MM_YYYY (e.g., \"08_03_2025\")\n",
        "    today_date = datetime.today().strftime(\"%d_%m_%Y\")\n",
        "    # Format current time as HH_MM (e.g., \"01_20\")\n",
        "    start_time = datetime.now().strftime(\"%H_%M\")\n",
        "\n",
        "    # Construct the new folder path\n",
        "    new_folder_path = f\"{base_folder}/train_date_{today_date}/train_time_{start_time}\"\n",
        "\n",
        "    return new_folder_path\n",
        "\n",
        "DEST_PATH = \"/content/drive/MyDrive/FYPData/train_loss/instructir_wm_train_loss.txt\"\n",
        "\n",
        "print(f\"DEST_PATH: {DEST_PATH}\")\n",
        "\n",
        "def save_performance_to_file(dest_path, text):\n",
        "    \"\"\"\n",
        "    Append the given text to the file at dest_path.\n",
        "    If the file does not exist, it is created.\n",
        "    If the folder does not exist, it is created.\n",
        "\n",
        "    Args:\n",
        "        dest_path (str): The path to the destination file.\n",
        "        text (str): The text to be appended to the file.\n",
        "    \"\"\"\n",
        "    # Extract the directory from the destination path.\n",
        "    directory = os.path.dirname(dest_path)\n",
        "    # if directory and not os.path.exists(directory):\n",
        "    os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "    print(f\"Saving Training Loss to {dest_path}\")\n",
        "    with open(dest_path, \"a\") as f:\n",
        "        f.write(text)\n",
        "        f.write(\"\\n\")\n",
        "    print(\"Saving Training Loss done!\")\n",
        "\n",
        "\n",
        "def train_instructir_model(instructir_model, language_model, lm_head, train_data_loader,human_instructions, optimizer, scheduler, loss_fn, opt, device,\n",
        "    accumulation_steps, image_batch_size, ini_message = \"Stage One:\", check_point_epoch=20):\n",
        "    \"\"\"\n",
        "    Train the InstructIR model.\n",
        "    Args:\n",
        "        instructir_model: The image restoration model to train.\n",
        "        language_model: A language model that has a tokenizer and a model (e.g., Hugging Face model).\n",
        "        lm_head: A head applied on the language model output (e.g., to get text embeddings).\n",
        "        train_data_loader: DataLoader yielding training batches.\n",
        "        human_instructions: List of human instruction strings.\n",
        "        optimizer: The optimizer (e.g., Adam).\n",
        "        scheduler: Learning rate scheduler.\n",
        "        loss_fn: Loss function (e.g., L1 loss).\n",
        "        opt: An object with training options (must contain opt.epochs and opt.checkpoint_dir).\n",
        "        device: The torch device (e.g., 'cuda' or 'cpu').\n",
        "        accumulation_steps: Number of batches over which to accumulate gradients.\n",
        "        image_batch_size: The number of images per batch (used to generate instructions).\n",
        "    Returns:\n",
        "        The trained instructir_model.\n",
        "    \"\"\"\n",
        "    base = 0\n",
        "    save_performance_to_file(DEST_PATH, ini_message)\n",
        "    checkpoint_dir = create_training_checkpoint_folder(opt.checkpoint_dir)\n",
        "    for epoch in range(opt.epochs):\n",
        "        now = datetime.now()\n",
        "        epoch_msg = f\"{now.strftime('%Y-%m-%d %H:%M:%S')} --- Start of Epoch: {epoch + 1 + base}\\n\"\n",
        "        print(epoch_msg)\n",
        "\n",
        "        instructir_model.train()\n",
        "        running_loss = 0.0\n",
        "        # accumulated_loss = 0.0  # Initialize accumulated loss\n",
        "        for batch_idx, batch in enumerate(train_data_loader):\n",
        "            # Unpack batch\n",
        "            [clean_name, deg_id], degrad_patch, clean_patch = batch\n",
        "            # print(f\"clean name: {clean_name}\")\n",
        "            # Get a random human instruction for each image in the batch\n",
        "            human_instruction = [random.choice(human_instructions) for _ in range(image_batch_size)]\n",
        "\n",
        "            lm_embd = language_model(human_instruction)\n",
        "            lm_embd = lm_embd.to(device)\n",
        "            text_embd, deg_pred = lm_head(lm_embd)\n",
        "            # Forward pass through the InstructIR model\n",
        "            degrad_patch = degrad_patch.to(device)\n",
        "            clean_patch = clean_patch.to(device)\n",
        "            restored_image = instructir_model(degrad_patch, text_embd)\n",
        "            # Compute loss\n",
        "            loss = loss_fn(restored_image, clean_patch)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            # Backpropagate\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            if( batch_idx + 1) % 100 == 0:\n",
        "                batch_msg = f\"Epoch [{epoch + 1 + base}/{opt.epochs}], Batch [{batch_idx+1}/{len(train_data_loader)}], Running Avg Loss: {running_loss/(batch_idx+1):.20f}\\n\"\n",
        "                # print(batch_msg)\n",
        "                if( batch_idx + 1) % 250 == 0:\n",
        "                    print(batch_msg)\n",
        "                epoch_msg += batch_msg\n",
        "\n",
        "        # Step the scheduler after each epoch\n",
        "        if scheduler is not None:\n",
        "            scheduler.step(epoch)\n",
        "        avg_epoch_loss = running_loss / len(train_data_loader)\n",
        "        avg_epoch_loss_str = f\"Epoch [{epoch + 1 + base}/{opt.epochs }], Average Loss: {avg_epoch_loss:.20f}\\n\"\n",
        "        epoch_msg += avg_epoch_loss_str\n",
        "        print(avg_epoch_loss_str)\n",
        "\n",
        "        # Save a checkpoint for the current epoch\n",
        "        if (epoch + 1 + base) % check_point_epoch == 0:\n",
        "            os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "            checkpoint_path_model = os.path.join(checkpoint_dir, f\"64_chan_epoch_{epoch + 1 + base}_model.pt\")\n",
        "            checkpoint_path_scheduler = os.path.join(checkpoint_dir, f\"epoch_{epoch + 1 + base}_scheduler.pt\")\n",
        "            checkpoint_path_optimizer = os.path.join(checkpoint_dir, f\"epoch_{epoch + 1 + base}_optimizer.pt\")\n",
        "\n",
        "            torch.save(instructir_model.state_dict(), checkpoint_path_model)\n",
        "            torch.save(scheduler.state_dict(), checkpoint_path_scheduler)\n",
        "            torch.save(optimizer.state_dict(), checkpoint_path_optimizer)\n",
        "            print(f\"Checkpoint saved at {checkpoint_path_model}\\n{checkpoint_path_scheduler}\\n{checkpoint_path_optimizer}\")\n",
        "        # print end of epoch\n",
        "        now = datetime.now()\n",
        "        end_epoch_msg = f\"{now.strftime('%Y-%m-%d %H:%M:%S')} --- End of Epoch: {epoch + 1 + base}\\n\"\n",
        "        print(end_epoch_msg)\n",
        "        epoch_msg += end_epoch_msg\n",
        "        # save current epochs statistics\n",
        "        save_performance_to_file(DEST_PATH, epoch_msg)\n",
        "    return instructir_model\n",
        "\n",
        "def seed_everything(SEED=42):\n",
        "    random.seed(SEED)\n",
        "    np.random.seed(SEED)\n",
        "    torch.manual_seed(SEED)\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "def count_params(model):\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad is True)\n",
        "    return trainable_params\n",
        "\n",
        "def dict2namespace(config):\n",
        "    namespace = argparse.Namespace()\n",
        "    for key, value in config.items():\n",
        "        if isinstance(value, dict):\n",
        "            new_value = dict2namespace(value)\n",
        "        else:\n",
        "            new_value = value\n",
        "        setattr(namespace, key, new_value)\n",
        "    return namespace\n",
        "\n",
        "import os\n",
        "def main():\n",
        "    now = datetime.now()\n",
        "\n",
        "    # Format datetime as \"YYYY-MM-DD HH:MM\"\n",
        "    formatted_datetime = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "    training_message = f\"{formatted_datetime} Training Log for Re-train InstructIR (output channel in OffsetGenertor conv = 32) - last 20 epochs\\n\\n\"\n",
        "\n",
        "    # torch.cuda.empty_cache()\n",
        "    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = \"expandable_segments:True\"\n",
        "    # Set seeds for reproducibility\n",
        "    seed_everything()\n",
        "    print(f\"Torch version: {torch.__version__}\")\n",
        "    print(f\"cuda version: {torch.version.cuda}\")\n",
        "    print(\"CUDA available:\", torch.cuda.is_available())\n",
        "\n",
        "    # Parse input arguments\n",
        "    opt = parser.parse_args([])\n",
        "    print(\"Options:\")\n",
        "    print(opt)\n",
        "\n",
        "\n",
        "    LANGUAGE_HEAD = opt.lm_head\n",
        "    CONFIG = opt.config\n",
        "    IMAGE_BATCH_SIZE_DATALOADER = opt.dataloader_batch_size\n",
        "    INITIAL_LEARNING_RATE = opt.initial_lr\n",
        "    MAX_EPOCHS = opt.epochs\n",
        "    WARMUP_EPOCHS = opt.warmup_epochs\n",
        "    WARMUP_START_LR=opt.warmup_lr\n",
        "    ETA_MIN = opt.eta_min\n",
        "    CHECK_POINT_EPOCH = opt.chkpt_epoch\n",
        "    accumulation_steps = opt.instructir_batch_size/IMAGE_BATCH_SIZE_DATALOADER\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"\\n\\n*************** device: {device} ***************\\n\\n\")\n",
        "\n",
        "\n",
        "    training_message += f\"options: {opt}\\n\\n*************** device: {device} ***************\\n\\n\"\n",
        "\n",
        "    # Parse config file and convert to namespace for easy attribute access\n",
        "    # with open(os.path.join(CONFIG), \"r\") as f:\n",
        "    # config = yaml.safe_load(f)\n",
        "    cfg = dict2namespace(config)\n",
        "\n",
        "    # Load human instructions from JSON\n",
        "    # print(\"Loading human instructions...\")\n",
        "    instruction_file_path = \"/content/drive/MyDrive/FYPData/human_instructions.json\"\n",
        "    with open(instruction_file_path, \"r\") as f:\n",
        "        data = json.load(f)\n",
        "    human_instructions = data[\"denoising\"] + data[\"deraining\"] + data[\"dehazing\"]\n",
        "    random.shuffle(human_instructions)\n",
        "    print(f\"Human Instruction Set Length: {len(human_instructions)}\\n\")\n",
        "\n",
        "    training_message += f\"Human Instruction Set Length: {len(human_instructions)}\\n\"\n",
        "\n",
        "    # Create training dataset and dataloader\n",
        "    training_dataset = InstructIRTrainDataset(args=opt)\n",
        "    train_data_loader = DataLoader(\n",
        "        training_dataset,\n",
        "        batch_size=IMAGE_BATCH_SIZE_DATALOADER,\n",
        "        pin_memory=True,\n",
        "        shuffle=True,\n",
        "        drop_last=True,\n",
        "        num_workers=opt.num_workers,\n",
        "        persistent_workers=True\n",
        "    )\n",
        "\n",
        "\n",
        "    training_message += f\"Loading Language Model: {cfg.llm.model}\"\n",
        "    # Load language model and LM head\n",
        "    print(f\"\\nLoading Language Model: {cfg.llm.model}\")\n",
        "    language_model = LanguageModel(model=cfg.llm.model).to(device)\n",
        "    lm_head = LMHead(\n",
        "        embedding_dim=cfg.llm.model_dim,\n",
        "        hidden_dim=cfg.llm.embd_dim,\n",
        "        num_classes=cfg.llm.nclasses\n",
        "    ).to(device)\n",
        "\n",
        "    lm_nparams = count_params(lm_head)\n",
        "    print(\"Projection Head CKPT Path:\", LANGUAGE_HEAD)\n",
        "    lm_head.load_state_dict(torch.load(LANGUAGE_HEAD, map_location=device), strict=True)\n",
        "    print(\"\\nProjection Head loaded weights!\", lm_nparams/1e6, \"M\\n\")\n",
        "\n",
        "    training_message += f\"Projection Head CKPT Path: {LANGUAGE_HEAD}\\nProjection Head loaded weights: {lm_nparams/1e6}M\\nFreezing language model and project head\"\n",
        "\n",
        "    # Freeze parameters of the language model and LM head\n",
        "    for param in language_model.parameters():\n",
        "        param.requires_grad = False\n",
        "    for param in lm_head.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    print(f\"------ Training for {opt.de_type}\")\n",
        "\n",
        "    training_message += f\"------ Training for {opt.de_type}\\nDataset:\"\n",
        "\n",
        "    noise_dataset_size = \"\"\n",
        "    rain_dataset_size = \"\"\n",
        "    haze_dataset_size = \"\"\n",
        "\n",
        "    if (\"denoise_15\" in opt.de_type) or (\"denoise_25\" in opt.de_type) or (\"denoise_50\" in opt.de_type):\n",
        "        noise_dataset_size = f\"Noise sigma 15: {len(training_dataset.s15_ids)}, Noise sigma 25: {len(training_dataset.s25_ids)}, Noise sigma 50: {len(training_dataset.s50_ids)}\\n\"\n",
        "    if \"dehaze\" in opt.de_type:\n",
        "        haze_dataset_size = f\"Haze Images Length: {len(training_dataset.hazy_ids)}\\n\"\n",
        "    if \"derain\" in opt.de_type:\n",
        "        rain_dataset_size = f\"Rain Images Length: {len(training_dataset.rs_ids)}\\n\"\n",
        "\n",
        "    training_message += noise_dataset_size\n",
        "    training_message += haze_dataset_size\n",
        "    training_message += rain_dataset_size\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    print(\"----------------- Retrain InstructIR started... ----------------------\")\n",
        "    print(\"Start loading the pre-trained InstructIR model...\")\n",
        "    # Create the InstructIR model and move to device\n",
        "    instructir_model = create_model(txtdim=256, middle_blk_num =4, width=opt.instructir_batch_size, include_offset=True)\n",
        "    instructir_model = instructir_model.to(device)\n",
        "    print(\"middle_block = 4\")\n",
        "\n",
        "    im_checkpoint_path = \"/content/drive/MyDrive/FYPData/models/im_instructir-7d.pt\"\n",
        "\n",
        "\n",
        "    # model_file_path = os.path.join(opt.trained_model_weights, \"instructir_weights_3d_12_03_2025.pt\")\n",
        "    # model_file_path = opt.image_model\n",
        "    instructir_stage_one_path = im_checkpoint_path\n",
        "    print(\"\\nInstructIR Path:\", instructir_stage_one_path, \"\\n\")\n",
        "    instructir_model.load_state_dict(torch.load(instructir_stage_one_path, map_location=device), strict=False)\n",
        "\n",
        "    training_message += f\"\\nInstructIR Path: {instructir_stage_one_path}\\n\"\n",
        "\n",
        "    instructir_model_nparams = count_params(instructir_model)\n",
        "    total_params_msg = f\"\\n---- InstructIR + Modifcation: {instructir_model_nparams/1e6}M parameters\\n\"\n",
        "\n",
        "    for param in instructir_model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "  # enable prompt generator blocks\n",
        "    for prompt_block in instructir_model.promptBlocks:\n",
        "        for param in prompt_block.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "  # enable offset generators\n",
        "    for offset_gen in instructir_model.offset_generators:\n",
        "        for param in offset_gen.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "    for param in instructir_model.prompt_block_middle_blks.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "    for param in instructir_model.middleblock_offsetGen.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "\n",
        "\n",
        "    # instructir_model_nparams = count_params(instructir_model)\n",
        "    # total_params_msg = f\"\\n---- InstructIR + Modifcation: {instructir_model_nparams/1e6}M parameters\\n\"\n",
        "\n",
        "    params_to_train = list(instructir_model.promptBlocks.parameters()) + list(instructir_model.offset_generators.parameters()) + \\\n",
        "        list(instructir_model.prompt_block_middle_blks.parameters()) + list(instructir_model.middleblock_offsetGen.parameters())\n",
        "\n",
        "    mod_params_msg = f\"\\n---- OffsetGenerators + PromptGenBlocks: {sum(p.numel() for p in params_to_train if p.requires_grad is True)/1e6}M parameters\\n\"\n",
        "\n",
        "    print(total_params_msg, mod_params_msg)\n",
        "\n",
        "    training_message += total_params_msg\n",
        "    training_message += mod_params_msg\n",
        "\n",
        "    assert count_params(instructir_model) == sum(p.numel() for p in params_to_train if p.requires_grad is True)\n",
        "\n",
        "    scheduler = None\n",
        "    # Set up new optimizer and learning rate scheduler\n",
        "    optimizer = optim.AdamW(params_to_train,  lr=INITIAL_LEARNING_RATE)\n",
        "    scheduler = LinearWarmupCosineAnnealingLR(optimizer=optimizer,  warmup_epochs=WARMUP_EPOCHS, max_epochs=MAX_EPOCHS,\n",
        "      warmup_start_lr=WARMUP_START_LR, eta_min=ETA_MIN )\n",
        "\n",
        "    training_params = f\"\"\"\\nLearning Rate: {INITIAL_LEARNING_RATE}, Image Model Batch_size: {opt.instructir_batch_size}, Dataloader Batch_size: {IMAGE_BATCH_SIZE_DATALOADER}, Acccumulation steps: {accumulation_steps}\\n\"\"\"\n",
        "\n",
        "    scheduler_setting = f\"Scheduler: {str(scheduler)}\\nmax_epochs: {MAX_EPOCHS}, warmup_start_lr:{WARMUP_START_LR}, eta_min: {ETA_MIN})\"\n",
        "\n",
        "    training_message += training_params\n",
        "    training_message += scheduler_setting\n",
        "\n",
        "    print(training_params, scheduler_setting)\n",
        "\n",
        "    if scheduler is None:\n",
        "        print(\"\\nNot using scheduler\")\n",
        "\n",
        "    #################################################### Load optimizer, scheduler, instructIR model ##############################\n",
        "\n",
        "    # optimizer_checkpoint_path = \"/content/drive/MyDrive/FYPData/trained_weights/epoch_280_optimizer.pth\"\n",
        "    # scheduler_checkpoint_path = \"/content/drive/MyDrive/FYPData/trained_weights/epoch_280_scheduler.pth\"\n",
        "\n",
        "    # # Load the optimizer checkpoint with map_location set to device\n",
        "    # optimizer.load_state_dict(torch.load(optimizer_checkpoint_path, map_location=device))\n",
        "\n",
        "    # # Load the scheduler checkpoint with map_location set to device\n",
        "    # scheduler.load_state_dict(torch.load(scheduler_checkpoint_path, map_location=device))\n",
        "\n",
        "    # training_message += f\"Loaded optimizer checkpoint at: {optimizer_checkpoint_path}\\nLoaded scheduler checkpoint at: {scheduler_checkpoint_path}\"\n",
        "\n",
        "\n",
        "    #################################################### Load optimizer, scheduler, instructIR model ##############################\n",
        "\n",
        "\n",
        "    # Loss function\n",
        "    loss_fn = nn.L1Loss()\n",
        "\n",
        "    # train for the offset generator and promptblock\n",
        "    instructir_model = train_instructir_model(instructir_model=instructir_model, language_model=language_model, lm_head=lm_head,\n",
        "        train_data_loader=train_data_loader, human_instructions=human_instructions, optimizer=optimizer, scheduler=scheduler,\n",
        "        loss_fn=loss_fn, opt=opt, device=device, accumulation_steps=accumulation_steps, image_batch_size=IMAGE_BATCH_SIZE_DATALOADER,\n",
        "        ini_message=training_message, check_point_epoch=CHECK_POINT_EPOCH\n",
        "    )\n",
        "\n",
        "    # Save the final model weights\n",
        "    model_weights_file_name = \"instructir_with_weight_modulation_32ch.pt\"\n",
        "    os.makedirs(opt.trained_model_weights, exist_ok=True)\n",
        "    model_file_path = os.path.join(opt.trained_model_weights, model_weights_file_name)\n",
        "    torch.save(instructir_model.state_dict(), model_file_path)\n",
        "    print(f\"Retraining finished, final model weights saved at: {model_file_path}\\n\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hCHROuBOr1h5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}